"""
çœŸæ­£çš„æ‡’åŠ è½½è¯­è¨€æ¨¡å‹æ¨¡å—

åªåœ¨å®é™…ä½¿ç”¨æ—¶æ‰åˆå§‹åŒ–æ¨¡å‹ï¼Œå¤§å¹…æå‡å¯¼å…¥é€Ÿåº¦ã€‚
"""

import os
from functools import lru_cache
from typing import Dict, Optional
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ¨¡å‹é…ç½®å­—å…¸ï¼ˆä¸åŒ…å«å®é™…çš„æ¨¡å‹å®ä¾‹ï¼‰
MODEL_CONFIGS = {
    # Gemini ç³»åˆ—
    'gemini_2_5_flash': {
        'model': 'models/gemini-2.5-flash',
        'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
        'api_key_env': 'GEMINI_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'gemini_2_5_pro': {
        'model': 'gemini-2.5-pro',
        'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
        'api_key_env': 'GEMINI_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'gemini_2_flash': {
        'model': 'gemini-2.0-flash',
        'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
        'api_key_env': 'GEMINI_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'gemini_2_flash_lite': {
        'model': 'gemini-2.0-flash-lite-preview-02-05',
        'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
        'api_key_env': 'GEMINI_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    
    # DeepSeek ç³»åˆ—
    'deepseek_v3': {
        'model': 'deepseek-ai/DeepSeek-V3',
        'base_url': 'https://api.siliconflow.cn/v1',
        'api_key_env': 'SILICONFLOW_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'deepseek_r1': {
        'model': 'deepseek-ai/DeepSeek-R1',
        'base_url': 'https://api.siliconflow.cn/v1',
        'api_key_env': 'SILICONFLOW_API_KEY',
        'max_tokens': 8192,
        'temperature': 0
    },
    'deepseek_chat': {
        'model': 'deepseek-chat',
        'base_url': 'https://api.deepseek.com',
        'api_key_env': 'DEEPSEEK_API_KEY',
        'max_tokens': 8192,
        'temperature': 0
    },
    'deepseek_reasoner': {
        'model': 'deepseek-reasoner',
        'base_url': 'https://api.deepseek.com',
        'api_key_env': 'DEEPSEEK_API_KEY',
        'max_tokens': 8192,
        'temperature': 0.6
    },
    
    # Qwen ç³»åˆ—
    'qwen_qwq_32b': {
        'model': 'Qwen/QwQ-32B',
        'base_url': 'https://api.siliconflow.cn/v1',
        'api_key_env': 'SILICONFLOW_API_KEY',
        'max_tokens': 8192,
        'temperature': 0
    },
    'qwen_2_5_coder_32b': {
        'model': 'Qwen/Qwen2.5-Coder-32B-Instruct',
        'base_url': 'https://api.siliconflow.cn/v1',
        'api_key_env': 'SILICONFLOW_API_KEY',
        'max_tokens': 8192,
        'temperature': 0
    },
    'qwen_2_5_72b': {
        'model': 'qwen/qwen-2.5-72b-instruct',
        'base_url': 'https://openrouter.ai/api/v1',
        'api_key_env': 'OPENROUTER_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    
    # Claude ç³»åˆ—
    'claude_35_sonnet': {
        'model': 'anthropic/claude-3.5-sonnet:beta',
        'base_url': 'https://openrouter.ai/api/v1',
        'api_key_env': 'OPENROUTER_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'claude_37_sonnet': {
        'model': 'anthropic/claude-3.7-sonnet',
        'base_url': 'https://openrouter.ai/api/v1',
        'api_key_env': 'OPENROUTER_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'claude_sonnet_4': {
        'model': 'anthropic/claude-sonnet-4',
        'base_url': 'https://openrouter.ai/api/v1',
        'api_key_env': 'OPENROUTER_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    
    # OpenAI ç³»åˆ—
    'gpt_4o_mini': {
        'model': 'openai/gpt-4o-mini',
        'base_url': 'https://openrouter.ai/api/v1',
        'api_key_env': 'OPENROUTER_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    },
    'o3_mini': {
        'model': 'openai/o3-mini',
        'base_url': 'https://openrouter.ai/api/v1',
        'api_key_env': 'OPENROUTER_API_KEY',
        'max_tokens': 4096,
        'temperature': 0
    }
}

# åˆ›å»ºHTTPå®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰
_http_client = None

def _get_http_client():
    """æ‡’åŠ è½½HTTPå®¢æˆ·ç«¯"""
    global _http_client
    if _http_client is None:
        try:
            import httpx
            _http_client = httpx.Client(
                proxy='socks5://127.0.0.1:7890',
                timeout=10,
                verify=False
            )
        except Exception:
            import httpx
            _http_client = httpx.Client(timeout=10, verify=False)
    return _http_client

@lru_cache(maxsize=None)
def get_model(model_name: str):
    """
    æ‡’åŠ è½½è·å–è¯­è¨€æ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        ChatOpenAIå®ä¾‹æˆ–None
    """
    if model_name not in MODEL_CONFIGS:
        available_models = ', '.join(MODEL_CONFIGS.keys())
        print(f"âš ï¸  æœªçŸ¥æ¨¡å‹åç§°: {model_name}")
        print(f"å¯ç”¨æ¨¡å‹: {available_models}")
        return None
    
    config = MODEL_CONFIGS[model_name]
    api_key = os.getenv(config['api_key_env'])
    
    if not api_key:
        print(f"âš ï¸  ç¼ºå°‘APIå¯†é’¥ç¯å¢ƒå˜é‡: {config['api_key_env']}")
        return None
    
    try:
        # åªåœ¨éœ€è¦æ—¶æ‰å¯¼å…¥ChatOpenAI
        from langchain_openai import ChatOpenAI
        
        model = ChatOpenAI(
            temperature=config.get('temperature', 0),
            model=config['model'],
            base_url=config['base_url'],
            api_key=api_key,
            max_tokens=config.get('max_tokens', 4096),
            http_client=_get_http_client()
        )
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
        return model
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {e}")
        return None

def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    for name, config in MODEL_CONFIGS.items():
        print(f"  {name:20} -> {config['model']}")
    return MODEL_CONFIGS

# ä¾¿æ·è®¿é—®å‡½æ•°
def get_default():
    """è·å–é»˜è®¤æ¨¡å‹ï¼ˆGemini 2.5 Flashï¼‰"""
    return get_model('gemini_2_5_flash')

def get_smart():
    """è·å–æ™ºèƒ½æ¨¡å‹ï¼ˆGemini 2.5 Proï¼‰"""
    return get_model('gemini_2_5_pro')

def get_coder():
    """è·å–ä»£ç æ¨¡å‹ï¼ˆDeepSeek V3ï¼‰"""
    return get_model('deepseek_v3')

def get_reasoner():
    """è·å–æ¨ç†æ¨¡å‹ï¼ˆQwen QwQ 32Bï¼‰"""
    return get_model('qwen_qwq_32b')

# å…¼å®¹æ€§æ˜ å°„ï¼ˆå¯¹åº”åŸpythonTaskä¸­çš„å˜é‡åï¼‰
COMPATIBILITY_MAPPING = {
    'llm_gemini_2_5_flash_google': 'gemini_2_5_flash',
    'llm_gemini_2_5_pro_google': 'gemini_2_5_pro',
    'llm_gemini_2_flash_google': 'gemini_2_flash',
    'llm_DeepSeek_V3_siliconflow': 'deepseek_v3',
    'llm_DeepSeek_R1_siliconflow': 'deepseek_r1',
    'llm_deepseek': 'deepseek_chat',
    'llm_deepseek_r1': 'deepseek_reasoner',
    'llm_Qwen_QwQ_32B_siliconflow': 'qwen_qwq_32b',
}

def get_model_by_old_name(old_name: str):
    """é€šè¿‡åŸpythonTaskä¸­çš„å˜é‡åè·å–æ¨¡å‹"""
    if old_name in COMPATIBILITY_MAPPING:
        return get_model(COMPATIBILITY_MAPPING[old_name])
    else:
        print(f"âš ï¸  æœªçŸ¥çš„æ—§æ¨¡å‹åç§°: {old_name}")
        return None

def demo_lazy_loading():
    """æ¼”ç¤ºæ‡’åŠ è½½çš„æ€§èƒ½ä¼˜åŠ¿"""
    import time
    print("ğŸš€ çœŸæ­£çš„æ‡’åŠ è½½è¯­è¨€æ¨¡å‹æ¼”ç¤º")
    print("=" * 50)
    
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹:")
    list_models()
    
    print("\nâš¡ æ€§èƒ½æµ‹è¯•:")
    
    # æµ‹è¯•æ‡’åŠ è½½æ€§èƒ½
    start_time = time.time()
    llm1 = get_model('gemini_2_5_flash')
    first_load_time = time.time() - start_time
    
    start_time = time.time()
    llm2 = get_model('gemini_2_5_flash')  # ç¬¬äºŒæ¬¡è®¿é—®ï¼Œä½¿ç”¨ç¼“å­˜
    cached_load_time = time.time() - start_time
    
    print(f"é¦–æ¬¡åŠ è½½è€—æ—¶: {first_load_time:.3f}s")
    print(f"ç¼“å­˜åŠ è½½è€—æ—¶: {cached_load_time:.6f}s")
    if cached_load_time > 0:
        print(f"ç¼“å­˜æ€§èƒ½æå‡: {first_load_time/cached_load_time:.0f}x")
    
    print(f"âœ… æ¨¡å‹å¯¹è±¡ç›¸åŒ: {llm1 is llm2}")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. å¿«é€Ÿå¯¼å…¥: from llm_lazy import get_model")
    print("2. æŒ‰éœ€è·å–: llm = get_model('gemini_2_5_flash')")
    print("3. ä¾¿æ·å‡½æ•°: llm = get_default()")

print("ğŸ’¡ çœŸæ­£çš„æ‡’åŠ è½½æ¨¡å‹æ¨¡å—å·²åŠ è½½ã€‚ä½¿ç”¨ get_model('model_name') æŒ‰éœ€è·å–æ¨¡å‹ã€‚")