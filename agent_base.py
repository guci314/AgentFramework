from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.language_models import BaseChatModel
from typing import Iterator
import tiktoken # Add tiktoken import for accurate counting
import functools
import inspect
import random
import string
import uuid
from datetime import datetime
from typing import List, Dict, Any, Tuple, Union, Optional, Callable
import os # éœ€è¦å¯¼å…¥ os æ¨¡å—

# å®šä¹‰æœ€å¤§tokenæ•°å¸¸é‡ (ä½œä¸ºæœ€ç»ˆçš„åå¤‡)
MAX_TOKENS = 60000

#region Result
class Result:
    """
    æ‰§è¡Œç»“æœç±»
    
    ç”¨äºå°è£…æ™ºèƒ½ä½“æ‰§è¡Œæ“ä½œåçš„ç»“æœä¿¡æ¯ï¼ŒåŒ…æ‹¬æ‰§è¡ŒçŠ¶æ€ã€ä»£ç ã€è¾“å‡ºç­‰ã€‚
    æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥æ‰§è¡Œç»“æœçš„ç»Ÿä¸€è¡¨ç¤ºã€‚
    
    æ³¨æ„ï¼šå¯¹äºå•å…ƒæµ‹è¯•æˆ–éªŒè¯ç±»æŒ‡ä»¤ï¼Œsuccess=Trueè¡¨ç¤ºæˆåŠŸå®Œæˆäº†æµ‹è¯•æˆ–éªŒè¯æ“ä½œï¼Œ
    è€Œä¸æ˜¯æµ‹è¯•ç»“æœæ˜¯å¦é€šè¿‡ã€‚æµ‹è¯•æˆ–éªŒè¯çš„å®é™…ç»“æœï¼ˆé€šè¿‡/å¤±è´¥ï¼‰è®°å½•åœ¨return_valueä¸­ã€‚
        
    Attributes:
        success (bool): æ‰§è¡ŒæˆåŠŸæ ‡å¿—
        code (str): æ ¹æ®æŒ‡ä»¤ç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç 
        stdout (str): æ ‡å‡†è¾“å‡ºå†…å®¹
        stderr (str): æ ‡å‡†é”™è¯¯è¾“å‡ºå†…å®¹
        return_value (str): æ‰§è¡Œç»“æœçš„è¿”å›å€¼
    """
    def __init__(self, success: bool, code: str, stdout: str = None, stderr: str = None, return_value: str = None):
        self.success = success
        self.code = code
        self.stdout = stdout
        self.stderr = stderr
        self.return_value = return_value
    
    def __str__(self) -> str:
        code = self.code or ""
        stdout = self.stdout or ""
        stderr = self.stderr or ""
        try:
            return_value = str(self.return_value) if self.return_value else ""
        except:
            return_value = ""
        return f''' 
            "success":{self.success} 
            "code":{code} 
            "stdout":{stdout} 
            "stderr":{stderr} 
            "return_value":{return_value} 
        '''
    
    def __repr__(self) -> str:
        return self.__str__()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'success': self.success,
            'code': self.code,
            'stdout': self.stdout,
            'stderr': self.stderr,
            'return_value': self.return_value
        }
#endregion

#region reduce_memory_decorator (ä¿®æ”¹å)
def reduce_memory_decorator(func=None, *, max_tokens=None):
    """
    è£…é¥°å™¨ï¼šåœ¨å‡½æ•°æ‰§è¡Œåæ£€æŸ¥memoryå¤§å°ï¼Œå¦‚æœè¶…è¿‡max_tokensåˆ™å‡å°‘memory
    é‡è¦çš„æ˜¯ä¿æŒmemoryä¸­messageçš„äº¤æ›¿é¡ºåº
    å¯ä»¥ç›´æ¥è£…é¥°å‡½æ•°æˆ–ä½¿ç”¨å‚æ•°ï¼š@reduce_memory_decorator æˆ– @reduce_memory_decorator(max_tokens=1000)

    åŠ¨æ€è®¾ç½® max_tokens çš„ä¼˜å…ˆçº§:
    1. è£…é¥°å™¨å‚æ•°: @reduce_memory_decorator(max_tokens=value)
    2. ç¯å¢ƒå˜é‡: AGENT_MAX_TOKENS
    3. å…¨å±€å¸¸é‡: MAX_TOKENS
    """
    # 1. ç¡®å®šæœ€ç»ˆç”Ÿæ•ˆçš„ max_tokens å€¼
    effective_max_tokens = MAX_TOKENS # é»˜è®¤ä½¿ç”¨å…¨å±€å¸¸é‡
    try:
        # 2. å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        env_max_tokens = os.environ.get("AGENT_MAX_TOKENS")
        if env_max_tokens is not None:
            effective_max_tokens = int(env_max_tokens)
    except (ValueError, TypeError):
        # å¦‚æœç¯å¢ƒå˜é‡æ ¼å¼é”™è¯¯ï¼Œåˆ™å¿½ç•¥ï¼Œç»§ç»­ä½¿ç”¨å…¨å±€å¸¸é‡
        pass

    # 3. å¦‚æœè£…é¥°å™¨ç›´æ¥æä¾›äº† max_tokens å‚æ•°ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®ƒ
    if max_tokens is not None:
        effective_max_tokens = max_tokens

    # --- å†…éƒ¨ decorator å’Œ _reduce_memory ä¿æŒä¸å˜ï¼Œä½†ä½¿ç”¨ effective_max_tokens ---
    def decorator(decorated_func):
        @functools.wraps(decorated_func)
        def wrapper(*args, **kwargs):
            # ç¡®ä¿æˆ‘ä»¬èƒ½å¤Ÿè·å–åˆ°agentå¯¹è±¡
            agent = None
            # æ›´å¥å£®åœ°æ£€æŸ¥ agent å®ä¾‹ (å‡è®¾ AgentBase æ˜¯ç¬¬ä¸€ä¸ªå‚æ•°)
            # æ³¨æ„: è¿™é‡Œéœ€è¦çŸ¥é“ AgentBase çš„å®é™…ç±»å®šä¹‰æ‰èƒ½è¿›è¡Œ isinstance æ£€æŸ¥
            # ä¸ºäº†ä¸ä¿®æ”¹ AgentBase å®šä¹‰æœ¬èº«ï¼Œæˆ‘ä»¬æš‚æ—¶ä¿ç•™åŸå§‹çš„æ£€æŸ¥æ–¹å¼
            # æˆ–è€…å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ€»æ˜¯ agent å®ä¾‹
            if args: # å¦‚æœæœ‰ä½ç½®å‚æ•°
                 agent = args[0] # å‡å®šç¬¬ä¸€ä¸ªå‚æ•°æ˜¯self/agentå®ä¾‹


            # ä½¿ç”¨ä¸Šé¢ç¡®å®šçš„ effective_max_tokens
            limit_to_use = effective_max_tokens

            # æ‰§è¡Œå‰æ£€æŸ¥memoryå¤§å°
            if agent is not None and hasattr(agent, 'memory'):
                try:
                    # Attempt to get model name from llm object, fallback otherwise
                    model_name = agent.llm.model_name if hasattr(agent.llm, 'model_name') else "gpt-3.5-turbo"
                    encoding = tiktoken.encoding_for_model(model_name)
                except Exception: # Catch potential errors during encoding lookup
                    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo") # Fallback
                tokens = sum(len(encoding.encode(msg.content)) for msg in agent.memory)

                # å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œå…ˆå‡å°‘memory
                if tokens > limit_to_use * 0.9: # ä½¿ç”¨ limit_to_use
                    _reduce_memory(agent, limit_to_use, encoding) # ä¼ é€’ limit_to_use

            # æ‰§è¡ŒåŸå§‹å‡½æ•°
            result = decorated_func(*args, **kwargs)

            # å¦‚æœæ²¡æœ‰è·å–åˆ°agentï¼Œç›´æ¥è¿”å›ç»“æœ
            if agent is None or not hasattr(agent, 'memory'):
                return result

            # æ‰§è¡Œåå†æ¬¡æ£€æŸ¥å¹¶å‡å°‘memory
            # Use precise token count for check
            try:
                # Attempt to get model name from llm object, fallback otherwise
                model_name = agent.llm.model_name if hasattr(agent.llm, 'model_name') else "gpt-3.5-turbo"
                encoding = tiktoken.encoding_for_model(model_name)
            except Exception: # Catch potential errors during encoding lookup
                encoding = tiktoken.encoding_for_model("gpt-3.5-turbo") # Fallback
            tokens = sum(len(encoding.encode(msg.content)) for msg in agent.memory)
            if tokens > limit_to_use * 0.8: # ä½¿ç”¨ limit_to_use
                _reduce_memory(agent, limit_to_use, encoding) # ä¼ é€’ limit_to_use

            return result

        return wrapper

    # ç”¨äºå†…éƒ¨å‡å°‘memoryçš„å‡½æ•°
    # --- _reduce_memory å‡½æ•°æœ¬èº«ä¸éœ€è¦æ”¹å˜ï¼Œå› ä¸ºå®ƒå·²ç»æ¥å— max_tokens å‚æ•° ---
    def _reduce_memory(agent, max_tokens_limit, encoding): # å‚æ•°åæ”¹ä¸º max_tokens_limit é¿å…æ··æ·†
        """ä½¿ç”¨ tiktoken ç²¾ç¡®è®¡ç®—å¹¶å‡å°‘ agent çš„ memory ä»¥æ»¡è¶³ max_tokens_limit é™åˆ¶ã€‚
        ä¼˜å…ˆä¿ç•™ SystemMessage å’Œæ ‡è®°ä¸º protected=True çš„æ¶ˆæ¯ã€‚
        ç„¶åä»æœ€æ–°çš„æ™®é€šæ¶ˆæ¯å¼€å§‹å‘å‰å¡«å……å‰©ä½™ç©ºé—´ã€‚
        """
        # 1. åˆ†ç¦»å¿…è¦æ¶ˆæ¯å’Œæ™®é€šæ¶ˆæ¯
        essential_messages = []
        other_messages = []
        for msg in agent.memory:
            # ä¿æŠ¤ SystemMessages å’Œæ˜¾å¼æ ‡è®°ä¸º protected çš„æ¶ˆæ¯
            if isinstance(msg, SystemMessage) or (hasattr(msg, 'protected') and msg.protected):
                essential_messages.append(msg)
            else:
                other_messages.append(msg)

        # 2. è®¡ç®—å¿…è¦æ¶ˆæ¯çš„ token
        used_tokens = sum(len(encoding.encode(msg.content)) for msg in essential_messages)

        # 3. è®¡ç®—æ™®é€šæ¶ˆæ¯çš„å¯ç”¨ token
        available_tokens = max_tokens_limit - used_tokens # ä½¿ç”¨ä¼ å…¥çš„é™åˆ¶

        # 4. ä»æœ€æ–°çš„æ™®é€šæ¶ˆæ¯å¼€å§‹ï¼ŒæŒ‰ (Human, AI) å¯¹å¡«å……å‰©ä½™ç©ºé—´
        temp_memory_for_others = []
        tokens_for_others = 0
        human_ai_pairs = []

        # æ”¶é›†æ‰€æœ‰ (Human, AI) å¯¹ï¼Œä»åå¾€å‰
        # ä¿®æ­£å¾ªç¯èŒƒå›´ï¼Œç¡®ä¿æ£€æŸ¥åˆ°ç´¢å¼• 0
        i = len(other_messages) - 1
        while i > 0:
            current_msg = other_messages[i]
            prev_msg = other_messages[i-1]
            # å¯»æ‰¾ AI æ¶ˆæ¯åŠå…¶å‰é¢çš„äººç±»æ¶ˆæ¯
            if isinstance(current_msg, AIMessage) and isinstance(prev_msg, HumanMessage):
                human_ai_pairs.append((prev_msg, current_msg))
                i -= 2 # è·³è¿‡è¿™å¯¹æ¶ˆæ¯ï¼Œç»§ç»­å‘å‰æ‰¾
            else:
                i -= 1 # åªå‘å‰ç§»åŠ¨ä¸€æ­¥

        # ä»æœ€æ–°çš„å¯¹è¯å¯¹å¼€å§‹æ·»åŠ ï¼Œç›´åˆ° token è€—å°½
        # human_ai_pairs åˆ—è¡¨æ˜¯ä»æœ€æ–°åˆ°æœ€æ—§çš„é¡ºåº
        for human_msg, ai_msg in human_ai_pairs:
             try:
                 pair_tokens = len(encoding.encode(human_msg.content)) + len(encoding.encode(ai_msg.content))
             except Exception:
                 pair_tokens = (len(human_msg.content) + len(ai_msg.content)) // 2 # Fallback

             if tokens_for_others + pair_tokens <= available_tokens:
                 # åœ¨åˆ—è¡¨å¼€å¤´æ’å…¥ï¼Œä¿æŒæ—¶é—´é¡ºåºï¼ˆè™½ç„¶æœ€åä¼šåè½¬ï¼‰
                 temp_memory_for_others.insert(0, ai_msg)
                 temp_memory_for_others.insert(0, human_msg)
                 tokens_for_others += pair_tokens
             else:
                 # å¦‚æœè¿™å¯¹æ¶ˆæ¯ä¼šå¯¼è‡´è¶…è¿‡é™åˆ¶ï¼Œåˆ™åœæ­¢æ·»åŠ 
                 break

        # ç¡®ä¿ memory ä¸­è‡³å°‘ä¿ç•™ä¸€å¯¹æœ€æ–°çš„å¯¹è¯ï¼ˆå¦‚æœç©ºé—´å…è®¸ä¸”å­˜åœ¨å¯¹è¯å¯¹ï¼‰
        if not temp_memory_for_others and human_ai_pairs:
            human_msg, ai_msg = human_ai_pairs[0] # è·å–æœ€æ–°çš„ä¸€å¯¹
            try:
                pair_tokens = len(encoding.encode(human_msg.content)) + len(encoding.encode(ai_msg.content))
            except Exception:
                pair_tokens = (len(human_msg.content) + len(ai_msg.content)) // 2 # Fallback

            # åªæœ‰åœ¨è¿™ä¸€å¯¹æœ¬èº«ä¸è¶…è¿‡å¯ç”¨ç©ºé—´æ—¶æ‰æ·»åŠ 
            if pair_tokens <= available_tokens:
                 temp_memory_for_others.extend([human_msg, ai_msg])
                 tokens_for_others += pair_tokens

        # 5. åˆå¹¶å¿…è¦æ¶ˆæ¯å’Œé€‰å®šçš„æ™®é€šæ¶ˆæ¯
        new_memory = essential_messages + temp_memory_for_others

        # 6. ä»…åœ¨å†…å­˜ç¡®å®è¢«ç¼©å‡æ—¶æ›´æ–° agent çš„ memory
        if len(new_memory) < len(agent.memory):
            agent.memory = new_memory
            agent.memory_overloaded = True # æ ‡è®°å‘ç”Ÿäº†ç¼©å‡
            # print(f"Memory reduced. New token count (estimated): {used_tokens + tokens_for_others}") # Optional debug print
        else:
            # å¦‚æœæ²¡æœ‰æ¶ˆæ¯è¢«ç§»é™¤ï¼ˆä¾‹å¦‚ï¼Œå·²åœ¨é™åˆ¶å†…æˆ–åªæœ‰å¿…è¦æ¶ˆæ¯ï¼‰
            # ç¡®ä¿æ ‡å¿—å‡†ç¡®åæ˜ å½“å‰çŠ¶æ€ï¼ˆå¦‚æœä¹‹å‰ä¸º trueï¼‰ã€‚
            # å¦‚æœä»…å¿…è¦æ¶ˆæ¯å°±è¶…é™ï¼Œå†…å­˜å¯èƒ½ä»ç•¥é«˜äº max_tokensã€‚
            current_total_tokens = used_tokens + tokens_for_others
            if current_total_tokens <= max_tokens_limit: # ä½¿ç”¨ä¼ å…¥çš„é™åˆ¶
                 agent.memory_overloaded = False # å¦‚æœç°åœ¨åœ¨é™åˆ¶å†…ï¼Œåˆ™é‡ç½®æ ‡å¿—

    # æ”¯æŒä¸¤ç§è£…é¥°å™¨ä½¿ç”¨æ–¹å¼
    if func is None:
        # å¦‚æœ @reduce_memory_decorator è¿™æ ·è°ƒç”¨ (å¸¦å‚æ•°æˆ–ä¸å¸¦)
        # å°†ç»‘å®šäº† effective_max_tokens çš„ decorator è¿”å›
        return decorator
    else:
        # å¦‚æœ @reduce_memory_decorator è¿™æ ·è°ƒç”¨ (ç›´æ¥è£…é¥°å‡½æ•°)
        # max_tokens ä¼šæ˜¯ None (å› ä¸ºå®ƒæ˜¯å…³é”®å­—å‚æ•°)
        # effective_max_tokens ä¼šæ ¹æ® ç¯å¢ƒå˜é‡ -> å…¨å±€å¸¸é‡ ç¡®å®š
        # å°†åº”ç”¨äº† wrapper çš„å‡½æ•°è¿”å›
        return decorator(func)
#endregion

#region reduce_memory_decorator_compress (æ–°å¢)
def reduce_memory_decorator_compress(func=None, *, max_tokens=None):
    """
    å‹ç¼©ç‰ˆå†…å­˜ç®¡ç†è£…é¥°å™¨ï¼šåœ¨å‡½æ•°æ‰§è¡Œåæ£€æŸ¥memoryå¤§å°ï¼Œå¦‚æœè¶…è¿‡max_tokensåˆ™ä½¿ç”¨å‹ç¼©ç­–ç•¥å‡å°‘memory
    å‹ç¼©ç­–ç•¥ï¼šä¿ç•™protectedæ¶ˆæ¯å’Œæœ€å10æ¡æ¶ˆæ¯ï¼Œå‹ç¼©ä¸­é—´çš„æ¶ˆæ¯
    å¯ä»¥ç›´æ¥è£…é¥°å‡½æ•°æˆ–ä½¿ç”¨å‚æ•°ï¼š@reduce_memory_decorator_compress æˆ– @reduce_memory_decorator_compress(max_tokens=1000)

    åŠ¨æ€è®¾ç½® max_tokens çš„ä¼˜å…ˆçº§:
    1. è£…é¥°å™¨å‚æ•°: @reduce_memory_decorator_compress(max_tokens=value)
    2. ç¯å¢ƒå˜é‡: AGENT_MAX_TOKENS
    3. å…¨å±€å¸¸é‡: MAX_TOKENS
    """
    # 1. ç¡®å®šæœ€ç»ˆç”Ÿæ•ˆçš„ max_tokens å€¼
    effective_max_tokens = MAX_TOKENS # é»˜è®¤ä½¿ç”¨å…¨å±€å¸¸é‡
    try:
        # 2. å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        env_max_tokens = os.environ.get("AGENT_MAX_TOKENS")
        if env_max_tokens is not None:
            effective_max_tokens = int(env_max_tokens)
    except (ValueError, TypeError):
        # å¦‚æœç¯å¢ƒå˜é‡æ ¼å¼é”™è¯¯ï¼Œåˆ™å¿½ç•¥ï¼Œç»§ç»­ä½¿ç”¨å…¨å±€å¸¸é‡
        pass

    # 3. å¦‚æœè£…é¥°å™¨ç›´æ¥æä¾›äº† max_tokens å‚æ•°ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®ƒ
    if max_tokens is not None:
        effective_max_tokens = max_tokens

    def decorator(decorated_func):
        @functools.wraps(decorated_func)
        def wrapper(*args, **kwargs):
            # ç¡®ä¿æˆ‘ä»¬èƒ½å¤Ÿè·å–åˆ°agentå¯¹è±¡
            agent = None
            if args: # å¦‚æœæœ‰ä½ç½®å‚æ•°
                 agent = args[0] # å‡å®šç¬¬ä¸€ä¸ªå‚æ•°æ˜¯self/agentå®ä¾‹

            # ä½¿ç”¨ä¸Šé¢ç¡®å®šçš„ effective_max_tokens
            limit_to_use = effective_max_tokens

            # æ‰§è¡Œå‰æ£€æŸ¥memoryå¤§å°
            if agent is not None and hasattr(agent, 'memory'):
                try:
                    # Attempt to get model name from llm object, fallback otherwise
                    model_name = agent.llm.model_name if hasattr(agent.llm, 'model_name') else "gpt-3.5-turbo"
                    encoding = tiktoken.encoding_for_model(model_name)
                except Exception: # Catch potential errors during encoding lookup
                    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo") # Fallback
                tokens = sum(len(encoding.encode(msg.content)) for msg in agent.memory)

                # å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œå…ˆå‡å°‘memory
                if tokens > limit_to_use * 0.9: # ä½¿ç”¨ limit_to_use
                    _reduce_memory_compress(agent, limit_to_use, encoding) # ä½¿ç”¨å‹ç¼©ç‰ˆæœ¬

            # æ‰§è¡ŒåŸå§‹å‡½æ•°
            result = decorated_func(*args, **kwargs)

            # å¦‚æœæ²¡æœ‰è·å–åˆ°agentï¼Œç›´æ¥è¿”å›ç»“æœ
            if agent is None or not hasattr(agent, 'memory'):
                return result

            # æ‰§è¡Œåå†æ¬¡æ£€æŸ¥å¹¶å‡å°‘memory
            try:
                # Attempt to get model name from llm object, fallback otherwise
                model_name = agent.llm.model_name if hasattr(agent.llm, 'model_name') else "gpt-3.5-turbo"
                encoding = tiktoken.encoding_for_model(model_name)
            except Exception: # Catch potential errors during encoding lookup
                encoding = tiktoken.encoding_for_model("gpt-3.5-turbo") # Fallback
            tokens = sum(len(encoding.encode(msg.content)) for msg in agent.memory)
            if tokens > limit_to_use * 0.8: # ä½¿ç”¨ limit_to_use
                _reduce_memory_compress(agent, limit_to_use, encoding) # ä½¿ç”¨å‹ç¼©ç‰ˆæœ¬

            return result

        return wrapper

    def _reduce_memory_compress(agent, max_tokens_limit, encoding):
        """ä½¿ç”¨å‹ç¼©ç­–ç•¥å‡å°‘ agent çš„ memory ä»¥æ»¡è¶³ max_tokens_limit é™åˆ¶ã€‚
        ç­–ç•¥ï¼šä¿ç•™ protected æ¶ˆæ¯å’Œæœ€å10æ¡æ¶ˆæ¯ï¼Œå‹ç¼©ä¸­é—´çš„æ¶ˆæ¯ã€‚
        """
        try:
            # ç¡®ä¿ä»å½“å‰ç›®å½•å¯¼å…¥
            from .message_compress import compress_messages
        except ImportError:
            # å¤‡ç”¨å¯¼å…¥æ–¹æ¡ˆ
            from message_compress import compress_messages
        
        print(f"âœ… æˆåŠŸå¯¼å…¥ compress_messages: {compress_messages.__module__}")
        
        # 1. åˆ†ç¦»protectedæ¶ˆæ¯å’Œæ™®é€šæ¶ˆæ¯
        protected_messages = []
        regular_messages = []
        
        for msg in agent.memory:
            # ä¿æŠ¤ SystemMessages å’Œæ˜¾å¼æ ‡è®°ä¸º protected çš„æ¶ˆæ¯
            if isinstance(msg, SystemMessage) or (hasattr(msg, 'protected') and msg.protected):
                protected_messages.append(msg)
            else:
                regular_messages.append(msg)

        # 2. è®¡ç®—protectedæ¶ˆæ¯çš„tokenæ•°
        protected_tokens = sum(len(encoding.encode(msg.content)) for msg in protected_messages)
        
        # 3. è®¡ç®—å¯ç”¨äºæ™®é€šæ¶ˆæ¯çš„tokenæ•°
        available_tokens = max_tokens_limit - protected_tokens
        
        print(f"\nğŸ”„ å¼€å§‹æ¶ˆæ¯å‹ç¼©...")
        print(f"ğŸ“Š åŸå§‹æ¶ˆæ¯ç»Ÿè®¡: æ€»æ¶ˆæ¯ {len(agent.memory)} æ¡ (ä¿æŠ¤æ¶ˆæ¯ {len(protected_messages)} æ¡, æ™®é€šæ¶ˆæ¯ {len(regular_messages)} æ¡)")
        print(f"ğŸ¯ Tokené™åˆ¶: {max_tokens_limit}, ä¿æŠ¤æ¶ˆæ¯å ç”¨: {protected_tokens}, æ™®é€šæ¶ˆæ¯å¯ç”¨: {available_tokens}")
        
        # 4. å¤„ç†æ™®é€šæ¶ˆæ¯
        try:
            # ç›´æ¥è°ƒç”¨compress_messagesï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†ä¿ç•™æœ€å10æ¡æ¶ˆæ¯çš„é€»è¾‘
            final_regular_messages = compress_messages(regular_messages)
        except Exception as e:
            # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œfallbackåˆ°åŸæœ‰çš„tokené™åˆ¶ç­–ç•¥
            print(f"âŒ å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨fallbackç­–ç•¥: {e}")
            final_regular_messages = _fallback_token_strategy(regular_messages, available_tokens, encoding)
        
        # 5. æ£€æŸ¥æœ€ç»ˆç»“æœæ˜¯å¦ç¬¦åˆtokené™åˆ¶
        final_regular_tokens = sum(len(encoding.encode(msg.content)) for msg in final_regular_messages)
        
        # å¦‚æœä»ç„¶è¶…è¿‡é™åˆ¶ï¼Œä½¿ç”¨fallbackç­–ç•¥
        if final_regular_tokens > available_tokens:
            print(f"âš ï¸  å‹ç¼©åä»è¶…è¿‡é™åˆ¶ï¼Œä½¿ç”¨fallbackç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–")
            final_regular_messages = _fallback_token_strategy(final_regular_messages, available_tokens, encoding)
        
        # 6. ç»„åˆæœ€ç»ˆçš„memory
        new_memory = protected_messages + final_regular_messages
        
        # 7. æ›´æ–°agentçš„memory
        original_length = len(agent.memory)
        if len(new_memory) < original_length:
            agent.memory = new_memory
            agent.memory_overloaded = True # æ ‡è®°å‘ç”Ÿäº†ç¼©å‡
            print(f"âœ… Memoryå‹ç¼©å®Œæˆ! åŸå§‹: {original_length} æ¡æ¶ˆæ¯ â†’ å‹ç¼©å: {len(new_memory)} æ¡æ¶ˆæ¯")
            print(f"ğŸ“ˆ æœ€ç»ˆæ„æˆ: ä¿æŠ¤æ¶ˆæ¯ {len(protected_messages)} æ¡ + æ™®é€šæ¶ˆæ¯ {len(final_regular_messages)} æ¡\n")
        else:
            # å¦‚æœæ²¡æœ‰æ¶ˆæ¯è¢«ç§»é™¤ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®æ ‡å¿—
            total_tokens = protected_tokens + final_regular_tokens
            if total_tokens <= max_tokens_limit:
                agent.memory_overloaded = False
                print(f"âœ… Memoryåœ¨é™åˆ¶èŒƒå›´å†…ï¼Œæ— éœ€å‹ç¼©\n")

    def _fallback_token_strategy(messages, available_tokens, encoding):
        """Fallbackç­–ç•¥ï¼šåŸºäºtokené™åˆ¶é€‰æ‹©æ¶ˆæ¯"""
        selected_messages = []
        used_tokens = 0
        
        # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹å‘å‰é€‰æ‹©
        for msg in reversed(messages):
            try:
                msg_tokens = len(encoding.encode(msg.content))
            except Exception:
                msg_tokens = len(msg.content) // 2 # Fallback estimation
            
            if used_tokens + msg_tokens <= available_tokens:
                selected_messages.insert(0, msg) # ä¿æŒæ—¶é—´é¡ºåº
                used_tokens += msg_tokens
            else:
                break
        
        return selected_messages

    # æ”¯æŒä¸¤ç§è£…é¥°å™¨ä½¿ç”¨æ–¹å¼
    if func is None:
        return decorator
    else:
        return decorator(func)
#endregion

#region AgentBase
class AgentBase:
    '''
    æ™ºèƒ½ä½“åŸºç±»
    chat å’Œ execute çš„åˆ’åˆ†ç¡®å®åæ˜ äº†äººç±»ï¼ˆä»¥åŠå¯èƒ½æ˜¯æ™ºèƒ½ä½“ï¼‰ä¸ä¸–ç•Œäº¤äº’çš„ä¸¤ç§åŸºæœ¬æ–¹å¼ï¼šé€šè¿‡è¯­è¨€è¿›è¡Œäº¤æµå’Œæ€è€ƒ (chat)ï¼Œä»¥åŠé€šè¿‡è¡ŒåŠ¨æ”¹å˜æˆ–æ„ŸçŸ¥ä¸–ç•Œ (execute)ã€‚
    æ„å‘æ€§ (Intentionality): å“²å­¦ä¸­æŒ‡å¿ƒçµçŠ¶æ€ï¼ˆå¦‚ä¿¡å¿µã€æ¬²æœ›ã€æ„å›¾ï¼‰æŒ‡å‘æˆ–å…³äºä¸–ç•Œä¸­å¯¹è±¡çš„ç‰¹æ€§ã€‚
    chat å¯ä»¥è¢«çœ‹ä½œæ˜¯å¤„ç†å’Œè¡¨è¾¾æ„å‘æ€§çŠ¶æ€çš„ä¸»è¦æ–¹å¼ï¼Œä¾‹å¦‚ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œè¡¨è¾¾æ™ºèƒ½ä½“çš„"ä¿¡å¿µ"ï¼ˆçŸ¥è¯†åº“ï¼‰æˆ–"æ„å›¾"ï¼ˆä¸‹ä¸€æ­¥è®¡åˆ’ï¼‰ã€‚
    execute åˆ™æ˜¯å°†è¿™äº›æ„å‘æ€§çŠ¶æ€ï¼ˆç‰¹åˆ«æ˜¯è¡ŒåŠ¨æ„å›¾ï¼‰è½¬åŒ–ä¸ºå®é™…è¡ŒåŠ¨çš„æœºåˆ¶ã€‚
    '''
    def __init__(self, llm:BaseChatModel=None, system_message:str=None):
        self.llm = llm
        self.system_message = system_message
        self.memory = []
        # self.protected_messages = [] # å·²ç§»é™¤ï¼Œä¿æŠ¤ç°åœ¨æ˜¯æ¶ˆæ¯å¯¹è±¡çš„ä¸€ä¸ªå±æ€§
        self.api_specification = None
        self.name = None
        self.memory_overloaded = False  # æ·»åŠ å†…å­˜è¶…è½½æ ‡è®°
        
        if system_message:
            system_msg = SystemMessage(system_message)
            system_msg.protected = True # å°†ç³»ç»Ÿæ¶ˆæ¯æ ‡è®°ä¸ºå—ä¿æŠ¤
            self.memory = [system_msg]
            # self.protected_messages = [system_msg] # å·²ç§»é™¤

    def loadKnowledge(self, knowledge:str):
        '''
        åŠ è½½çŸ¥è¯†åˆ°agentçš„è®°å¿†ä¸­ï¼Œç¡®ä¿æ¶ˆæ¯äº¤æ›¿
        Args:
            knowledge: çŸ¥è¯†
        '''
        human_msg = HumanMessage(knowledge)
        human_msg.protected = True # å°†åŠ è½½çš„çŸ¥è¯†æ ‡è®°ä¸ºå—ä¿æŠ¤
        ai_msg = AIMessage('ok')
        ai_msg.protected = True # å°†å¯¹åº”çš„ AI å“åº”ä¹Ÿæ ‡è®°ä¸ºå—ä¿æŠ¤
        # ç¡®ä¿æ¶ˆæ¯äº¤æ›¿
        if self.memory and isinstance(self.memory[-1], HumanMessage):
            self.memory.append(ai_msg)
            self.memory.append(human_msg)
        else:
            self.memory.append(human_msg)
            self.memory.append(ai_msg)
        # self.protected_messages.extend([human_msg, ai_msg]) # å·²ç§»é™¤
    
    def calculate_memory_tokens(self, model_name: str = "gpt-3.5-turbo") -> int:
        '''
        è®¡ç®—memoryçš„tokenæ•°é‡
        Args:
            model_name: æ¨¡å‹åç§°
        Returns:
            int: tokenæ•°é‡
        '''
        encoding = tiktoken.encoding_for_model(model_name)
        return sum(len(encoding.encode(msg.content)) for msg in self.memory)

    # @reduce_memory_decorator
    def execute_stream(self, instruction:str=None) -> Iterator[object]:
        '''
        æ‰§è¡Œæµå¼æ–¹æ³•
        Args:
            instruction: æ‰§è¡ŒæŒ‡ä»¤
        Returns:
            Iterator[object]: æµå¼ç»“æœ
            æµå¼ç»“æœçš„ç»“æ„æ˜¯è¿‡ç¨‹åŠ çŠ¶æ€ã€‚æµçš„å‰é¢æ˜¯è¿‡ç¨‹ï¼Œæµçš„æœ€åä¸€ä¸ªå…ƒç´ æ˜¯æœ€ç»ˆçŠ¶æ€ã€‚
        '''
        pass

    # @reduce_memory_decorator
    def execute_sync(self, instruction:str=None) -> Result:
        '''
        åŒæ­¥æ‰§è¡Œæ–¹æ³•
        Args:
            instruction: æ‰§è¡ŒæŒ‡ä»¤
        Returns:
            Result: æ‰§è¡Œç»“æœ
        '''
        pass

    # @reduce_memory_decorator
    def chat_stream(self, message: str, response_format: Optional[Dict] = None) -> Iterator[object]:
        '''
        æµå¼èŠå¤©æ–¹æ³•
        Args:
            message: èŠå¤©æ¶ˆæ¯
        Returns:
            Iterator[object]: æµå¼ç»“æœï¼ŒåŒ…æ‹¬æ–‡æœ¬ç‰‡æ®µå’Œæœ€ç»ˆçš„Resultå¯¹è±¡
        '''
        human_msg = HumanMessage(message)
        self.memory.append(human_msg)
        content = ''
        # Only pass response_format if it's not None to avoid API compatibility issues
        stream_kwargs = {}
        if response_format is not None:
            stream_kwargs['response_format'] = response_format
        chunks = self.llm.stream(self.memory, **stream_kwargs)
        for chunk in chunks:
            content += chunk.content
            yield chunk.content
        ai_msg = AIMessage(content)
        self.memory.append(ai_msg)
        yield Result(True, "", "", None, content)

    # @reduce_memory_decorator
    def chat_sync(self, message: str, response_format: Optional[Dict] = None) -> Result:
        '''
        åŒæ­¥èŠå¤©æ–¹æ³•ï¼Œç¡®ä¿æ¶ˆæ¯äº¤æ›¿
        Args:
            message: èŠå¤©æ¶ˆæ¯
        Returns:
            Result: èŠå¤©ç»“æœ
        '''
        human_msg = HumanMessage(message)
        self.memory.append(human_msg)
        # Only pass response_format if it's not None to avoid API compatibility issues
        invoke_kwargs = {}
        if response_format is not None:
            invoke_kwargs['response_format'] = response_format
        content = self.llm.invoke(self.memory, **invoke_kwargs).content
        ai_msg = AIMessage(content)
        self.memory.append(ai_msg)
        return Result(True, "", "", None, content)

    def classify_instruction(self, instruction: str) -> bool:
        '''
        åˆ¤æ–­ç”¨æˆ·æŒ‡ä»¤æ˜¯"æ€ç»´"è¿˜æ˜¯"åŠ¨ä½œ"
        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
        Returns:
            bool: å¦‚æœæ˜¯åŠ¨ä½œç±»å‹è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        '''
        # æ„å»ºåˆ†ç±»æç¤ºè¯
        classification_prompt = """# è¾“å…¥ç±»å‹åˆ¤æ–­æŒ‡å—

ä½œä¸ºæ™ºèƒ½ä½“ï¼Œæˆ‘éœ€è¦å‡†ç¡®åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯"æ€ç»´"è¿˜æ˜¯"åŠ¨ä½œ"ï¼Œä»¥ä¾¿é€‰æ‹©æ­£ç¡®çš„å¤„ç†æ–¹æ³•ã€‚

## æ€ç»´ï¼ˆThoughtï¼‰
- å®šä¹‰ï¼šæ€ç»´æ˜¯æŒ‡ä»…ä¸æ™ºèƒ½ä½“çš„å†…éƒ¨è®°å¿†ï¼ˆmemoryï¼‰äº¤äº’ï¼Œä¸äº§ç”Ÿä»»ä½•å¤–éƒ¨å‰¯ä½œç”¨çš„æ“ä½œ
- ç‰¹ç‚¹ï¼š
  - åªè¯»å–å’Œä¿®æ”¹æ™ºèƒ½ä½“çš„å†…éƒ¨çŠ¶æ€ï¼ˆè®°å¿†ï¼‰
  - ä¸è°ƒç”¨ä»»ä½•å¤–éƒ¨å·¥å…·æˆ–API
  - ä¸ä¿®æ”¹å¤–éƒ¨ä¸–ç•Œ
  - ç»“æœä»…ä¾èµ–äºè¾“å…¥å’Œå½“å‰è®°å¿†çŠ¶æ€
- å¤„ç†æ–¹æ³•ï¼šä½¿ç”¨æ€ç»´å¤„ç†æ–¹æ³•
- ç¤ºä¾‹ï¼š
  - "è¯·æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯"
  - "ä½ èƒ½è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å—ï¼Ÿ"
  - "æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„è®¨è®ºï¼Œä½ è®¤ä¸ºå“ªç§æ–¹æ¡ˆæ›´å¥½ï¼Ÿ"

## åŠ¨ä½œï¼ˆActionï¼‰
- å®šä¹‰ï¼šåŠ¨ä½œæ˜¯æŒ‡ä¼šè°ƒç”¨å·¥å…·å¯¹å¤–éƒ¨ä¸–ç•Œäº§ç”Ÿå‰¯ä½œç”¨çš„æ“ä½œ
- ç‰¹ç‚¹ï¼š
  - éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·ã€APIæˆ–æ‰§è¡Œå‘½ä»¤
  - ä¼šä¿®æ”¹å¤–éƒ¨ä¸–ç•Œï¼ˆå¦‚æ–‡ä»¶ç³»ç»Ÿã€ç½‘ç»œè¯·æ±‚ã€æ•°æ®åº“ç­‰ï¼‰
  - å¯èƒ½äº§ç”ŸæŒä¹…åŒ–çš„å˜åŒ–
  - ç»“æœä¸ä»…ä¾èµ–äºè¾“å…¥å’Œè®°å¿†ï¼Œè¿˜ä¾èµ–äºå¤–éƒ¨ç¯å¢ƒ
- å¤„ç†æ–¹æ³•ï¼šä½¿ç”¨åŠ¨ä½œå¤„ç†æ–¹æ³•
- ç¤ºä¾‹ï¼š
  - "è¯·åˆ›å»ºä¸€ä¸ªåä¸º'project'çš„æ–°æ–‡ä»¶å¤¹"
  - "æœç´¢å…³äºäººå·¥æ™ºèƒ½çš„æœ€æ–°ç ”ç©¶è®ºæ–‡"
  - "å°†è¿™æ®µä»£ç ä¿å­˜åˆ°æ–‡ä»¶ä¸­"
  - "è¿æ¥åˆ°æ•°æ®åº“å¹¶æ‰§è¡ŒæŸ¥è¯¢"

## åˆ¤æ–­æ ‡å‡†
1. å¦‚æœç”¨æˆ·è¯·æ±‚æ¶‰åŠå¤–éƒ¨å·¥å…·è°ƒç”¨ã€æ–‡ä»¶æ“ä½œã€ç½‘ç»œè¯·æ±‚æˆ–ä»»ä½•ä¼šæ”¹å˜å¤–éƒ¨ä¸–ç•Œçš„æ“ä½œï¼Œåˆ™è§†ä¸º"åŠ¨ä½œ"
2. å¦‚æœç”¨æˆ·è¯·æ±‚ä»…æ¶‰åŠä¿¡æ¯æ£€ç´¢ã€åˆ†æã€æ€»ç»“æˆ–ä¸æ™ºèƒ½ä½“è®°å¿†çš„äº¤äº’ï¼Œåˆ™è§†ä¸º"æ€ç»´"
3. å½“ä¸ç¡®å®šæ—¶ï¼Œä¼˜å…ˆè€ƒè™‘æ˜¯å¦ä¸º"åŠ¨ä½œ"ï¼Œå› ä¸ºé”™è¯¯åœ°å°†åŠ¨ä½œå½“ä½œæ€ç»´å¤„ç†å¯èƒ½å¯¼è‡´æ— æ³•å®Œæˆç”¨æˆ·è¯·æ±‚

## å“åº”æ ¼å¼
è¯·åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·æŒ‡ä»¤æ˜¯å¦ä¸º"åŠ¨ä½œ"ç±»å‹ã€‚
å¦‚æœæ˜¯åŠ¨ä½œç±»å‹ï¼Œè¯·åªè¿”å›"true"ã€‚
å¦‚æœä¸æ˜¯åŠ¨ä½œç±»å‹ï¼ˆå³æ€ç»´ç±»å‹ï¼‰ï¼Œè¯·åªè¿”å›"false"ã€‚
ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼Œä¸è¦è§£é‡ŠåŸå› ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ–‡å­—ã€‚

è¯·åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·æŒ‡ä»¤çš„ç±»å‹ï¼š
{instruction}
"""
        
        # åˆ›å»ºä¸´æ—¶æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸ä¿®æ”¹åŸå§‹è®°å¿†
        temp_messages = []
        human_msg = HumanMessage(classification_prompt.format(instruction=instruction))
        temp_messages.append(human_msg)
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­
        response = self.llm.invoke(temp_messages)
        
        # è§£æå“åº”
        try:
            # æå–å“åº”ä¸­çš„true/false
            content = response.content.lower().strip()
            return "true" in content
        except Exception as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼ï¼ˆé»˜è®¤ä¸ºåŠ¨ä½œç±»å‹ï¼‰
            return True
    
#endregion
