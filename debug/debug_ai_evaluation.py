#!/usr/bin/env python3
"""
è°ƒè¯•AIè¯„ä¼°é€»è¾‘
==============

æ£€æŸ¥ä¸ºä»€ä¹ˆAIè¯„ä¼°å™¨å¯¹é€šè¿‡çš„æµ‹è¯•è¿”å›å¤±è´¥ã€‚
"""

import subprocess
import sys
from static_workflow.result_evaluator import MockTestResultEvaluator
from agent_base import Result


def debug_ai_evaluation():
    """è°ƒè¯•AIè¯„ä¼°é€»è¾‘"""
    
    print("ğŸ” è°ƒè¯•AIè¯„ä¼°é€»è¾‘")
    print("=" * 60)
    
    # è¿è¡Œå®é™…æµ‹è¯•
    result = subprocess.run([
        sys.executable, "-m", "unittest", "test_calculator.py", "-v"
    ], capture_output=True, text=True)
    
    print("ğŸ“Š å®é™…æµ‹è¯•ç»“æœ:")
    print(f"Return code: {result.returncode}")
    print(f"STDOUT: {repr(result.stdout)}")
    print(f"STDERR: {repr(result.stderr)}")
    
    # åˆ›å»ºResultå¯¹è±¡æ¨¡æ‹Ÿå·¥ä½œæµä¸­çš„æƒ…å†µ
    test_result = Result(
        success=(result.returncode == 0),
        code="python -m unittest test_calculator.py -v",
        stdout=result.stdout,
        stderr=result.stderr,
        return_value=result.returncode
    )
    
    print(f"\nğŸ“¦ Resultå¯¹è±¡:")
    print(f"success: {test_result.success}")
    print(f"stdout: {repr(test_result.stdout)}")
    print(f"stderr: {repr(test_result.stderr)}")
    print(f"return_value: {test_result.return_value}")
    
    # ä½¿ç”¨MockTestResultEvaluatorè¯„ä¼°
    evaluator = MockTestResultEvaluator()
    
    # æ–¹å¼1: ç›´æ¥ä¼ é€’å‚æ•°
    evaluation1 = evaluator.evaluate_test_result(
        result_stdout=test_result.stdout,
        result_stderr=test_result.stderr,
        result_return_value=str(test_result.return_value)
    )
    
    print(f"\nğŸ¤– æ–¹å¼1 - ç›´æ¥å‚æ•°è¯„ä¼°:")
    print(f"é€šè¿‡: {evaluation1['passed']}")
    print(f"ç½®ä¿¡åº¦: {evaluation1['confidence']}")
    print(f"ç†ç”±: {evaluation1['reason']}")
    
    # æ–¹å¼2: ä½¿ç”¨Resultå¯¹è±¡çš„å­—æ®µ
    evaluation2 = evaluator.evaluate_test_result(
        result_code=test_result.code,
        result_stdout=test_result.stdout,
        result_stderr=test_result.stderr,
        result_return_value=test_result.return_value
    )
    
    print(f"\nğŸ¤– æ–¹å¼2 - Resultå¯¹è±¡å­—æ®µè¯„ä¼°:")
    print(f"é€šè¿‡: {evaluation2['passed']}")
    print(f"ç½®ä¿¡åº¦: {evaluation2['confidence']}")
    print(f"ç†ç”±: {evaluation2['reason']}")
    
    # æ£€æŸ¥å·¥ä½œæµä¸­å¯èƒ½çš„æ•°æ®ä¼ é€’é—®é¢˜
    print(f"\nğŸ” æ•°æ®åˆ†æ:")
    combined_output = f"{test_result.stdout} {test_result.stderr} {test_result.return_value}".lower()
    print(f"åˆå¹¶è¾“å‡º: {repr(combined_output)}")
    
    fail_indicators = [
        "failed", "error", "exception", "traceback", 
        "assertion error", "test failed", "0 passed",
        "failure", "fatal", "critical", "1 failed", 
        "2 failed", "3 failed", "4 failed", "5 failed"
    ]
    
    success_indicators = [
        "passed", "success", "ok", "all tests passed",
        "build successful", "completed successfully"
    ]
    
    found_fail = [ind for ind in fail_indicators if ind in combined_output]
    found_success = [ind for ind in success_indicators if ind in combined_output]
    
    print(f"æ‰¾åˆ°çš„å¤±è´¥æŒ‡æ ‡: {found_fail}")
    print(f"æ‰¾åˆ°çš„æˆåŠŸæŒ‡æ ‡: {found_success}")
    print(f"åŒ…å«'0 failed': {'0 failed' in combined_output}")
    
    # æ£€æŸ¥AIè¯„ä¼°å™¨å†…éƒ¨é€»è¾‘
    print(f"\nğŸ§® è¯„ä¼°é€»è¾‘æ£€æŸ¥:")
    has_failures = any(indicator in combined_output for indicator in fail_indicators)
    has_success = any(indicator in combined_output for indicator in success_indicators)
    has_zero_failed = "0 failed" in combined_output
    
    print(f"has_failures (before 0 failed check): {has_failures}")
    print(f"has_success: {has_success}")
    print(f"has_zero_failed: {has_zero_failed}")
    
    if has_zero_failed:
        has_failures = False
        print(f"has_failures (after 0 failed check): {has_failures}")
    
    if has_failures:
        expected_result = False
        expected_reason = "æ£€æµ‹åˆ°å¤±è´¥æŒ‡æ ‡"
    elif has_success:
        expected_result = True
        expected_reason = "æ£€æµ‹åˆ°æˆåŠŸæŒ‡æ ‡"
    else:
        expected_result = True
        expected_reason = "é»˜è®¤åˆ¤æ–­ä¸ºé€šè¿‡"
    
    print(f"é¢„æœŸç»“æœ: {expected_result}")
    print(f"é¢„æœŸç†ç”±: {expected_reason}")
    
    # éªŒè¯è¯„ä¼°æ˜¯å¦æ­£ç¡®
    if evaluation1['passed'] == expected_result:
        print(f"\nâœ… AIè¯„ä¼°é€»è¾‘æ­£ç¡®")
    else:
        print(f"\nâŒ AIè¯„ä¼°é€»è¾‘æœ‰é—®é¢˜")
        print(f"   è¯„ä¼°ç»“æœ: {evaluation1['passed']}")
        print(f"   é¢„æœŸç»“æœ: {expected_result}")


if __name__ == "__main__":
    debug_ai_evaluation()