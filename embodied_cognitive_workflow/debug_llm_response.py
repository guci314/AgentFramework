#!/usr/bin/env python3
"""
è°ƒè¯•LLMå“åº”é—®é¢˜
æ£€æŸ¥ä¸ºä»€ä¹ˆLLMè¿”å›ç©ºå“åº”
"""

import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    from langchain_openai import ChatOpenAI
    print("âœ… æˆåŠŸå¯¼å…¥ChatOpenAI")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def test_llm_response():
    """æµ‹è¯•LLMå“åº”"""
    print("ğŸ” æµ‹è¯•LLMå“åº”")
    print("=" * 40)
    
    try:
        # åˆå§‹åŒ–LLM
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3,
            max_tokens=1000
        )
        
        # æµ‹è¯•ç®€å•çš„ç­–ç•¥ä¼˜åŒ–æç¤º
        test_prompt = """
        åŸºäºå½“å‰æ€§èƒ½æŒ‡æ ‡å’Œä¸Šä¸‹æ–‡ï¼Œä¼˜åŒ–è®¤çŸ¥ç­–ç•¥ï¼š
        
        å½“å‰æ€§èƒ½ï¼š{"efficiency": 0.8, "accuracy": 0.9}
        ä¸Šä¸‹æ–‡ï¼š{"task": "æµ‹è¯•ä»»åŠ¡", "complexity": "ä¸­ç­‰"}
        ç›®æ ‡ï¼š["æé«˜æ•ˆç‡", "ä¿æŒå‡†ç¡®æ€§"]
        
        è¯·åˆ†æå¹¶æä¾›ï¼š
        1. å½“å‰ç­–ç•¥çš„ç“¶é¢ˆåˆ†æ
        2. ä¼˜åŒ–å»ºè®®å’Œå…·ä½“ç­–ç•¥
        3. é¢„æœŸæ”¹è¿›æ•ˆæœ
        4. å®æ–½ä¼˜å…ˆçº§
        
        è¿”å›JSONæ ¼å¼ï¼š
        {
            "bottleneck_analysis": "ç“¶é¢ˆåˆ†æ",
            "optimization_strategies": ["ç­–ç•¥1", "ç­–ç•¥2"],
            "expected_improvement": "é¢„æœŸæ”¹è¿›",
            "implementation_priority": "high/medium/low",
            "confidence_score": 0.0-1.0
        }
        """
        
        print("ğŸ“¤ å‘é€æµ‹è¯•æç¤º...")
        print(f"æç¤ºé•¿åº¦: {len(test_prompt)} å­—ç¬¦")
        
        # è°ƒç”¨LLM
        response = llm.invoke(test_prompt)
        
        print("ğŸ“¥ LLMå“åº”:")
        print(f"å“åº”ç±»å‹: {type(response)}")
        print(f"å“åº”å†…å®¹: '{response.content}'")
        print(f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        
        if not response.content.strip():
            print("âš ï¸ å“åº”ä¸ºç©ºï¼")
            return False
        
        # å°è¯•è§£æJSON
        import json
        try:
            parsed = json.loads(response.content.strip())
            print("âœ… JSONè§£ææˆåŠŸ:")
            print(json.dumps(parsed, ensure_ascii=False, indent=2))
            return True
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print("åŸå§‹å“åº”å†…å®¹:")
            print(repr(response.content))
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_simple_prompt():
    """æµ‹è¯•ç®€å•æç¤º"""
    print("\nğŸ” æµ‹è¯•ç®€å•æç¤º")
    print("=" * 40)
    
    try:
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3,
            max_tokens=500
        )
        
        simple_prompt = "è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼šä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿæ ¼å¼ï¼š{\"greeting\": \"å›åº”\", \"weather\": \"å¤©æ°”æè¿°\"}"
        
        print("ğŸ“¤ å‘é€ç®€å•æç¤º...")
        response = llm.invoke(simple_prompt)
        
        print("ğŸ“¥ ç®€å•æç¤ºå“åº”:")
        print(f"å“åº”å†…å®¹: '{response.content}'")
        print(f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        
        if response.content.strip():
            print("âœ… ç®€å•æç¤ºæœ‰å“åº”")
            return True
        else:
            print("âŒ ç®€å•æç¤ºä¹Ÿæ— å“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ ç®€å•æç¤ºæµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    print("ğŸ”§ LLMå“åº”è°ƒè¯•å·¥å…·")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    import os
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
    else:
        print("âœ… æ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
    
    # æµ‹è¯•ç®€å•æç¤º
    simple_success = test_simple_prompt()
    
    # æµ‹è¯•å¤æ‚æç¤º
    complex_success = test_llm_response()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"ç®€å•æç¤º: {'âœ… æˆåŠŸ' if simple_success else 'âŒ å¤±è´¥'}")
    print(f"å¤æ‚æç¤º: {'âœ… æˆåŠŸ' if complex_success else 'âŒ å¤±è´¥'}")
    
    if not simple_success:
        print("\nğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥OPENAI_API_KEYæ˜¯å¦æ­£ç¡®è®¾ç½®")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. æ£€æŸ¥APIé¢åº¦")
    elif not complex_success:
        print("\nğŸ’¡ å»ºè®®:")
        print("1. ç®€åŒ–æç¤ºè¯")
        print("2. å¢åŠ max_tokens")
        print("3. è°ƒæ•´temperatureå‚æ•°")