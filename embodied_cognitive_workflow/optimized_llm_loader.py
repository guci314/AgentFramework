#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹åŠ è½½å™¨ - è§£å†³\1åŠ è½½ç¼“æ…¢é—®é¢˜

ä½¿ç”¨æ‡’åŠ è½½æ¨¡å¼ï¼Œåªåœ¨å®é™…ä½¿ç”¨æ—¶æ‰åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼Œ
å¤§å¹…æå‡æ¨¡å—å¯¼å…¥é€Ÿåº¦ã€‚
"""

import os
from functools import lru_cache
from typing import Dict, Optional
import httpx
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®ä»£ç†
try:
    http_client = httpx.Client(
        proxies="http://127.0.0.1:7890",
        timeout=60.0
    )
except Exception:
    # å¦‚æœä»£ç†é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å®¢æˆ·ç«¯
    http_client = httpx.Client(timeout=60.0)

class LLMLoader:
    """æ‡’åŠ è½½è¯­è¨€æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self._models: Dict[str, ChatOpenAI] = {}
        self._configs = {
            # Google Gemini ç³»åˆ—
            'gemini_2_5_flash': {
                'model': 'models/gemini-2.5-flash',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
                'api_key_env': 'GEMINI_API_KEY',
                'max_tokens': 4096
            },
            'gemini_2_5_pro': {
                'model': 'gemini-2.5-pro',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
                'api_key_env': 'GEMINI_API_KEY',
                'max_tokens': 4096
            },
            'gemini_2_flash': {
                'model': 'gemini-2.0-flash',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',
                'api_key_env': 'GEMINI_API_KEY',
                'max_tokens': 4096
            },
            
            # DeepSeek ç³»åˆ—
            'deepseek_v3': {
                'model': 'deepseek-ai/DeepSeek-V3',
                'base_url': 'https://api.siliconflow.cn/v1',
                'api_key_env': 'SILICONFLOW_API_KEY',
                'max_tokens': 4096
            },
            'deepseek_r1': {
                'model': 'deepseek-ai/DeepSeek-R1',
                'base_url': 'https://api.siliconflow.cn/v1',
                'api_key_env': 'SILICONFLOW_API_KEY',
                'max_tokens': 8192
            },
            'deepseek_chat': {
                'model': 'deepseek-chat',
                'base_url': 'https://api.deepseek.com/v1',
                'api_key_env': 'DEEPSEEK_API_KEY',
                'max_tokens': 4096
            },
            
            # Qwen ç³»åˆ—
            'qwen_qwq_32b': {
                'model': 'Qwen/QwQ-32B',
                'base_url': 'https://api.siliconflow.cn/v1',
                'api_key_env': 'SILICONFLOW_API_KEY',
                'max_tokens': 8192
            },
            'qwen_2_5_coder_32b': {
                'model': 'Qwen/Qwen2.5-Coder-32B-Instruct',
                'base_url': 'https://api.siliconflow.cn/v1',
                'api_key_env': 'SILICONFLOW_API_KEY',
                'max_tokens': 4096
            }
        }
    
    @lru_cache(maxsize=None)
    def get_model(self, model_name: str) -> Optional[ChatOpenAI]:
        """
        æ‡’åŠ è½½è·å–è¯­è¨€æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            ChatOpenAIå®ä¾‹ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨æˆ–APIå¯†é’¥ç¼ºå¤±åˆ™è¿”å›None
        """
        if model_name in self._models:
            return self._models[model_name]
        
        if model_name not in self._configs:
            print(f"âš ï¸  æœªçŸ¥çš„æ¨¡å‹åç§°: {model_name}")
            return None
        
        config = self._configs[model_name]
        api_key = os.getenv(config['api_key_env'])
        
        if not api_key:
            print(f"âš ï¸  ç¼ºå°‘APIå¯†é’¥ç¯å¢ƒå˜é‡: {config['api_key_env']}")
            return None
        
        try:
            model = ChatOpenAI(
                temperature=0,
                model=config['model'],
                base_url=config['base_url'],
                api_key=api_key,
                max_tokens=config.get('max_tokens', 4096),
                http_client=http_client
            )
            self._models[model_name] = model
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {e}")
            return None
    
    def list_available_models(self) -> Dict[str, str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®"""
        return {name: config['model'] for name, config in self._configs.items()}
    
    def clear_cache(self):
        """æ¸…ç©ºæ¨¡å‹ç¼“å­˜"""
        self._models.clear()
        self.get_model.cache_clear()
        print("ğŸ—‘ï¸  æ¨¡å‹ç¼“å­˜å·²æ¸…ç©º")

# å…¨å±€æ‡’åŠ è½½ç®¡ç†å™¨
_llm_loader = LLMLoader()

# ä¾¿æ·è®¿é—®å‡½æ•°
def get_llm(model_name: str) -> Optional[ChatOpenAI]:
    """è·å–è¯­è¨€æ¨¡å‹çš„ä¾¿æ·å‡½æ•°"""
    return _llm_loader.get_model(model_name)

# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›æœ€å¸¸ç”¨çš„æ¨¡å‹ä½œä¸ºå±æ€§
@property
def get_model("gemini_2_5_flash")():
    """æœ€å¸¸ç”¨çš„Gemini Flashæ¨¡å‹"""
    return get_llm('gemini_2_5_flash')

@property  
def get_model("deepseek_chat")_v3():
    """DeepSeek V3æ¨¡å‹"""
    return get_llm('deepseek_v3')

@property
def llm_qwen_qwq_32b():
    """Qwen QwQ 32Bæ¨¡å‹"""
    return get_llm('qwen_qwq_32b')

# æ¨¡å—çº§åˆ«çš„ä¾¿æ·è®¿é—®
def demo_optimized_loading():
    """æ¼”ç¤ºä¼˜åŒ–çš„åŠ è½½æ€§èƒ½"""
    import time
    
    print("ğŸš€ ä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹åŠ è½½æ¼”ç¤º")
    print("=" * 50)
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    for name, model_id in _llm_loader.list_available_models().items():
        print(f"  - {name}: {model_id}")
    
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”:")
    print("ä¼ ç»Ÿæ–¹å¼: å¯¼å…¥æ—¶ç«‹å³åˆå§‹åŒ–49ä¸ªæ¨¡å‹ â†’ æ…¢")
    print("ä¼˜åŒ–æ–¹å¼: æŒ‰éœ€æ‡’åŠ è½½æ¨¡å‹ â†’ å¿«")
    
    print("\nğŸ§ª æµ‹è¯•æ‡’åŠ è½½:")
    start_time = time.time()
    
    # ç¬¬ä¸€æ¬¡è®¿é—® - éœ€è¦åˆå§‹åŒ–
    model1 = get_llm('gemini_2_5_flash')
    first_load_time = time.time() - start_time
    
    # ç¬¬äºŒæ¬¡è®¿é—® - ä½¿ç”¨ç¼“å­˜
    start_time = time.time()
    model2 = get_llm('gemini_2_5_flash')
    cached_load_time = time.time() - start_time
    
    print(f"é¦–æ¬¡åŠ è½½è€—æ—¶: {first_load_time:.3f}s")
    print(f"ç¼“å­˜åŠ è½½è€—æ—¶: {cached_load_time:.6f}s")
    print(f"æ€§èƒ½æå‡: {first_load_time/cached_load_time:.0f}x")
    
    print(f"\nâœ… æ¨¡å‹å¯¹è±¡ç›¸åŒ: {model1 is model2}")
    print("ğŸ¯ ç»“è®º: æ‡’åŠ è½½å¤§å¹…æå‡æ¨¡å—å¯¼å…¥é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒåŠŸèƒ½å®Œæ•´æ€§")

if __name__ == "__main__":
    demo_optimized_loading()