#!/usr/bin/env python3
"""
LLM è°ƒç”¨ MCP çš„æœ€ä½³å®è·µæŒ‡å—
å±•ç¤ºæ ‡å‡†åŒ–çš„é›†æˆå¥—è·¯å’Œå…³é”®æ­¥éª¤
"""

import asyncio
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod

# MCP ç›¸å…³å¯¼å…¥
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# LLM ç›¸å…³å¯¼å…¥ï¼ˆç¤ºä¾‹ï¼‰
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic


@dataclass
class MCPTool:
    """MCP å·¥å…·çš„ç»Ÿä¸€è¡¨ç¤º"""
    name: str
    description: str
    input_schema: Dict[str, Any]
    

class LLMProvider(ABC):
    """LLM æä¾›å•†çš„æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def convert_tools(self, mcp_tools: List[MCPTool]) -> List[Dict[str, Any]]:
        """å°† MCP å·¥å…·æ ¼å¼è½¬æ¢ä¸ºç‰¹å®š LLM çš„æ ¼å¼"""
        pass
    
    @abstractmethod
    async def call_with_tools(self, prompt: str, tools: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨å·¥å…·è°ƒç”¨ LLM"""
        pass


class OpenAIProvider(LLMProvider):
    """OpenAI/DeepSeek æ ¼å¼æä¾›å•†"""
    
    def convert_tools(self, mcp_tools: List[MCPTool]) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸º OpenAI function calling æ ¼å¼"""
        return [{
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.input_schema
            }
        } for tool in mcp_tools]
    
    async def call_with_tools(self, prompt: str, tools: List[Dict[str, Any]]) -> Dict[str, Any]:
        # å®é™…å®ç°çœç•¥ï¼Œè¿”å›ç¤ºä¾‹
        return {
            "tool_calls": [{
                "id": "call_123",
                "type": "function",
                "function": {
                    "name": "calculator",
                    "arguments": '{"operation": "add", "a": 5, "b": 3}'
                }
            }]
        }


class AnthropicProvider(LLMProvider):
    """Anthropic Claude æ ¼å¼æä¾›å•†"""
    
    def convert_tools(self, mcp_tools: List[MCPTool]) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸º Claude tool use æ ¼å¼"""
        return [{
            "name": tool.name,
            "description": tool.description,
            "input_schema": tool.input_schema
        } for tool in mcp_tools]
    
    async def call_with_tools(self, prompt: str, tools: List[Dict[str, Any]]) -> Dict[str, Any]:
        # å®é™…å®ç°çœç•¥ï¼Œè¿”å›ç¤ºä¾‹
        return {
            "tool_use": {
                "name": "calculator",
                "input": {"operation": "add", "a": 5, "b": 3}
            }
        }


class MCPBestPractices:
    """LLM è°ƒç”¨ MCP çš„æœ€ä½³å®è·µå®ç°"""
    
    def __init__(self, server_command: str = "python", server_args: List[str] = ["server.py"]):
        self.server_params = StdioServerParameters(
            command=server_command,
            args=server_args
        )
        self.session: Optional[ClientSession] = None
        self.available_tools: List[MCPTool] = []
        self.available_resources = []
        self.available_prompts = []
    
    async def demonstrate_best_practices(self):
        """æ¼”ç¤º LLM è°ƒç”¨ MCP çš„æœ€ä½³å®è·µ"""
        print("ğŸš€ LLM è°ƒç”¨ MCP çš„æœ€ä½³å®è·µæŒ‡å—")
        print("=" * 80)
        
        # 1. è¿æ¥å’Œåˆå§‹åŒ–
        await self._demonstrate_connection()
        
        # 2. èƒ½åŠ›å‘ç°
        await self._demonstrate_capability_discovery()
        
        # 3. å·¥å…·æ ¼å¼è½¬æ¢
        await self._demonstrate_tool_conversion()
        
        # 4. æ ‡å‡†è°ƒç”¨æµç¨‹
        await self._demonstrate_standard_flow()
        
        # 5. é«˜çº§æ¨¡å¼
        await self._demonstrate_advanced_patterns()
        
        # 6. é”™è¯¯å¤„ç†
        await self._demonstrate_error_handling()
        
        # 7. æ€§èƒ½ä¼˜åŒ–
        await self._demonstrate_performance_optimization()
    
    async def _demonstrate_connection(self):
        """æ­¥éª¤ 1: è¿æ¥å’Œåˆå§‹åŒ–"""
        print("\nğŸ“¡ æ­¥éª¤ 1: è¿æ¥å’Œåˆå§‹åŒ–")
        print("-" * 40)
        
        print("æœ€ä½³å®è·µï¼š")
        print("â€¢ ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ¸…ç†")
        print("â€¢ åˆå§‹åŒ–æ—¶è¿›è¡Œå¥åº·æ£€æŸ¥")
        print("â€¢ è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´")
        
        print("\nç¤ºä¾‹ä»£ç ï¼š")
        print("""
async with stdio_client(self.server_params) as (read, write):
    async with ClientSession(read, write) as session:
        # åˆå§‹åŒ–ä¼šè¯
        await session.initialize()
        
        # å¥åº·æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        try:
            tools = await session.list_tools()
            print(f"âœ… MCP æœåŠ¡å™¨æ­£å¸¸ï¼Œå‘ç° {len(tools.tools)} ä¸ªå·¥å…·")
        except Exception as e:
            print(f"âŒ MCP æœåŠ¡å™¨å¼‚å¸¸: {e}")
        """)
    
    async def _demonstrate_capability_discovery(self):
        """æ­¥éª¤ 2: èƒ½åŠ›å‘ç°"""
        print("\nğŸ” æ­¥éª¤ 2: èƒ½åŠ›å‘ç°")
        print("-" * 40)
        
        print("æœ€ä½³å®è·µï¼š")
        print("â€¢ å¯åŠ¨æ—¶ä¸€æ¬¡æ€§è·å–æ‰€æœ‰èƒ½åŠ›")
        print("â€¢ ç¼“å­˜èƒ½åŠ›ä¿¡æ¯ï¼Œé¿å…é‡å¤æŸ¥è¯¢")
        print("â€¢ åŠ¨æ€æ„å»º LLM çš„ç³»ç»Ÿæç¤º")
        
        print("\næ ¸å¿ƒå®ç°ï¼š")
        print("""
class MCPCapabilityManager:
    def __init__(self):
        self.tools = {}
        self.resources = {}
        self.prompts = {}
    
    async def discover_capabilities(self, session):
        # å¹¶è¡Œè·å–æ‰€æœ‰èƒ½åŠ›
        tools_task = session.list_tools()
        resources_task = session.list_resources()
        prompts_task = session.list_prompts()
        
        tools, resources, prompts = await asyncio.gather(
            tools_task, resources_task, prompts_task
        )
        
        # æ„å»ºç´¢å¼•
        self.tools = {t.name: t for t in tools.tools}
        self.resources = {r.uri: r for r in resources.resources}
        self.prompts = {p.name: p for p in prompts.prompts}
        
    def build_system_prompt(self):
        prompt = "ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ MCP åŠŸèƒ½ï¼š\\n\\n"
        
        # å·¥å…·åˆ—è¡¨
        prompt += "ğŸ”§ å¯ç”¨å·¥å…·ï¼š\\n"
        for name, tool in self.tools.items():
            prompt += f"- {name}: {tool.description}\\n"
        
        # èµ„æºåˆ—è¡¨
        if self.resources:
            prompt += "\\nğŸ“„ å¯ç”¨èµ„æºï¼š\\n"
            for uri, resource in self.resources.items():
                prompt += f"- {uri}: {resource.description}\\n"
        
        return prompt
        """)
    
    async def _demonstrate_tool_conversion(self):
        """æ­¥éª¤ 3: å·¥å…·æ ¼å¼è½¬æ¢"""
        print("\nğŸ”„ æ­¥éª¤ 3: å·¥å…·æ ¼å¼è½¬æ¢")
        print("-" * 40)
        
        print("æœ€ä½³å®è·µï¼š")
        print("â€¢ ä¸ºæ¯ä¸ª LLM æä¾›å•†å®ç°è½¬æ¢å™¨")
        print("â€¢ ä¿æŒ MCP schema çš„å®Œæ•´æ€§")
        print("â€¢ å¤„ç†æšä¸¾ã€çº¦æŸç­‰ç‰¹æ®Šæƒ…å†µ")
        
        print("\nè½¬æ¢ç¤ºä¾‹ï¼š")
        print("""
# MCP åŸå§‹æ ¼å¼
mcp_tool = {
    "name": "weather",
    "description": "è·å–å¤©æ°”ä¿¡æ¯",
    "inputSchema": {
        "type": "object",
        "properties": {
            "city": {"type": "string", "description": "åŸå¸‚å"},
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
        },
        "required": ["city"]
    }
}

# OpenAI æ ¼å¼
openai_tool = {
    "type": "function",
    "function": {
        "name": "weather",
        "description": "è·å–å¤©æ°”ä¿¡æ¯",
        "parameters": mcp_tool["inputSchema"]  # ç›´æ¥ä½¿ç”¨
    }
}

# Anthropic æ ¼å¼
claude_tool = {
    "name": "weather",
    "description": "è·å–å¤©æ°”ä¿¡æ¯",
    "input_schema": mcp_tool["inputSchema"]  # å­—æ®µåä¸åŒ
}
        """)
    
    async def _demonstrate_standard_flow(self):
        """æ­¥éª¤ 4: æ ‡å‡†è°ƒç”¨æµç¨‹"""
        print("\nğŸ¯ æ­¥éª¤ 4: æ ‡å‡†è°ƒç”¨æµç¨‹")
        print("-" * 40)
        
        print("æ ‡å‡†å¥—è·¯ï¼ˆ6 æ­¥ï¼‰ï¼š")
        print("""
async def process_user_request(self, user_message: str) -> str:
    # 1. å‡†å¤‡æ¶ˆæ¯å’Œå·¥å…·
    messages = [
        {"role": "system", "content": self.system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    # 2. ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨ï¼ˆå·¥å…·é€‰æ‹©ï¼‰
    llm_response = await self.llm.call_with_tools(
        messages=messages,
        tools=self.converted_tools
    )
    
    # 3. è§£æå·¥å…·è°ƒç”¨è¯·æ±‚
    if has_tool_calls(llm_response):
        tool_results = []
        
        for tool_call in llm_response.tool_calls:
            # 4. æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨
            result = await self.mcp_session.call_tool(
                name=tool_call.name,
                arguments=json.loads(tool_call.arguments)
            )
            
            # 5. æ”¶é›†å·¥å…·ç»“æœ
            tool_results.append({
                "tool_call_id": tool_call.id,
                "content": extract_content(result)
            })
        
        # 6. ç¬¬äºŒæ¬¡ LLM è°ƒç”¨ï¼ˆç”Ÿæˆæœ€ç»ˆå›å¤ï¼‰
        messages.extend([
            {"role": "assistant", "tool_calls": llm_response.tool_calls},
            {"role": "tool", "content": tool_results}
        ])
        
        final_response = await self.llm.call(messages)
        return final_response.content
    
    return llm_response.content
        """)
    
    async def _demonstrate_advanced_patterns(self):
        """æ­¥éª¤ 5: é«˜çº§æ¨¡å¼"""
        print("\nğŸ¨ æ­¥éª¤ 5: é«˜çº§æ¨¡å¼")
        print("-" * 40)
        
        print("1ï¸âƒ£ æµå¼å“åº”å¤„ç†ï¼š")
        print("""
async def stream_with_tools(self, user_message: str):
    # å·¥å…·è°ƒç”¨éƒ¨åˆ†ä¸èƒ½æµå¼
    tool_response = await self.get_tool_calls(user_message)
    
    if tool_response.has_tools:
        # æ‰§è¡Œå·¥å…·
        tool_results = await self.execute_tools(tool_response)
        
        # æµå¼ç”Ÿæˆæœ€ç»ˆå›å¤
        async for chunk in self.llm.stream_with_context(tool_results):
            yield chunk
    else:
        # ç›´æ¥æµå¼å›å¤
        async for chunk in self.llm.stream(user_message):
            yield chunk
        """)
        
        print("\n2ï¸âƒ£ å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼š")
        print("""
# å¤šä¸ªç‹¬ç«‹å·¥å…·å¯ä»¥å¹¶è¡Œè°ƒç”¨
tool_tasks = []
for tool_call in llm_response.tool_calls:
    task = self.mcp_session.call_tool(
        tool_call.name,
        json.loads(tool_call.arguments)
    )
    tool_tasks.append(task)

# å¹¶è¡Œæ‰§è¡Œ
results = await asyncio.gather(*tool_tasks)
        """)
        
        print("\n3ï¸âƒ£ å·¥å…·é“¾å¼è°ƒç”¨ï¼š")
        print("""
# æ”¯æŒå·¥å…·ç»“æœä½œä¸ºä¸‹ä¸€ä¸ªå·¥å…·çš„è¾“å…¥
async def chain_tools(self, tool_chain: List[Dict]):
    result = None
    for tool_spec in tool_chain:
        # ä½¿ç”¨ä¸Šä¸€ä¸ªç»“æœæ›´æ–°å‚æ•°
        if result and "use_previous_result" in tool_spec:
            tool_spec["arguments"].update({"input": result})
        
        result = await self.mcp_session.call_tool(
            tool_spec["name"],
            tool_spec["arguments"]
        )
    return result
        """)
    
    async def _demonstrate_error_handling(self):
        """æ­¥éª¤ 6: é”™è¯¯å¤„ç†"""
        print("\nâš ï¸ æ­¥éª¤ 6: é”™è¯¯å¤„ç†")
        print("-" * 40)
        
        print("æœ€ä½³å®è·µï¼š")
        print("""
async def safe_tool_call(self, name: str, arguments: Dict) -> Dict:
    try:
        result = await self.mcp_session.call_tool(name, arguments)
        return {"success": True, "content": result}
    
    except ValidationError as e:
        # MCP å‚æ•°éªŒè¯å¤±è´¥
        return {
            "success": False,
            "error": "å‚æ•°éªŒè¯å¤±è´¥",
            "details": str(e)
        }
    
    except TimeoutError:
        # å·¥å…·æ‰§è¡Œè¶…æ—¶
        return {
            "success": False,
            "error": "å·¥å…·æ‰§è¡Œè¶…æ—¶",
            "fallback": "è¯·ç¨åé‡è¯•"
        }
    
    except MCPError as e:
        # MCP åè®®é”™è¯¯
        return {
            "success": False,
            "error": "MCP é€šä¿¡é”™è¯¯",
            "should_retry": True
        }
    
    except Exception as e:
        # æœªçŸ¥é”™è¯¯
        logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": "å·¥å…·æ‰§è¡Œå¤±è´¥",
            "fallback": self.get_fallback_response(name)
        }
        """)
    
    async def _demonstrate_performance_optimization(self):
        """æ­¥éª¤ 7: æ€§èƒ½ä¼˜åŒ–"""
        print("\nâš¡ æ­¥éª¤ 7: æ€§èƒ½ä¼˜åŒ–")
        print("-" * 40)
        
        print("ä¼˜åŒ–æŠ€å·§ï¼š")
        print("""
1. è¿æ¥æ± ç®¡ç†ï¼š
   class MCPConnectionPool:
       def __init__(self, size=5):
           self.pool = asyncio.Queue(size)
           self.size = size
       
       async def get_session(self):
           # ä»æ± ä¸­è·å–æˆ–åˆ›å»ºæ–°è¿æ¥
           pass

2. å·¥å…·ç»“æœç¼“å­˜ï¼š
   from functools import lru_cache
   
   @lru_cache(maxsize=100)
   async def cached_tool_call(tool_name: str, args_hash: str):
       # å¯¹ç¡®å®šæ€§å·¥å…·ä½¿ç”¨ç¼“å­˜
       pass

3. æ‰¹é‡è¯·æ±‚ï¼š
   # JSON-RPC 2.0 æ”¯æŒæ‰¹é‡è¯·æ±‚
   batch_request = [
       {"method": "tools/call", "params": {...}, "id": 1},
       {"method": "tools/call", "params": {...}, "id": 2}
   ]

4. é¢„åŠ è½½å¸¸ç”¨èµ„æºï¼š
   async def preload_resources(self):
       common_resources = ["config://settings", "data://cache"]
       tasks = [self.session.read_resource(r) for r in common_resources]
       await asyncio.gather(*tasks)
        """)


async def demonstrate_complete_flow():
    """æ¼”ç¤ºå®Œæ•´çš„ LLM + MCP é›†æˆæµç¨‹"""
    print("\nğŸ“‹ å®Œæ•´é›†æˆç¤ºä¾‹")
    print("=" * 80)
    
    print("""
class LLMMCPIntegration:
    def __init__(self, llm_provider: str = "openai"):
        self.llm_provider = llm_provider
        self.mcp_client = MCPClient()
        self.capability_manager = MCPCapabilityManager()
    
    async def setup(self):
        # 1. è¿æ¥ MCP
        await self.mcp_client.connect()
        
        # 2. å‘ç°èƒ½åŠ›
        await self.capability_manager.discover_capabilities(
            self.mcp_client.session
        )
        
        # 3. å‡†å¤‡ LLM
        self.system_prompt = self.capability_manager.build_system_prompt()
        self.tools = self.convert_tools_for_llm()
    
    async def chat(self, message: str) -> str:
        # æ ‡å‡† 6 æ­¥æµç¨‹
        # 1. å‡†å¤‡æ¶ˆæ¯
        # 2. LLM å·¥å…·é€‰æ‹©
        # 3. è§£æå·¥å…·è°ƒç”¨
        # 4. æ‰§è¡Œ MCP å·¥å…·
        # 5. æ”¶é›†ç»“æœ
        # 6. ç”Ÿæˆæœ€ç»ˆå›å¤
        pass
    
    async def cleanup(self):
        await self.mcp_client.disconnect()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    integration = LLMMCPIntegration("openai")
    await integration.setup()
    
    response = await integration.chat("åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    print(response)
    
    await integration.cleanup()
    """)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ LLM è°ƒç”¨ MCP çš„æœ€ä½³å®è·µå’Œæ ‡å‡†å¥—è·¯")
    print("å±•ç¤ºå¦‚ä½•ä¼˜é›…åœ°é›†æˆ LLM å’Œ MCP")
    print()
    
    # æ¼”ç¤ºæœ€ä½³å®è·µ
    best_practices = MCPBestPractices()
    await best_practices.demonstrate_best_practices()
    
    # æ¼”ç¤ºå®Œæ•´æµç¨‹
    await demonstrate_complete_flow()
    
    print("\n" + "=" * 80)
    print("âœ… æœ€ä½³å®è·µæ€»ç»“")
    print("\nğŸ¯ æ ¸å¿ƒå¥—è·¯ï¼ˆ6æ­¥ï¼‰ï¼š")
    print("1. è¿æ¥ MCP æœåŠ¡å™¨å¹¶åˆå§‹åŒ–")
    print("2. å‘ç°å¹¶ç¼“å­˜æ‰€æœ‰èƒ½åŠ›")
    print("3. è½¬æ¢å·¥å…·æ ¼å¼é€‚é… LLM")
    print("4. LLM åˆ†æå¹¶é€‰æ‹©å·¥å…·")
    print("5. æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨")
    print("6. æ•´åˆç»“æœç”Ÿæˆå›å¤")
    
    print("\nğŸ’¡ å…³é”®è¦ç‚¹ï¼š")
    print("â€¢ ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç®¡ç†è¿æ¥")
    print("â€¢ ç¼“å­˜èƒ½åŠ›ä¿¡æ¯é¿å…é‡å¤æŸ¥è¯¢")
    print("â€¢ ä¸ºä¸åŒ LLM å®ç°æ ¼å¼è½¬æ¢å™¨")
    print("â€¢ æ”¯æŒå¹¶è¡Œå’Œé“¾å¼å·¥å…·è°ƒç”¨")
    print("â€¢ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥")
    print("â€¢ æ€§èƒ½ä¼˜åŒ–ï¼ˆè¿æ¥æ± ã€ç¼“å­˜ç­‰ï¼‰")


if __name__ == "__main__":
    asyncio.run(main())