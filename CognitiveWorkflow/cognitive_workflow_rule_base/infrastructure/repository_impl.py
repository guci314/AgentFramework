# -*- coding: utf-8 -*-
"""
ä»“å‚¨å®ç°

æä¾›åŸºäºå†…å­˜å’Œæ–‡ä»¶çš„ä»“å‚¨å®ç°ï¼Œç”¨äºæ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨ã€‚
åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥æ›¿æ¢ä¸ºåŸºäºæ•°æ®åº“çš„å®ç°ã€‚
"""

from typing import Dict, List, Optional, Tuple
import json
import os
import logging
from datetime import datetime
from pathlib import Path

from ..domain.entities import (
    ProductionRule, RuleSet, RuleExecution, GlobalState
)
from ..domain.repositories import (
    RuleRepository, StateRepository, ExecutionRepository
)
from ..domain.value_objects import RulePhase, ExecutionStatus
from ..utils.concurrent_safe_id_generator import id_generator, SafeFileOperations

logger = logging.getLogger(__name__)


class RuleRepositoryImpl(RuleRepository):
    """è§„åˆ™ä»“å‚¨å®ç° - åŸºäºæ–‡ä»¶å­˜å‚¨"""
    
    def __init__(self, storage_path: str = "./.cognitive_workflow_data/rules"):
        """
        åˆå§‹åŒ–è§„åˆ™ä»“å‚¨
        
        Args:
            storage_path: å­˜å‚¨è·¯å¾„
        """
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ç¼“å­˜
        self._rule_sets_cache: Dict[str, RuleSet] = {}
        self._rules_cache: Dict[str, ProductionRule] = {}
        
        # åŠ è½½ç°æœ‰æ•°æ®
        self._load_existing_data()
    
    def save_rule_set(self, rule_set: RuleSet) -> None:
        """ä¿å­˜è§„åˆ™é›†ï¼ˆå¹¶å‘å®‰å…¨ï¼‰"""
        try:
            file_path = self.storage_path / f"rule_set_{rule_set.id}.json"
            
            # ğŸ”‘ æ£€æŸ¥æ–‡ä»¶å†²çª
            if SafeFileOperations.check_file_conflict(file_path):
                logger.warning(f"æ£€æµ‹åˆ°æ–‡ä»¶å†²çªï¼Œç­‰å¾…åé‡è¯•: {file_path}")
                import time
                time.sleep(0.1)
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            rule_set_data = {
                'id': rule_set.id,
                'goal': rule_set.goal,
                'rules': [self._rule_to_dict(rule) for rule in rule_set.rules],
                # 'created_at': rule_set.created_at.isoformat(),  # Removed for LLM caching
                # 'updated_at': rule_set.updated_at.isoformat(),  # Removed for LLM caching
                'version': rule_set.version,
                'status': rule_set.status.value,
                'modification_history': [self._modification_to_dict(mod) for mod in rule_set.modification_history]
            }
            
            # ğŸ”‘ ä½¿ç”¨åŸå­æ€§å†™å…¥
            if not SafeFileOperations.atomic_write_json(file_path, rule_set_data):
                raise IOError(f"åŸå­æ€§å†™å…¥å¤±è´¥: {file_path}")
            
            # æ›´æ–°ç¼“å­˜
            self._rule_sets_cache[rule_set.id] = rule_set
            for rule in rule_set.rules:
                self._rules_cache[rule.id] = rule
            
            logger.debug(f"è§„åˆ™é›†å·²å®‰å…¨ä¿å­˜: {rule_set.id}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è§„åˆ™é›†å¤±è´¥: {e}")
            raise
    
    def load_rule_set(self, rule_set_id: str) -> RuleSet:
        """åŠ è½½è§„åˆ™é›†"""
        try:
            # å…ˆæ£€æŸ¥ç¼“å­˜
            if rule_set_id in self._rule_sets_cache:
                return self._rule_sets_cache[rule_set_id]
            
            file_path = self.storage_path / f"rule_set_{rule_set_id}.json"
            
            if not file_path.exists():
                raise ValueError(f"è§„åˆ™é›†ä¸å­˜åœ¨: {rule_set_id}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                rule_set_data = json.load(f)
            
            rule_set = self._dict_to_rule_set(rule_set_data)
            
            # æ›´æ–°ç¼“å­˜
            self._rule_sets_cache[rule_set_id] = rule_set
            
            return rule_set
            
        except Exception as e:
            logger.error(f"åŠ è½½è§„åˆ™é›†å¤±è´¥: {e}")
            raise
    
    def find_rules_by_condition(self, condition_pattern: str) -> List[ProductionRule]:
        """æ ¹æ®æ¡ä»¶æ¨¡å¼æŸ¥æ‰¾è§„åˆ™"""
        try:
            matching_rules = []
            
            for rule in self._rules_cache.values():
                if condition_pattern.lower() in rule.condition.lower():
                    matching_rules.append(rule)
            
            return matching_rules
            
        except Exception as e:
            logger.error(f"æŒ‰æ¡ä»¶æŸ¥æ‰¾è§„åˆ™å¤±è´¥: {e}")
            return []
    
    def find_rules_by_phase(self, phase: RulePhase) -> List[ProductionRule]:
        """æ ¹æ®é˜¶æ®µæŸ¥æ‰¾è§„åˆ™"""
        try:
            matching_rules = []
            
            for rule in self._rules_cache.values():
                if rule.phase == phase:
                    matching_rules.append(rule)
            
            return matching_rules
            
        except Exception as e:
            logger.error(f"æŒ‰é˜¶æ®µæŸ¥æ‰¾è§„åˆ™å¤±è´¥: {e}")
            return []
    
    def save_rule(self, rule: ProductionRule) -> None:
        """ä¿å­˜å•ä¸ªè§„åˆ™"""
        try:
            self._rules_cache[rule.id] = rule
            logger.debug(f"è§„åˆ™å·²ç¼“å­˜: {rule.id}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è§„åˆ™å¤±è´¥: {e}")
            raise
    
    def load_rule(self, rule_id: str) -> ProductionRule:
        """åŠ è½½å•ä¸ªè§„åˆ™"""
        try:
            if rule_id in self._rules_cache:
                return self._rules_cache[rule_id]
            
            raise ValueError(f"è§„åˆ™ä¸å­˜åœ¨: {rule_id}")
            
        except Exception as e:
            logger.error(f"åŠ è½½è§„åˆ™å¤±è´¥: {e}")
            raise
    
    def delete_rule(self, rule_id: str) -> bool:
        """åˆ é™¤è§„åˆ™"""
        try:
            if rule_id in self._rules_cache:
                del self._rules_cache[rule_id]
                return True
            return False
            
        except Exception as e:
            logger.error(f"åˆ é™¤è§„åˆ™å¤±è´¥: {e}")
            return False
    
    def find_rules_by_agent_capability(self, agent_name: str) -> List[ProductionRule]:
        """æ ¹æ®æ™ºèƒ½ä½“åç§°æŸ¥æ‰¾è§„åˆ™ï¼ˆä¿æŒæ–¹æ³•åå…¼å®¹æ€§ï¼‰"""
        return self.find_rules_by_agent_name(agent_name)
    
    def find_rules_by_agent_name(self, agent_name: str) -> List[ProductionRule]:
        """æ ¹æ®æ™ºèƒ½ä½“åç§°æŸ¥æ‰¾è§„åˆ™"""
        try:
            matching_rules = []
            
            for rule in self._rules_cache.values():
                if rule.agent_name == agent_name:
                    matching_rules.append(rule)
            
            return matching_rules
            
        except Exception as e:
            logger.error(f"æŒ‰æ™ºèƒ½ä½“åç§°æŸ¥æ‰¾è§„åˆ™å¤±è´¥: {e}")
            return []
    
    def find_rules_by_priority_range(self, min_priority: int, max_priority: int) -> List[ProductionRule]:
        """æ ¹æ®ä¼˜å…ˆçº§èŒƒå›´æŸ¥æ‰¾è§„åˆ™"""
        try:
            matching_rules = []
            
            for rule in self._rules_cache.values():
                if min_priority <= rule.priority <= max_priority:
                    matching_rules.append(rule)
            
            return matching_rules
            
        except Exception as e:
            logger.error(f"æŒ‰ä¼˜å…ˆçº§èŒƒå›´æŸ¥æ‰¾è§„åˆ™å¤±è´¥: {e}")
            return []
    
    def get_rule_count(self) -> int:
        """è·å–è§„åˆ™æ€»æ•°"""
        return len(self._rules_cache)
    
    def list_all_rule_sets(self) -> List[RuleSet]:
        """åˆ—å‡ºæ‰€æœ‰è§„åˆ™é›†"""
        return list(self._rule_sets_cache.values())
    
    def _load_existing_data(self) -> None:
        """åŠ è½½ç°æœ‰æ•°æ®"""
        try:
            for file_path in self.storage_path.glob("rule_set_*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        rule_set_data = json.load(f)
                    
                    rule_set = self._dict_to_rule_set(rule_set_data)
                    self._rule_sets_cache[rule_set.id] = rule_set
                    
                    for rule in rule_set.rules:
                        self._rules_cache[rule.id] = rule
                        
                except Exception as e:
                    logger.error(f"åŠ è½½è§„åˆ™é›†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    
        except Exception as e:
            logger.error(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
    
    def _rule_to_dict(self, rule: ProductionRule) -> Dict:
        """å°†è§„åˆ™è½¬æ¢ä¸ºå­—å…¸"""
        return rule.to_dict()
    
    def _dict_to_rule_set(self, data: Dict) -> RuleSet:
        """ä»å­—å…¸åˆ›å»ºè§„åˆ™é›†"""
        from ..domain.value_objects import RuleSetStatus, ModificationType
        
        # è½¬æ¢è§„åˆ™åˆ—è¡¨
        rules = []
        for rule_data in data.get('rules', []):
            rule = self._dict_to_rule(rule_data)
            rules.append(rule)
        
        # è½¬æ¢ä¿®æ”¹å†å²
        modification_history = []
        for mod_data in data.get('modification_history', []):
            mod = self._dict_to_modification(mod_data)
            modification_history.append(mod)
        
        rule_set = RuleSet(
            id=data['id'],
            goal=data['goal'],
            rules=rules,
            # created_at=datetime.fromisoformat(data.get('created_at')),  # Removed for LLM caching
            # updated_at=datetime.fromisoformat(data.get('updated_at')),  # Removed for LLM caching
            version=data.get('version', 1),
            status=RuleSetStatus(data.get('status', 'active')),
            modification_history=modification_history
        )
        
        return rule_set
    
    def _dict_to_rule(self, data: Dict) -> ProductionRule:
        """ä»å­—å…¸åˆ›å»ºè§„åˆ™ï¼Œæ”¯æŒå‘åå…¼å®¹æ€§"""
        # Handle backward compatibility: agent_capability_id -> agent_name
        agent_name = data.get('agent_name') or data.get('agent_capability_id', 'coder')
        
        rule = ProductionRule(
            id=data['id'],
            name=data['name'],
            condition=data['condition'],
            action=data['action'],
            agent_name=agent_name,
            priority=data.get('priority', 50),
            phase=self._parse_phase_with_compatibility(data.get('phase', 'execution')),
            expected_outcome=data.get('expected_outcome', ''),
            # created_at=datetime.fromisoformat(data['created_at']),  # Removed for LLM caching
            # updated_at=datetime.fromisoformat(data['updated_at']),  # Removed for LLM caching
            metadata=data.get('metadata', {})
        )
        
        return rule
    
    def _parse_phase_with_compatibility(self, phase_value: str) -> RulePhase:
        """
        è§£æé˜¶æ®µå€¼ï¼Œæ”¯æŒå‘åå…¼å®¹æ€§
        
        Args:
            phase_value: é˜¶æ®µå­—ç¬¦ä¸²å€¼
            
        Returns:
            RulePhase: è§£æåçš„é˜¶æ®µæšä¸¾
        """
        try:
            # å¤„ç†æ—§çš„é˜¶æ®µå€¼æ˜ å°„
            if phase_value == 'problem_solving':
                logger.debug("å°†æ—§çš„ 'problem_solving' é˜¶æ®µè½¬æ¢ä¸º 'execution'")
                return RulePhase.EXECUTION
            elif phase_value == 'cleanup':
                logger.debug("å°†æ—§çš„ 'cleanup' é˜¶æ®µè½¬æ¢ä¸º 'verification'")
                return RulePhase.VERIFICATION
            else:
                # å°è¯•ç›´æ¥è§£æ
                return RulePhase(phase_value)
                
        except ValueError as e:
            logger.warning(f"æ— æ³•è§£æé˜¶æ®µå€¼ '{phase_value}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'execution': {e}")
            return RulePhase.EXECUTION
    
    def _modification_to_dict(self, modification) -> Dict:
        """å°†ä¿®æ”¹è®°å½•è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'modification_type': modification.modification_type.value,
            'target_rule_id': modification.target_rule_id,
            'new_rule_data': modification.new_rule_data,
            'modification_reason': modification.modification_reason,
            'timestamp': modification.timestamp.isoformat()
        }
    
    def _dict_to_modification(self, data: Dict):
        """ä»å­—å…¸åˆ›å»ºä¿®æ”¹è®°å½•"""
        from ..domain.value_objects import RuleModification, ModificationType
        
        return RuleModification(
            modification_type=ModificationType(data['modification_type']),
            target_rule_id=data.get('target_rule_id'),
            new_rule_data=data.get('new_rule_data'),
            modification_reason=data['modification_reason'],
            timestamp=datetime.fromisoformat(data['timestamp'])
        )


class StateRepositoryImpl(StateRepository):
    """çŠ¶æ€ä»“å‚¨å®ç° - åŸºäºæ–‡ä»¶å­˜å‚¨"""
    
    def __init__(self, storage_path: str = "./.cognitive_workflow_data/states"):
        """
        åˆå§‹åŒ–çŠ¶æ€ä»“å‚¨
        
        Args:
            storage_path: å­˜å‚¨è·¯å¾„
        """
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ç¼“å­˜
        self._states_cache: Dict[str, GlobalState] = {}
        self._workflow_states: Dict[str, List[GlobalState]] = {}
    
    def save_state(self, global_state: GlobalState) -> None:
        """ä¿å­˜çŠ¶æ€ï¼ˆå¹¶å‘å®‰å…¨ï¼‰"""
        try:
            file_path = self.storage_path / f"state_{global_state.id}.json"
            
            # ğŸ”‘ æ£€æŸ¥æ–‡ä»¶å†²çª
            if SafeFileOperations.check_file_conflict(file_path):
                logger.warning(f"æ£€æµ‹åˆ°çŠ¶æ€æ–‡ä»¶å†²çªï¼Œç­‰å¾…åé‡è¯•: {file_path}")
                import time
                time.sleep(0.05)
            
            state_data = global_state.to_dict()
            
            # ğŸ”‘ ä½¿ç”¨åŸå­æ€§å†™å…¥
            if not SafeFileOperations.atomic_write_json(file_path, state_data):
                raise IOError(f"çŠ¶æ€åŸå­æ€§å†™å…¥å¤±è´¥: {file_path}")
            
            # æ›´æ–°ç¼“å­˜
            self._states_cache[global_state.id] = global_state
            
            # æ›´æ–°å·¥ä½œæµçŠ¶æ€å†å²
            workflow_id = global_state.workflow_id
            if workflow_id:
                if workflow_id not in self._workflow_states:
                    self._workflow_states[workflow_id] = []
                self._workflow_states[workflow_id].append(global_state)
            
            logger.debug(f"çŠ¶æ€å·²å®‰å…¨ä¿å­˜: {global_state.id}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
            raise
    
    def load_state(self, state_id: str) -> GlobalState:
        """åŠ è½½çŠ¶æ€"""
        try:
            # å…ˆæ£€æŸ¥ç¼“å­˜
            if state_id in self._states_cache:
                return self._states_cache[state_id]
            
            file_path = self.storage_path / f"state_{state_id}.json"
            
            if not file_path.exists():
                raise ValueError(f"çŠ¶æ€ä¸å­˜åœ¨: {state_id}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            global_state = self._dict_to_state(state_data)
            
            # æ›´æ–°ç¼“å­˜
            self._states_cache[state_id] = global_state
            
            return global_state
            
        except Exception as e:
            logger.error(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
            raise
    
    def get_state_history(self, workflow_id: str) -> List[GlobalState]:
        """è·å–å·¥ä½œæµçš„çŠ¶æ€å†å²"""
        try:
            if workflow_id in self._workflow_states:
                # Cannot sort by timestamp anymore as it was removed for LLM caching
                return self._workflow_states[workflow_id]
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ–‡ä»¶åŠ è½½
            workflow_states = []
            for file_path in self.storage_path.glob("state_*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        state_data = json.load(f)
                    
                    if state_data.get('workflow_id') == workflow_id:
                        state = self._dict_to_state(state_data)
                        workflow_states.append(state)
                        
                except Exception as e:
                    logger.error(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            # æ›´æ–°ç¼“å­˜
            self._workflow_states[workflow_id] = workflow_states
            
            return workflow_states  # Cannot sort by timestamp anymore
            
        except Exception as e:
            logger.error(f"è·å–çŠ¶æ€å†å²å¤±è´¥: {e}")
            return []
    
    def save_state_snapshot(self, state: GlobalState, snapshot_name: str) -> None:
        """ä¿å­˜çŠ¶æ€å¿«ç…§"""
        try:
            snapshot_path = self.storage_path / "snapshots"
            snapshot_path.mkdir(exist_ok=True)
            
            file_path = snapshot_path / f"snapshot_{snapshot_name}.json"
            
            state_data = state.to_dict()
            state_data['snapshot_name'] = snapshot_name
            state_data['snapshot_timestamp'] = datetime.now().isoformat()
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"çŠ¶æ€å¿«ç…§å·²ä¿å­˜: {snapshot_name}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€å¿«ç…§å¤±è´¥: {e}")
            raise
    
    def load_state_snapshot(self, snapshot_name: str) -> GlobalState:
        """åŠ è½½çŠ¶æ€å¿«ç…§"""
        try:
            snapshot_path = self.storage_path / "snapshots"
            file_path = snapshot_path / f"snapshot_{snapshot_name}.json"
            
            if not file_path.exists():
                raise ValueError(f"çŠ¶æ€å¿«ç…§ä¸å­˜åœ¨: {snapshot_name}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            return self._dict_to_state(state_data)
            
        except Exception as e:
            logger.error(f"åŠ è½½çŠ¶æ€å¿«ç…§å¤±è´¥: {e}")
            raise
    
    def find_states_by_workflow(self, workflow_id: str) -> List[GlobalState]:
        """æ ¹æ®å·¥ä½œæµIDæŸ¥æ‰¾çŠ¶æ€"""
        return self.get_state_history(workflow_id)
    
    def find_states_by_time_range(self, start_time: datetime, end_time: datetime) -> List[GlobalState]:
        """æ ¹æ®æ—¶é—´èŒƒå›´æŸ¥æ‰¾çŠ¶æ€"""
        try:
            matching_states = []
            
            for state in self._states_cache.values():
                # Time filtering removed as timestamp was removed for LLM caching
                matching_states.append(state)
            
            return matching_states  # Cannot sort by timestamp anymore
            
        except Exception as e:
            logger.error(f"æŒ‰æ—¶é—´èŒƒå›´æŸ¥æ‰¾çŠ¶æ€å¤±è´¥: {e}")
            return []
    
    def get_latest_state(self, workflow_id: str) -> Optional[GlobalState]:
        """è·å–å·¥ä½œæµçš„æœ€æ–°çŠ¶æ€"""
        try:
            history = self.get_state_history(workflow_id)
            if history:
                return history[-1]  # æœ€åä¸€ä¸ªçŠ¶æ€
            return None
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°çŠ¶æ€å¤±è´¥: {e}")
            return None
    
    def delete_old_states(self, cutoff_time: datetime) -> int:
        """åˆ é™¤æ—§çŠ¶æ€ï¼Œè¿”å›åˆ é™¤çš„æ•°é‡"""
        try:
            deleted_count = 0
            states_to_delete = []
            
            for state_id, state in self._states_cache.items():
                # Time comparison removed as timestamp was removed for LLM caching
                # For now, don't delete any states based on time
                pass
            
            for state_id in states_to_delete:
                try:
                    # åˆ é™¤æ–‡ä»¶
                    file_path = self.storage_path / f"state_{state_id}.json"
                    if file_path.exists():
                        file_path.unlink()
                    
                    # ä»ç¼“å­˜åˆ é™¤
                    del self._states_cache[state_id]
                    deleted_count += 1
                    
                except Exception as e:
                    logger.error(f"åˆ é™¤çŠ¶æ€å¤±è´¥ {state_id}: {e}")
            
            logger.info(f"åˆ é™¤äº† {deleted_count} ä¸ªæ—§çŠ¶æ€")
            return deleted_count
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ—§çŠ¶æ€å¤±è´¥: {e}")
            return 0
    
    def get_state_count(self, workflow_id: Optional[str] = None) -> int:
        """è·å–çŠ¶æ€æ•°é‡"""
        if workflow_id:
            return len(self.get_state_history(workflow_id))
        else:
            return len(self._states_cache)
    
    def _dict_to_state(self, data: Dict) -> GlobalState:
        """ä»å­—å…¸åˆ›å»ºçŠ¶æ€"""
        # Backward compatibility: handle both 'state' (new) and 'description' (old) field names
        state_value = data.get('state', data.get('description', ''))
        
        return GlobalState(
            id=data['id'],
            state=state_value,
            context_variables=data.get('context_variables', {}),
            execution_history=data.get('execution_history', []),
            # timestamp=datetime.fromisoformat(data.get('timestamp')),  # Removed for LLM caching
            workflow_id=data.get('workflow_id', ''),
            iteration_count=data.get('iteration_count', 0),
            goal_achieved=data.get('goal_achieved', False)
        )


class ExecutionRepositoryImpl(ExecutionRepository):
    """æ‰§è¡Œä»“å‚¨å®ç° - åŸºäºæ–‡ä»¶å­˜å‚¨"""
    
    def __init__(self, storage_path: str = "./.cognitive_workflow_data/executions"):
        """
        åˆå§‹åŒ–æ‰§è¡Œä»“å‚¨
        
        Args:
            storage_path: å­˜å‚¨è·¯å¾„
        """
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ç¼“å­˜
        self._executions_cache: Dict[str, RuleExecution] = {}
        self._rule_executions: Dict[str, List[RuleExecution]] = {}
    
    def save_execution(self, rule_execution: RuleExecution) -> None:
        """ä¿å­˜è§„åˆ™æ‰§è¡Œè®°å½•"""
        try:
            file_path = self.storage_path / f"execution_{rule_execution.id}.json"
            
            execution_data = self._execution_to_dict(rule_execution)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(execution_data, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°ç¼“å­˜
            self._executions_cache[rule_execution.id] = rule_execution
            
            # æ›´æ–°è§„åˆ™æ‰§è¡Œå†å²
            rule_id = rule_execution.rule_id
            if rule_id not in self._rule_executions:
                self._rule_executions[rule_id] = []
            self._rule_executions[rule_id].append(rule_execution)
            
            logger.debug(f"æ‰§è¡Œè®°å½•å·²ä¿å­˜: {rule_execution.id}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            raise
    
    def load_execution(self, execution_id: str) -> RuleExecution:
        """åŠ è½½è§„åˆ™æ‰§è¡Œè®°å½•"""
        try:
            # å…ˆæ£€æŸ¥ç¼“å­˜
            if execution_id in self._executions_cache:
                return self._executions_cache[execution_id]
            
            file_path = self.storage_path / f"execution_{execution_id}.json"
            
            if not file_path.exists():
                raise ValueError(f"æ‰§è¡Œè®°å½•ä¸å­˜åœ¨: {execution_id}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                execution_data = json.load(f)
            
            rule_execution = self._dict_to_execution(execution_data)
            
            # æ›´æ–°ç¼“å­˜
            self._executions_cache[execution_id] = rule_execution
            
            return rule_execution
            
        except Exception as e:
            logger.error(f"åŠ è½½æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            raise
    
    def find_executions_by_rule(self, rule_id: str) -> List[RuleExecution]:
        """æ ¹æ®è§„åˆ™IDæŸ¥æ‰¾æ‰§è¡Œè®°å½•"""
        try:
            if rule_id in self._rule_executions:
                return self._rule_executions[rule_id]  # Cannot sort by started_at anymore
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œä»æ–‡ä»¶åŠ è½½
            rule_executions = []
            for file_path in self.storage_path.glob("execution_*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        execution_data = json.load(f)
                    
                    if execution_data.get('rule_id') == rule_id:
                        execution = self._dict_to_execution(execution_data)
                        rule_executions.append(execution)
                        
                except Exception as e:
                    logger.error(f"åŠ è½½æ‰§è¡Œæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            # æ›´æ–°ç¼“å­˜
            self._rule_executions[rule_id] = rule_executions
            
            return rule_executions  # Cannot sort by started_at anymore
            
        except Exception as e:
            logger.error(f"æŒ‰è§„åˆ™æŸ¥æ‰¾æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return []
    
    def find_failed_executions(self, time_range: Tuple[datetime, datetime]) -> List[RuleExecution]:
        """æŸ¥æ‰¾å¤±è´¥çš„æ‰§è¡Œè®°å½•"""
        try:
            start_time, end_time = time_range
            failed_executions = []
            
            for execution in self._executions_cache.values():
                if execution.status == ExecutionStatus.FAILED:
                    # Time filtering removed as started_at was removed for LLM caching
                    failed_executions.append(execution)
            
            return failed_executions  # Cannot sort by started_at anymore
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾å¤±è´¥æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return []
    
    def find_executions_by_status(self, status: ExecutionStatus) -> List[RuleExecution]:
        """æ ¹æ®çŠ¶æ€æŸ¥æ‰¾æ‰§è¡Œè®°å½•"""
        try:
            matching_executions = []
            
            for execution in self._executions_cache.values():
                if execution.status == status:
                    matching_executions.append(execution)
            
            return matching_executions  # Cannot sort by started_at anymore
            
        except Exception as e:
            logger.error(f"æŒ‰çŠ¶æ€æŸ¥æ‰¾æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return []
    
    def find_executions_by_time_range(self, start_time: datetime, end_time: datetime) -> List[RuleExecution]:
        """æ ¹æ®æ—¶é—´èŒƒå›´æŸ¥æ‰¾æ‰§è¡Œè®°å½•"""
        try:
            matching_executions = []
            
            for execution in self._executions_cache.values():
                # Time filtering removed as started_at was removed for LLM caching
                matching_executions.append(execution)
            
            return matching_executions  # Cannot sort by started_at anymore
            
        except Exception as e:
            logger.error(f"æŒ‰æ—¶é—´èŒƒå›´æŸ¥æ‰¾æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return []
    
    def get_execution_statistics(self, rule_id: Optional[str] = None) -> dict:
        """è·å–æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        try:
            if rule_id:
                executions = self.find_executions_by_rule(rule_id)
            else:
                executions = list(self._executions_cache.values())
            
            total_executions = len(executions)
            successful_executions = sum(1 for e in executions if e.is_successful())
            failed_executions = total_executions - successful_executions
            
            # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
            execution_times = []
            for execution in executions:
                duration = execution.get_execution_duration()
                if duration is not None:
                    execution_times.append(duration)
            
            average_execution_time = (sum(execution_times) / len(execution_times)) if execution_times else 0.0
            total_execution_time = sum(execution_times)
            
            return {
                'total_executions': total_executions,
                'successful_executions': successful_executions,
                'failed_executions': failed_executions,
                'success_rate': successful_executions / total_executions if total_executions > 0 else 0.0,
                'average_execution_time': average_execution_time,
                'total_execution_time': total_execution_time,
                'rule_match_accuracy': 0.85  # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
            }
            
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡Œç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'total_executions': 0,
                'successful_executions': 0,
                'failed_executions': 0,
                'success_rate': 0.0,
                'average_execution_time': 0.0,
                'total_execution_time': 0.0,
                'rule_match_accuracy': 0.0
            }
    
    def get_recent_executions(self, limit: int = 100) -> List[RuleExecution]:
        """è·å–æœ€è¿‘çš„æ‰§è¡Œè®°å½•"""
        try:
            all_executions = list(self._executions_cache.values())
            # Cannot sort by started_at anymore as it was removed for LLM caching
            return all_executions[:limit]
            
        except Exception as e:
            logger.error(f"è·å–æœ€è¿‘æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return []
    
    def delete_old_executions(self, cutoff_time: datetime) -> int:
        """åˆ é™¤æ—§çš„æ‰§è¡Œè®°å½•ï¼Œè¿”å›åˆ é™¤çš„æ•°é‡"""
        try:
            deleted_count = 0
            executions_to_delete = []
            
            for execution_id, execution in self._executions_cache.items():
                # Time comparison removed as started_at was removed for LLM caching
                # For now, don't delete any executions based on time
                pass
            
            for execution_id in executions_to_delete:
                try:
                    # åˆ é™¤æ–‡ä»¶
                    file_path = self.storage_path / f"execution_{execution_id}.json"
                    if file_path.exists():
                        file_path.unlink()
                    
                    # ä»ç¼“å­˜åˆ é™¤
                    del self._executions_cache[execution_id]
                    deleted_count += 1
                    
                except Exception as e:
                    logger.error(f"åˆ é™¤æ‰§è¡Œè®°å½•å¤±è´¥ {execution_id}: {e}")
            
            logger.info(f"åˆ é™¤äº† {deleted_count} ä¸ªæ—§æ‰§è¡Œè®°å½•")
            return deleted_count
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ—§æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return 0
    
    def get_execution_count(self, rule_id: Optional[str] = None, status: Optional[ExecutionStatus] = None) -> int:
        """è·å–æ‰§è¡Œè®°å½•æ•°é‡"""
        try:
            if rule_id and status:
                executions = self.find_executions_by_rule(rule_id)
                return sum(1 for e in executions if e.status == status)
            elif rule_id:
                return len(self.find_executions_by_rule(rule_id))
            elif status:
                return len(self.find_executions_by_status(status))
            else:
                return len(self._executions_cache)
                
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡Œè®°å½•æ•°é‡å¤±è´¥: {e}")
            return 0
    
    def find_long_running_executions(self, threshold_seconds: int = 300) -> List[RuleExecution]:
        """æŸ¥æ‰¾é•¿æ—¶é—´è¿è¡Œçš„æ‰§è¡Œè®°å½•"""
        try:
            long_running = []
            
            for execution in self._executions_cache.values():
                duration = execution.get_execution_duration()
                if duration is not None and duration > threshold_seconds:
                    long_running.append(execution)
            
            return sorted(long_running, key=lambda e: e.get_execution_duration(), reverse=True)
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾é•¿æ—¶é—´è¿è¡Œæ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return []
    
    def update_execution_status(self, execution_id: str, status: ExecutionStatus, failure_reason: Optional[str] = None) -> bool:
        """æ›´æ–°æ‰§è¡ŒçŠ¶æ€"""
        try:
            if execution_id in self._executions_cache:
                execution = self._executions_cache[execution_id]
                execution.status = status
                if failure_reason:
                    execution.failure_reason = failure_reason
                
                # ä¿å­˜æ›´æ–°
                self.save_execution(execution)
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ‰§è¡ŒçŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _execution_to_dict(self, execution: RuleExecution) -> Dict:
        """å°†æ‰§è¡Œè®°å½•è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'id': execution.id,
            'rule_id': execution.rule_id,
            'status': execution.status.value,
            'result': execution.result.to_dict() if execution.result else None,
            # 'started_at': execution.started_at.isoformat(),  # Removed for LLM caching
            'completed_at': execution.completed_at.isoformat() if execution.completed_at else None,
            'execution_context': execution.execution_context,
            'failure_reason': execution.failure_reason,
            'confidence_score': execution.confidence_score
        }
    
    def _dict_to_execution(self, data: Dict) -> RuleExecution:
        """ä»å­—å…¸åˆ›å»ºæ‰§è¡Œè®°å½•"""
        from ..domain.entities import WorkflowResult
        
        # è½¬æ¢ç»“æœ
        result = None
        if data.get('result'):
            result_data = data['result']
            result = WorkflowResult(
                success=result_data['success'],
                message=result_data['message'],
                data=result_data.get('data'),
                error_details=result_data.get('error_details'),
                metadata=result_data.get('metadata', {}),
                timestamp=datetime.fromisoformat(result_data['timestamp'])
            )
        
        execution = RuleExecution(
            id=data['id'],
            rule_id=data['rule_id'],
            status=ExecutionStatus(data['status']),
            result=result,
            # started_at=datetime.fromisoformat(data['started_at']),  # Removed for LLM caching
            completed_at=datetime.fromisoformat(data['completed_at']) if data.get('completed_at') else None,
            execution_context=data.get('execution_context', {}),
            failure_reason=data.get('failure_reason'),
            confidence_score=data.get('confidence_score', 0.0)
        )
        
        return execution