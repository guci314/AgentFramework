#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶å‘å®‰å…¨æ€§æµ‹è¯•

æµ‹è¯•å¤šä¸ªå·¥ä½œæµå¼•æ“åŒæ—¶è¿è¡Œæ—¶ï¼ŒIDç”Ÿæˆå’Œæ–‡ä»¶ä¿å­˜çš„å®‰å…¨æ€§ã€‚
"""

import sys
import os
import threading
import time
import tempfile
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from cognitive_workflow_rule_base.utils.concurrent_safe_id_generator import (
    ConcurrentSafeIdGenerator, SafeFileOperations, id_generator
)
from cognitive_workflow_rule_base.domain.entities import GlobalState
from cognitive_workflow_rule_base.infrastructure.repository_impl import StateRepositoryImpl
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_concurrent_id_generation():
    """æµ‹è¯•å¹¶å‘IDç”Ÿæˆçš„å”¯ä¸€æ€§"""
    print("\n" + "="*60)
    print("ğŸ”¬ æµ‹è¯•å¹¶å‘IDç”Ÿæˆçš„å”¯ä¸€æ€§")
    print("="*60)
    
    generated_ids = set()
    lock = threading.Lock()
    error_count = 0
    
    def generate_workflow_id(thread_id):
        """åœ¨å•ä¸ªçº¿ç¨‹ä¸­ç”Ÿæˆå·¥ä½œæµID"""
        nonlocal error_count
        try:
            goal = f"å¹¶å‘æµ‹è¯•ç›®æ ‡_{thread_id}"
            workflow_id = id_generator.generate_workflow_id(goal)
            
            with lock:
                if workflow_id in generated_ids:
                    print(f"âŒ æ£€æµ‹åˆ°é‡å¤ID: {workflow_id} (çº¿ç¨‹ {thread_id})")
                    error_count += 1
                    return False
                else:
                    generated_ids.add(workflow_id)
                    print(f"âœ… çº¿ç¨‹ {thread_id}: {workflow_id}")
                    return True
        except Exception as e:
            print(f"âŒ çº¿ç¨‹ {thread_id} ç”ŸæˆIDå¤±è´¥: {e}")
            error_count += 1
            return False
    
    # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘ç”ŸæˆID
    num_threads = 20
    num_ids_per_thread = 5
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = []
        
        for thread_id in range(num_threads):
            for i in range(num_ids_per_thread):
                future = executor.submit(generate_workflow_id, f"{thread_id}_{i}")
                futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        success_count = 0
        for future in as_completed(futures):
            if future.result():
                success_count += 1
    
    total_expected = num_threads * num_ids_per_thread
    print(f"\nğŸ“Š IDç”Ÿæˆç»“æœ:")
    print(f"   é¢„æœŸç”Ÿæˆ: {total_expected} ä¸ª")
    print(f"   æˆåŠŸç”Ÿæˆ: {success_count} ä¸ª")
    print(f"   å”¯ä¸€IDæ•°: {len(generated_ids)} ä¸ª")
    print(f"   é”™è¯¯æ¬¡æ•°: {error_count} æ¬¡")
    
    # éªŒè¯æ‰€æœ‰IDéƒ½æ˜¯å”¯ä¸€çš„
    assert len(generated_ids) == success_count, f"IDå”¯ä¸€æ€§æ£€æŸ¥å¤±è´¥: {len(generated_ids)} != {success_count}"
    assert error_count == 0, f"å­˜åœ¨ {error_count} ä¸ªé”™è¯¯"
    
    print(f"âœ… å¹¶å‘IDç”Ÿæˆæµ‹è¯•é€šè¿‡!")
    
    # æ¸…ç†ç”Ÿæˆçš„ID
    for workflow_id in generated_ids:
        try:
            id_generator.release_workflow_id(workflow_id)
        except Exception:
            pass

def test_atomic_file_operations():
    """æµ‹è¯•åŸå­æ€§æ–‡ä»¶æ“ä½œ"""
    print("\n" + "="*60) 
    print("ğŸ”¬ æµ‹è¯•åŸå­æ€§æ–‡ä»¶æ“ä½œ")
    print("="*60)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        write_errors = []
        read_errors = []
        
        def concurrent_write(thread_id):
            """å¹¶å‘å†™å…¥åŒä¸€ä¸ªæ–‡ä»¶"""
            try:
                file_path = temp_path / "concurrent_test.json"
                test_data = {
                    'thread_id': thread_id,
                    'timestamp': time.time(),
                    'data': [i for i in range(100)]  # ä¸€äº›æµ‹è¯•æ•°æ®
                }
                
                # ä½¿ç”¨åŸå­æ€§å†™å…¥
                success = SafeFileOperations.atomic_write_json(file_path, test_data)
                if not success:
                    write_errors.append(f"çº¿ç¨‹ {thread_id} å†™å…¥å¤±è´¥")
                    return False
                
                print(f"âœ… çº¿ç¨‹ {thread_id} å†™å…¥æˆåŠŸ")
                return True
                
            except Exception as e:
                write_errors.append(f"çº¿ç¨‹ {thread_id} å¼‚å¸¸: {e}")
                return False
        
        def concurrent_read(thread_id):
            """å¹¶å‘è¯»å–æ–‡ä»¶"""
            try:
                file_path = temp_path / "concurrent_test.json"
                
                # ç­‰å¾…æ–‡ä»¶å­˜åœ¨
                max_wait = 50  # æœ€å¤šç­‰å¾…5ç§’
                for _ in range(max_wait):
                    if file_path.exists():
                        break
                    time.sleep(0.1)
                else:
                    read_errors.append(f"çº¿ç¨‹ {thread_id} æ–‡ä»¶ç­‰å¾…è¶…æ—¶")
                    return False
                
                # ä½¿ç”¨å®‰å…¨è¯»å–
                data = SafeFileOperations.safe_read_json(file_path)
                if data is None:
                    read_errors.append(f"çº¿ç¨‹ {thread_id} è¯»å–å¤±è´¥")
                    return False
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if 'thread_id' not in data or 'data' not in data:
                    read_errors.append(f"çº¿ç¨‹ {thread_id} æ•°æ®ä¸å®Œæ•´")
                    return False
                
                print(f"âœ… çº¿ç¨‹ {thread_id} è¯»å–æˆåŠŸ: thread_id={data['thread_id']}")
                return True
                
            except Exception as e:
                read_errors.append(f"çº¿ç¨‹ {thread_id} è¯»å–å¼‚å¸¸: {e}")
                return False
        
        # å¯åŠ¨å¹¶å‘å†™å…¥å’Œè¯»å–
        num_writers = 10
        num_readers = 15
        
        with ThreadPoolExecutor(max_workers=num_writers + num_readers) as executor:
            # æäº¤å†™å…¥ä»»åŠ¡
            write_futures = []
            for i in range(num_writers):
                future = executor.submit(concurrent_write, f"writer_{i}")
                write_futures.append(future)
            
            # ç¨å¾®å»¶è¿Ÿåæäº¤è¯»å–ä»»åŠ¡
            time.sleep(0.1)
            read_futures = []
            for i in range(num_readers):
                future = executor.submit(concurrent_read, f"reader_{i}")
                read_futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            write_success = sum(1 for future in as_completed(write_futures) if future.result())
            read_success = sum(1 for future in as_completed(read_futures) if future.result())
        
        print(f"\nğŸ“Š æ–‡ä»¶æ“ä½œç»“æœ:")
        print(f"   å†™å…¥æˆåŠŸ: {write_success}/{num_writers}")
        print(f"   è¯»å–æˆåŠŸ: {read_success}/{num_readers}")
        print(f"   å†™å…¥é”™è¯¯: {len(write_errors)}")
        print(f"   è¯»å–é”™è¯¯: {len(read_errors)}")
        
        if write_errors:
            print(f"   å†™å…¥é”™è¯¯è¯¦æƒ…: {write_errors[:3]}")
        if read_errors:
            print(f"   è¯»å–é”™è¯¯è¯¦æƒ…: {read_errors[:3]}")
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€äº›æˆåŠŸçš„æ“ä½œ
        assert write_success > 0, "æ²¡æœ‰æˆåŠŸçš„å†™å…¥æ“ä½œ"
        assert read_success > 0, "æ²¡æœ‰æˆåŠŸçš„è¯»å–æ“ä½œ"
        assert len(write_errors) < num_writers, "æ‰€æœ‰å†™å…¥éƒ½å¤±è´¥äº†"
        
        print(f"âœ… åŸå­æ€§æ–‡ä»¶æ“ä½œæµ‹è¯•é€šè¿‡!")

def test_concurrent_state_repository():
    """æµ‹è¯•å¹¶å‘çŠ¶æ€ä»“å‚¨æ“ä½œ"""
    print("\n" + "="*60)
    print("ğŸ”¬ æµ‹è¯•å¹¶å‘çŠ¶æ€ä»“å‚¨æ“ä½œ")
    print("="*60)
    
    # åˆ›å»ºä¸´æ—¶å­˜å‚¨è·¯å¾„
    with tempfile.TemporaryDirectory() as temp_dir:
        storage_path = os.path.join(temp_dir, "states")
        repo = StateRepositoryImpl(storage_path)
        
        save_errors = []
        load_errors = []
        saved_states = []
        lock = threading.Lock()
        
        def concurrent_save_state(thread_id):
            """å¹¶å‘ä¿å­˜çŠ¶æ€"""
            try:
                # ç”Ÿæˆå”¯ä¸€çš„å·¥ä½œæµIDå’ŒçŠ¶æ€
                workflow_id = id_generator.generate_workflow_id(f"concurrent_test_{thread_id}")
                state_id = id_generator.generate_state_id(workflow_id, 0)
                
                global_state = GlobalState(
                    id=state_id,
                    state=f"å¹¶å‘æµ‹è¯•çŠ¶æ€_{thread_id}",
                    workflow_id=workflow_id,
                    iteration_count=0,
                    context_variables={'thread_id': thread_id, 'test_data': list(range(10))}
                )
                
                # ä¿å­˜çŠ¶æ€
                repo.save_state(global_state)
                
                with lock:
                    saved_states.append((workflow_id, state_id))
                
                print(f"âœ… çº¿ç¨‹ {thread_id} çŠ¶æ€ä¿å­˜æˆåŠŸ: {state_id}")
                return True
                
            except Exception as e:
                save_errors.append(f"çº¿ç¨‹ {thread_id} ä¿å­˜å¤±è´¥: {e}")
                print(f"âŒ çº¿ç¨‹ {thread_id} ä¿å­˜å¤±è´¥: {e}")
                return False
        
        def concurrent_load_state(state_info):
            """å¹¶å‘åŠ è½½çŠ¶æ€"""
            workflow_id, state_id = state_info
            try:
                # åŠ è½½çŠ¶æ€
                loaded_state = repo.load_state(state_id)
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if loaded_state.id != state_id:
                    load_errors.append(f"çŠ¶æ€IDä¸åŒ¹é…: {loaded_state.id} != {state_id}")
                    return False
                
                if loaded_state.workflow_id != workflow_id:
                    load_errors.append(f"å·¥ä½œæµIDä¸åŒ¹é…: {loaded_state.workflow_id} != {workflow_id}")
                    return False
                
                print(f"âœ… çŠ¶æ€åŠ è½½æˆåŠŸ: {state_id}")
                return True
                
            except Exception as e:
                load_errors.append(f"åŠ è½½çŠ¶æ€ {state_id} å¤±è´¥: {e}")
                print(f"âŒ åŠ è½½çŠ¶æ€ {state_id} å¤±è´¥: {e}")
                return False
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶å‘ä¿å­˜çŠ¶æ€
        num_save_threads = 15
        
        with ThreadPoolExecutor(max_workers=num_save_threads) as executor:
            save_futures = []
            for i in range(num_save_threads):
                future = executor.submit(concurrent_save_state, i)
                save_futures.append(future)
            
            save_success = sum(1 for future in as_completed(save_futures) if future.result())
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘åŠ è½½çŠ¶æ€
        with ThreadPoolExecutor(max_workers=len(saved_states)) as executor:
            load_futures = []
            for state_info in saved_states:
                future = executor.submit(concurrent_load_state, state_info)
                load_futures.append(future)
            
            load_success = sum(1 for future in as_completed(load_futures) if future.result())
        
        print(f"\nğŸ“Š çŠ¶æ€ä»“å‚¨æ“ä½œç»“æœ:")
        print(f"   ä¿å­˜æˆåŠŸ: {save_success}/{num_save_threads}")
        print(f"   åŠ è½½æˆåŠŸ: {load_success}/{len(saved_states)}")
        print(f"   ä¿å­˜é”™è¯¯: {len(save_errors)}")
        print(f"   åŠ è½½é”™è¯¯: {len(load_errors)}")
        
        if save_errors:
            print(f"   ä¿å­˜é”™è¯¯è¯¦æƒ…: {save_errors[:3]}")
        if load_errors:
            print(f"   åŠ è½½é”™è¯¯è¯¦æƒ…: {load_errors[:3]}")
        
        # éªŒè¯ç»“æœ
        assert save_success > 0, "æ²¡æœ‰æˆåŠŸä¿å­˜çš„çŠ¶æ€"
        assert load_success == len(saved_states), f"åŠ è½½æˆåŠŸç‡ä¸ç¬¦åˆé¢„æœŸ: {load_success} != {len(saved_states)}"
        
        print(f"âœ… å¹¶å‘çŠ¶æ€ä»“å‚¨æ“ä½œæµ‹è¯•é€šè¿‡!")
        
        # æ¸…ç†å·¥ä½œæµID
        for workflow_id, _ in saved_states:
            try:
                id_generator.release_workflow_id(workflow_id)
            except Exception:
                pass

def test_real_world_scenario():
    """æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåœºæ™¯ï¼šå¤šä¸ªå·¥ä½œæµå¼•æ“åŒæ—¶å¯åŠ¨ç›¸åŒç›®æ ‡"""
    print("\n" + "="*60)
    print("ğŸ”¬ æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼šå¤šå·¥ä½œæµå¼•æ“åŒæ—¶å¯åŠ¨")
    print("="*60)
    
    # æ¨¡æ‹Ÿå¤šä¸ªå·¥ä½œæµå¼•æ“åŒæ—¶å¤„ç†ç›¸åŒçš„ç›®æ ‡
    same_goal = "å¤„ç†ç”¨æˆ·è¯·æ±‚æ•°æ®åˆ†æ"
    num_engines = 8
    results = []
    errors = []
    
    def simulate_workflow_engine(engine_id):
        """æ¨¡æ‹Ÿå•ä¸ªå·¥ä½œæµå¼•æ“çš„å¯åŠ¨"""
        try:
            # 1. ç”Ÿæˆå·¥ä½œæµID
            workflow_id = id_generator.generate_workflow_id(same_goal)
            
            # 2. ç”Ÿæˆè§„åˆ™é›†ID
            rule_set_id = id_generator.generate_rule_set_id(same_goal)
            
            # 3. ç”Ÿæˆå¤šä¸ªçŠ¶æ€ID
            state_ids = []
            for i in range(3):
                state_id = id_generator.generate_state_id(workflow_id, i)
                state_ids.append(state_id)
            
            # 4. ç”Ÿæˆæ‰§è¡ŒID
            execution_ids = []
            for i in range(2):
                exec_id = id_generator.generate_execution_id(f"rule_{i}")
                execution_ids.append(exec_id)
            
            result = {
                'engine_id': engine_id,
                'workflow_id': workflow_id,
                'rule_set_id': rule_set_id,
                'state_ids': state_ids,
                'execution_ids': execution_ids
            }
            
            print(f"âœ… å¼•æ“ {engine_id}: {workflow_id}")
            results.append(result)
            
            # æ¨¡æ‹Ÿå·¥ä½œæµå®Œæˆ
            time.sleep(0.1)
            id_generator.release_workflow_id(workflow_id)
            
            return True
            
        except Exception as e:
            error_msg = f"å¼•æ“ {engine_id} å¤±è´¥: {e}"
            errors.append(error_msg)
            print(f"âŒ {error_msg}")
            return False
    
    # å¹¶å‘å¯åŠ¨å¤šä¸ªå·¥ä½œæµå¼•æ“
    with ThreadPoolExecutor(max_workers=num_engines) as executor:
        futures = []
        for engine_id in range(num_engines):
            future = executor.submit(simulate_workflow_engine, engine_id)
            futures.append(future)
        
        success_count = sum(1 for future in as_completed(futures) if future.result())
    
    # éªŒè¯æ‰€æœ‰IDçš„å”¯ä¸€æ€§
    all_workflow_ids = {r['workflow_id'] for r in results}
    all_rule_set_ids = {r['rule_set_id'] for r in results}
    all_state_ids = {sid for r in results for sid in r['state_ids']}
    all_execution_ids = {eid for r in results for eid in r['execution_ids']}
    
    print(f"\nğŸ“Š çœŸå®åœºæ™¯æ¨¡æ‹Ÿç»“æœ:")
    print(f"   æˆåŠŸå¼•æ“: {success_count}/{num_engines}")
    print(f"   å¤±è´¥æ¬¡æ•°: {len(errors)}")
    print(f"   å”¯ä¸€å·¥ä½œæµID: {len(all_workflow_ids)}")
    print(f"   å”¯ä¸€è§„åˆ™é›†ID: {len(all_rule_set_ids)}")
    print(f"   å”¯ä¸€çŠ¶æ€ID: {len(all_state_ids)}")
    print(f"   å”¯ä¸€æ‰§è¡ŒID: {len(all_execution_ids)}")
    
    if errors:
        print(f"   é”™è¯¯è¯¦æƒ…: {errors}")
    
    # éªŒè¯å”¯ä¸€æ€§
    expected_workflow_ids = success_count
    expected_rule_set_ids = success_count
    expected_state_ids = success_count * 3
    expected_execution_ids = success_count * 2
    
    assert len(all_workflow_ids) == expected_workflow_ids, f"å·¥ä½œæµIDä¸å”¯ä¸€: {len(all_workflow_ids)} != {expected_workflow_ids}"
    assert len(all_rule_set_ids) == expected_rule_set_ids, f"è§„åˆ™é›†IDä¸å”¯ä¸€: {len(all_rule_set_ids)} != {expected_rule_set_ids}"
    assert len(all_state_ids) == expected_state_ids, f"çŠ¶æ€IDä¸å”¯ä¸€: {len(all_state_ids)} != {expected_state_ids}"
    assert len(all_execution_ids) == expected_execution_ids, f"æ‰§è¡ŒIDä¸å”¯ä¸€: {len(all_execution_ids)} != {expected_execution_ids}"
    
    print(f"âœ… çœŸå®åœºæ™¯æ¨¡æ‹Ÿæµ‹è¯•é€šè¿‡!")

def main():
    """è¿è¡Œæ‰€æœ‰å¹¶å‘å®‰å…¨æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å¹¶å‘å®‰å…¨æ€§æµ‹è¯•")
    print("="*80)
    
    try:
        test_concurrent_id_generation()
        test_atomic_file_operations()
        test_concurrent_state_repository()
        test_real_world_scenario()
        
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰å¹¶å‘å®‰å…¨æµ‹è¯•é€šè¿‡ï¼")
        print("="*80)
        
        print("\nğŸ“‹ å¹¶å‘å®‰å…¨ç‰¹æ€§éªŒè¯:")
        print("âœ… å”¯ä¸€IDç”Ÿæˆæœºåˆ¶å·¥ä½œæ­£å¸¸")
        print("âœ… åŸå­æ€§æ–‡ä»¶æ“ä½œé˜²æ­¢æ•°æ®æŸå")
        print("âœ… çŠ¶æ€ä»“å‚¨å¹¶å‘è®¿é—®å®‰å…¨")
        print("âœ… å¤šå·¥ä½œæµå¼•æ“å¯ä»¥å®‰å…¨å¹¶å‘è¿è¡Œ")
        print("\nğŸ”§ å¤šä¸ªå·¥ä½œæµå¼•æ“åŒæ—¶è¿è¡Œä¸ä¼šå†å‡ºç°JSONæ–‡ä»¶äº’ç›¸è¦†ç›–çš„é—®é¢˜ï¼")
        
    except Exception as e:
        print(f"\nâŒ å¹¶å‘å®‰å…¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)