#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è‡ªé€‚åº”è§„åˆ™æ›¿æ¢åŠŸèƒ½æµ‹è¯•

æµ‹è¯•è‡ªé€‚åº”æ›¿æ¢æœåŠ¡çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒéªŒè¯ä¸åŒç­–ç•¥çš„é€‰æ‹©å’Œæ‰§è¡Œã€‚
"""

import sys
import os
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "CognitiveWorkflow"))

from cognitive_workflow_rule_base.services.adaptive_replacement_service import AdaptiveReplacementService
from cognitive_workflow_rule_base.services.language_model_service import LanguageModelService
from cognitive_workflow_rule_base.domain.entities import ProductionRule, GlobalState
from cognitive_workflow_rule_base.domain.value_objects import RulePhase, ReplacementStrategyType
from pythonTask import llm_deepseek

def create_test_rules():
    """åˆ›å»ºæµ‹è¯•è§„åˆ™"""
    existing_rules = [
        ProductionRule(
            id="rule_001",
            name="åˆ†æéœ€æ±‚",
            condition="éœ€è¦åˆ†æç”¨æˆ·éœ€æ±‚",
            action="ä»”ç»†åˆ†æç”¨æˆ·çš„å…·ä½“éœ€æ±‚",
            agent_name="coder",
            priority=80,
            phase=RulePhase.INFORMATION_GATHERING,
            expected_outcome="æ˜ç¡®éœ€æ±‚è§„æ ¼"
        ),
        ProductionRule(
            id="rule_002", 
            name="ç¼–å†™ä»£ç ",
            condition="éœ€è¦ç¼–å†™ä»£ç å®ç°",
            action="æ ¹æ®éœ€æ±‚ç¼–å†™é«˜è´¨é‡ä»£ç ",
            agent_name="coder",
            priority=70,
            phase=RulePhase.EXECUTION,
            expected_outcome="å®Œæˆä»£ç å®ç°"
        ),
        ProductionRule(
            id="rule_003",
            name="æµ‹è¯•éªŒè¯",
            condition="éœ€è¦æµ‹è¯•ä»£ç åŠŸèƒ½",
            action="è¿è¡Œæµ‹è¯•ç¡®ä¿ä»£ç æ­£ç¡®",
            agent_name="coder",
            priority=60,
            phase=RulePhase.VERIFICATION,
            expected_outcome="æµ‹è¯•é€šè¿‡"
        )
    ]
    
    new_rules = [
        ProductionRule(
            id="rule_004",
            name="ä¼˜åŒ–ä»£ç ",
            condition="éœ€è¦ä¼˜åŒ–ä»£ç æ€§èƒ½",
            action="å¯¹ä»£ç è¿›è¡Œæ€§èƒ½ä¼˜åŒ–",
            agent_name="coder",
            priority=85,
            phase=RulePhase.EXECUTION,
            expected_outcome="ä»£ç æ€§èƒ½æå‡"
        ),
        ProductionRule(
            id="rule_005",
            name="æ–‡æ¡£ç¼–å†™",
            condition="éœ€è¦ç¼–å†™æŠ€æœ¯æ–‡æ¡£",
            action="ç¼–å†™æ¸…æ™°çš„æŠ€æœ¯æ–‡æ¡£",
            agent_name="coder",
            priority=50,
            phase=RulePhase.VERIFICATION,
            expected_outcome="æ–‡æ¡£å®Œæˆ"
        )
    ]
    
    return existing_rules, new_rules

def create_test_global_state():
    """åˆ›å»ºæµ‹è¯•å…¨å±€çŠ¶æ€"""
    return GlobalState(
        id="test_state_001",
        state="ç³»ç»Ÿæ­£åœ¨æ‰§è¡Œæµ‹è¯•ä»»åŠ¡ï¼Œå½“å‰éœ€è¦ä¼˜åŒ–è§„åˆ™é›†",
        context_variables={
            'current_phase': 'execution',
            'task_complexity': 'medium'
        },
        execution_history=[
            "è§„åˆ™æ‰§è¡ŒæˆåŠŸ: åˆ†æéœ€æ±‚",
            "è§„åˆ™æ‰§è¡ŒæˆåŠŸ: ç¼–å†™ä»£ç ", 
            "è§„åˆ™æ‰§è¡Œå¤±è´¥: æµ‹è¯•éªŒè¯ - å‘ç°é”™è¯¯",
            "è§„åˆ™æ‰§è¡ŒæˆåŠŸ: ä¼˜åŒ–ä»£ç "
        ],
        workflow_id="test_workflow",
        iteration_count=4,
        goal_achieved=False
    )

def test_situation_assessment():
    """æµ‹è¯•æƒ…å¢ƒè¯„ä¼°åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æƒ…å¢ƒè¯„ä¼°åŠŸèƒ½...")
    
    llm_service = LanguageModelService(llm_deepseek)
    adaptive_service = AdaptiveReplacementService(llm_service)
    
    existing_rules, new_rules = create_test_rules()
    global_state = create_test_global_state()
    context = {
        'goal': 'æµ‹è¯•è‡ªé€‚åº”æ›¿æ¢åŠŸèƒ½',
        'iteration_count': 4,
        'max_iterations': 20
    }
    
    # è¯„ä¼°å½“å‰æƒ…å¢ƒ
    situation_score = adaptive_service._assess_current_situation(
        existing_rules, global_state, context
    )
    
    print(f"ğŸ“Š æƒ…å¢ƒè¯„ä¼°ç»“æœ:")
    print(f"   è§„åˆ™å¯†åº¦: {situation_score.rule_density:.2f}")
    print(f"   æ‰§è¡Œæ•ˆç‡: {situation_score.execution_efficiency:.2f}")  
    print(f"   ç›®æ ‡è¿›åº¦: {situation_score.goal_progress:.2f}")
    print(f"   å¤±è´¥é¢‘ç‡: {situation_score.failure_frequency:.2f}")
    print(f"   æ™ºèƒ½ä½“åˆ©ç”¨ç‡: {situation_score.agent_utilization:.2f}")
    print(f"   é˜¶æ®µåˆ†å¸ƒ: {situation_score.phase_distribution:.2f}")
    print(f"   æ•´ä½“å¥åº·åº¦: {situation_score.get_overall_health():.2f}")
    print(f"   å…³é”®é—®é¢˜: {situation_score.get_critical_issues()}")
    
    return situation_score

def test_strategy_selection(situation_score):
    """æµ‹è¯•ç­–ç•¥é€‰æ‹©åŠŸèƒ½"""
    print("\nğŸ¯ æµ‹è¯•ç­–ç•¥é€‰æ‹©åŠŸèƒ½...")
    
    llm_service = LanguageModelService(llm_deepseek)
    adaptive_service = AdaptiveReplacementService(llm_service)
    
    context = {
        'goal': 'æµ‹è¯•è‡ªé€‚åº”æ›¿æ¢åŠŸèƒ½',
        'iteration_count': 4,
        'existing_rules': [1, 2, 3]  # ç®€åŒ–è¡¨ç¤º
    }
    
    # é€‰æ‹©ç­–ç•¥
    strategy = adaptive_service._select_optimal_strategy(situation_score, context)
    
    print(f"ğŸ“‹ é€‰æ‹©çš„ç­–ç•¥:")
    print(f"   ç­–ç•¥ç±»å‹: {strategy.strategy_type.value}")
    print(f"   ç­–ç•¥æè¿°: {strategy.get_strategy_description()}")
    print(f"   æ›¿æ¢æ¯”ä¾‹: {strategy.replacement_ratio:.2f}")
    print(f"   ç›¸ä¼¼æ€§é˜ˆå€¼: {strategy.similarity_threshold:.2f}")
    print(f"   æ€§èƒ½é˜ˆå€¼: {strategy.performance_threshold:.2f}")
    print(f"   ä¿å®ˆæ¨¡å¼: {strategy.conservative_mode}")
    print(f"   æ¿€è¿›ç­–ç•¥: {strategy.is_aggressive_strategy()}")
    
    return strategy

def test_adaptive_replacement():
    """æµ‹è¯•å®Œæ•´çš„è‡ªé€‚åº”æ›¿æ¢æµç¨‹"""
    print("\nğŸ”„ æµ‹è¯•å®Œæ•´çš„è‡ªé€‚åº”æ›¿æ¢æµç¨‹...")
    
    llm_service = LanguageModelService(llm_deepseek)
    adaptive_service = AdaptiveReplacementService(llm_service)
    
    existing_rules, new_rules = create_test_rules()
    global_state = create_test_global_state()
    context = {
        'goal': 'æµ‹è¯•è‡ªé€‚åº”æ›¿æ¢åŠŸèƒ½',
        'iteration_count': 4,
        'context_type': 'strategy_adjustment'
    }
    
    print(f"ğŸ“¥ è¾“å…¥è§„åˆ™:")
    print(f"   ç°æœ‰è§„åˆ™: {len(existing_rules)} ä¸ª")
    for rule in existing_rules:
        print(f"     - {rule.name} (ä¼˜å…ˆçº§: {rule.priority}, é˜¶æ®µ: {rule.phase.value})")
    print(f"   æ–°è§„åˆ™: {len(new_rules)} ä¸ª") 
    for rule in new_rules:
        print(f"     - {rule.name} (ä¼˜å…ˆçº§: {rule.priority}, é˜¶æ®µ: {rule.phase.value})")
    
    # æ‰§è¡Œè‡ªé€‚åº”æ›¿æ¢
    optimized_rules = adaptive_service.execute_adaptive_replacement(
        existing_rules=existing_rules,
        new_rules=new_rules,
        global_state=global_state,
        context=context
    )
    
    print(f"\nğŸ“¤ è¾“å‡ºè§„åˆ™:")
    print(f"   ä¼˜åŒ–åè§„åˆ™: {len(optimized_rules)} ä¸ª")
    for rule in optimized_rules:
        print(f"     - {rule.name} (ä¼˜å…ˆçº§: {rule.priority}, é˜¶æ®µ: {rule.phase.value})")
    
    # åˆ†ææ›¿æ¢æ•ˆæœ
    print(f"\nğŸ“ˆ æ›¿æ¢æ•ˆæœåˆ†æ:")
    print(f"   è§„åˆ™æ•°é‡å˜åŒ–: {len(existing_rules)} + {len(new_rules)} -> {len(optimized_rules)}")
    
    # åˆ†æé˜¶æ®µåˆ†å¸ƒ
    phase_dist = {}
    for rule in optimized_rules:
        phase = rule.phase.value
        phase_dist[phase] = phase_dist.get(phase, 0) + 1
    print(f"   é˜¶æ®µåˆ†å¸ƒ: {phase_dist}")
    
    # åˆ†æä¼˜å…ˆçº§åˆ†å¸ƒ
    avg_priority = sum(rule.priority for rule in optimized_rules) / len(optimized_rules)
    print(f"   å¹³å‡ä¼˜å…ˆçº§: {avg_priority:.1f}")
    
    return optimized_rules

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª è‡ªé€‚åº”è§„åˆ™æ›¿æ¢åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    try:
        # 1. æµ‹è¯•æƒ…å¢ƒè¯„ä¼°
        situation_score = test_situation_assessment()
        
        # 2. æµ‹è¯•ç­–ç•¥é€‰æ‹©
        strategy = test_strategy_selection(situation_score)
        
        # 3. æµ‹è¯•å®Œæ•´æ›¿æ¢æµç¨‹
        optimized_rules = test_adaptive_replacement()
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ‰ è‡ªé€‚åº”æ›¿æ¢åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()