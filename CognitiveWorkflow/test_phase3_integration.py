# -*- coding: utf-8 -*-
"""
Phase 3 é›†æˆæµ‹è¯•

æµ‹è¯•æ‰€æœ‰Phase 3 Self-Learning Optimizationç»„ä»¶çš„é›†æˆå’ŒååŒå·¥ä½œã€‚
"""

import unittest
import sys
import os
import time
import json
import random
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from cognitive_workflow_rule_base.domain.value_objects import (
    ReplacementStrategyType, StrategyEffectiveness, SituationScore,
    ExecutionMetrics, AdaptiveReplacementConstants
)
from cognitive_workflow_rule_base.services.dynamic_parameter_optimizer import (
    DynamicParameterOptimizer, OptimizationAlgorithm
)
from cognitive_workflow_rule_base.services.advanced_pattern_recognition import (
    AdvancedPatternRecognitionEngine
)
from cognitive_workflow_rule_base.services.predictive_optimization_framework import (
    PredictiveOptimizationFramework
)
from cognitive_workflow_rule_base.services.strategy_effectiveness_tracker import (
    StrategyEffectivenessTracker
)
from cognitive_workflow_rule_base.services.reinforcement_learning_optimizer import (
    ReinforcementLearningOptimizer, RLAlgorithmType
)
from cognitive_workflow_rule_base.services.adaptive_hyperparameter_optimizer import (
    AdaptiveHyperparameterOptimizer, OptimizationMethod, PerformanceObjectiveFunction
)
from cognitive_workflow_rule_base.services.intelligent_performance_benchmark import (
    IntelligentPerformanceBenchmark, BenchmarkType
)


class TestPhase3Integration(unittest.TestCase):
    """Phase 3 é›†æˆæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        print("\n" + "="*80)
        print("ğŸš€ Phase 3: Self-Learning Optimization é›†æˆæµ‹è¯•")
        print("="*80)
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.effectiveness_tracker = StrategyEffectivenessTracker()
        self.parameter_optimizer = DynamicParameterOptimizer(
            optimization_algorithm=OptimizationAlgorithm.ADAPTIVE_LEARNING_RATE
        )
        self.pattern_engine = AdvancedPatternRecognitionEngine()
        self.predictive_framework = PredictiveOptimizationFramework(
            self.parameter_optimizer,
            self.pattern_engine,
            self.effectiveness_tracker
        )
        self.rl_optimizer = ReinforcementLearningOptimizer(
            algorithm_type=RLAlgorithmType.Q_LEARNING
        )
        self.hyperparameter_optimizer = AdaptiveHyperparameterOptimizer(
            optimization_method=OptimizationMethod.BAYESIAN_OPTIMIZATION
        )
        self.benchmark_system = IntelligentPerformanceBenchmark()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        self.test_history = self._generate_test_effectiveness_history()
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆï¼Œç”Ÿæˆäº†{len(self.test_history)}æ¡æµ‹è¯•æ•°æ®")
    
    def _generate_test_effectiveness_history(self) -> list:
        """ç”Ÿæˆæµ‹è¯•ç”¨çš„ç­–ç•¥æ•ˆæœå†å²"""
        history = []
        base_time = datetime.now() - timedelta(hours=24)
        
        strategies = list(ReplacementStrategyType)
        
        for i in range(50):  # ç”Ÿæˆ50æ¡å†å²è®°å½•
            # åˆ›å»ºæ¸è¿›æ”¹å–„çš„æƒ…å¢ƒ
            health_trend = 0.3 + (i / 100.0)  # ä»0.3é€æ­¥æ”¹å–„åˆ°0.8
            
            context = SituationScore(
                rule_density=0.4 + (i % 10) * 0.03,
                execution_efficiency=0.5 + health_trend * 0.4,
                goal_progress=0.3 + health_trend * 0.5,
                failure_frequency=max(0.05, 0.3 - health_trend * 0.2),
                agent_utilization=0.4 + (i % 8) * 0.06,
                phase_distribution=0.5 + (i % 6) * 0.05
            )
            
            strategy = strategies[i % len(strategies)]
            
            # æ¨¡æ‹Ÿä¸€äº›ç­–ç•¥æ¯”å…¶ä»–ç­–ç•¥æ›´æœ‰æ•ˆ
            base_score = 0.4
            if strategy == ReplacementStrategyType.PERFORMANCE_FOCUSED:
                base_score = 0.7
            elif strategy == ReplacementStrategyType.STRATEGIC_PIVOT:
                base_score = 0.6
            
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–å’Œæ—¶é—´è¶‹åŠ¿
            improvement_score = base_score + health_trend * 0.2 + (i % 7 - 3) * 0.05
            improvement_score = max(0.1, min(0.95, improvement_score))
            
            # åˆ›å»ºbeforeå’ŒafteræŒ‡æ ‡
            total_rules = 10 + (i % 5)
            successful_before = int((0.6 + (i % 10) * 0.02) * total_rules)
            failed_before = total_rules - successful_before
            
            before_metrics = ExecutionMetrics(
                total_rules_executed=total_rules,
                successful_executions=successful_before,
                failed_executions=failed_before,
                average_execution_time=1.0 + (i % 8) * 0.1,
                total_execution_time=(1.0 + (i % 8) * 0.1) * total_rules,
                rule_match_accuracy=0.85 + (i % 10) * 0.01
            )
            
            total_rules_after = total_rules + 2
            successful_after = int((improvement_score * 0.8 + 0.2) * total_rules_after)
            failed_after = total_rules_after - successful_after
            
            after_metrics = ExecutionMetrics(
                total_rules_executed=total_rules_after,
                successful_executions=successful_after,
                failed_executions=failed_after,
                average_execution_time=max(0.5, before_metrics.average_execution_time - 0.2),
                total_execution_time=max(0.5, before_metrics.average_execution_time - 0.2) * total_rules_after,
                rule_match_accuracy=min(0.95, before_metrics.rule_match_accuracy + 0.05)
            )
            
            effectiveness = StrategyEffectiveness(
                strategy_type=strategy,
                applied_context=context,
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                improvement_score=improvement_score,
                application_timestamp=base_time + timedelta(minutes=i * 30)
            )
            
            history.append(effectiveness)
        
        return history
    
    def test_01_effectiveness_tracking_integration(self):
        """æµ‹è¯•ç­–ç•¥æ•ˆæœè·Ÿè¸ªé›†æˆ"""
        print("\nğŸ“Š æµ‹è¯•1: ç­–ç•¥æ•ˆæœè·Ÿè¸ªé›†æˆ")
        
        # æ‰¹é‡è®°å½•æ•ˆæœæ•°æ®
        for effectiveness in self.test_history:
            self.effectiveness_tracker.record_strategy_application(
                strategy_type=effectiveness.strategy_type,
                applied_context=effectiveness.applied_context,
                before_metrics=effectiveness.before_metrics,
                after_metrics=effectiveness.after_metrics,
                improvement_score=effectiveness.improvement_score
            )
        
        # è·å–è·Ÿè¸ªå™¨çŠ¶æ€
        export_data = self.effectiveness_tracker.export_performance_data('summary')
        print(f"   â€¢ è·Ÿè¸ªçš„ç­–ç•¥æ•°é‡: {len([s for s in export_data['strategy_performance'].values() if s])}")
        print(f"   â€¢ æ€»è®°å½•æ•°: {export_data['total_applications']}")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        self.assertEqual(export_data['total_applications'], len(self.test_history))
        self.assertGreater(len([s for s in export_data['strategy_performance'].values() if s]), 0)
        
        # è·å–æœ€ä½³ç­–ç•¥
        best_strategy = export_data.get('best_performing_strategy', 'performance_focused')
        print(f"   â€¢ æœ€ä½³ç­–ç•¥: {best_strategy}")
        
        self.assertIsInstance(best_strategy, str)
        self.assertGreater(len(best_strategy), 0)
        print("   âœ… ç­–ç•¥æ•ˆæœè·Ÿè¸ªé›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_02_pattern_recognition_integration(self):
        """æµ‹è¯•æ¨¡å¼è¯†åˆ«é›†æˆ"""
        print("\nğŸ” æµ‹è¯•2: é«˜çº§æ¨¡å¼è¯†åˆ«é›†æˆ")
        
        # åˆ†ææ¨¡å¼
        pattern_analysis = self.pattern_engine.analyze_patterns(
            self.test_history, include_predictions=True
        )
        
        print(f"   â€¢ å‘ç°çš„æ¨¡å¼æ€»æ•°: {pattern_analysis['summary']['total_patterns']}")
        print(f"   â€¢ é«˜ç½®ä¿¡åº¦æ¨¡å¼: {pattern_analysis['summary']['high_confidence_patterns']}")
        print(f"   â€¢ é¢„æµ‹æ•°é‡: {len(pattern_analysis['predictions'])}")
        
        # éªŒè¯æ¨¡å¼è¯†åˆ«ç»“æœ
        self.assertGreater(pattern_analysis['summary']['total_patterns'], 0)
        
        # æ£€æŸ¥å„ç±»å‹æ¨¡å¼
        patterns = pattern_analysis['patterns']
        for pattern_type, pattern_list in patterns.items():
            if pattern_list:
                print(f"   â€¢ {pattern_type}æ¨¡å¼: {len(pattern_list)}ä¸ª")
        
        # éªŒè¯é¢„æµ‹
        if pattern_analysis['predictions']:
            pred = pattern_analysis['predictions'][0]
            print(f"   â€¢ é¦–ä¸ªé¢„æµ‹: {pred['predicted_pattern']['description'][:50]}...")
        
        print("   âœ… é«˜çº§æ¨¡å¼è¯†åˆ«é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_03_parameter_optimization_integration(self):
        """æµ‹è¯•åŠ¨æ€å‚æ•°ä¼˜åŒ–é›†æˆ"""
        print("\nâš™ï¸ æµ‹è¯•3: åŠ¨æ€å‚æ•°ä¼˜åŒ–é›†æˆ")
        
        # å‡†å¤‡å‚æ•°å’Œä¸Šä¸‹æ–‡
        current_params = {
            'replacement_ratio': 0.3,
            'similarity_threshold': 0.8,
            'performance_threshold': 0.7,
            'learning_rate': 0.01
        }
        
        current_context = self.test_history[-1].applied_context
        strategy_type = ReplacementStrategyType.PERFORMANCE_FOCUSED
        
        # æ‰§è¡Œå‚æ•°ä¼˜åŒ–
        optimized_params = self.parameter_optimizer.optimize_parameters(
            strategy_type=strategy_type,
            current_parameters=current_params,
            performance_history=self.test_history[-10:],  # æœ€è¿‘10æ¡è®°å½•
            situation_context=current_context
        )
        
        print(f"   â€¢ åŸå§‹å‚æ•°: {current_params}")
        print(f"   â€¢ ä¼˜åŒ–å‚æ•°: {optimized_params}")
        
        # éªŒè¯å‚æ•°ä¼˜åŒ–
        self.assertIsInstance(optimized_params, dict)
        self.assertEqual(len(optimized_params), len(current_params))
        
        # è·å–ä¼˜åŒ–æ‘˜è¦
        optimization_summary = self.parameter_optimizer.get_optimization_summary()
        print(f"   â€¢ ä¼˜åŒ–è¿­ä»£: {optimization_summary.get('total_iterations', 0)}")
        
        print("   âœ… åŠ¨æ€å‚æ•°ä¼˜åŒ–é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_04_reinforcement_learning_integration(self):
        """æµ‹è¯•å¼ºåŒ–å­¦ä¹ é›†æˆ"""
        print("\nğŸ¯ æµ‹è¯•4: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–é›†æˆ")
        
        # åˆå§‹åŒ–å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨
        rl_optimizer = ReinforcementLearningOptimizer(
            algorithm_type=RLAlgorithmType.Q_LEARNING
        )
        
        # æ¨¡æ‹Ÿå­¦ä¹ è¿‡ç¨‹
        for i, effectiveness in enumerate(self.test_history[-20:]):  # ä½¿ç”¨æœ€è¿‘20æ¡æ•°æ®
            # ç¼–ç çŠ¶æ€
            if i == 0:
                last_strategy = ReplacementStrategyType.MINIMAL_REPLACEMENT
                last_performance = 0.5
            else:
                last_strategy = self.test_history[-(21-i)].strategy_type
                last_performance = self.test_history[-(21-i)].improvement_score
            
            state = rl_optimizer.encode_state(
                situation=effectiveness.applied_context,
                last_strategy=last_strategy,
                last_performance=last_performance,
                time_since_last=30  # å‡è®¾30åˆ†é’Ÿé—´éš”
            )
            
            # é€‰æ‹©è¡ŒåŠ¨
            action = rl_optimizer.choose_action(state)
            
            # è®¡ç®—å¥–åŠ±
            previous_health = 0.5 if i == 0 else self.test_history[-(21-i)].applied_context.get_overall_health()
            reward = rl_optimizer.calculate_reward(effectiveness, previous_health)
            
            # ä¸‹ä¸€çŠ¶æ€
            if i < len(self.test_history[-20:]) - 1:
                next_effectiveness = self.test_history[-(20-i-1)]
                next_state = rl_optimizer.encode_state(
                    situation=next_effectiveness.applied_context,
                    last_strategy=effectiveness.strategy_type,
                    last_performance=effectiveness.improvement_score
                )
                done = False
            else:
                next_state = state  # æœ€åä¸€ä¸ªçŠ¶æ€
                done = True
            
            # å­¦ä¹ 
            rl_optimizer.learn_from_experience(state, action, reward, next_state, done)
        
        # è·å–å­¦ä¹ ç»Ÿè®¡
        learning_stats = rl_optimizer.get_learning_statistics()
        print(f"   â€¢ æ€»ç»éªŒæ•°: {learning_stats['total_experiences']}")
        print(f"   â€¢ å¹³å‡å¥–åŠ±: {learning_stats['average_reward']:.3f}")
        print(f"   â€¢ æ¢ç´¢ç‡: {learning_stats['exploration_rate']:.3f}")
        
        # è·å–ç­–ç•¥å»ºè®®
        if self.test_history:
            test_state = rl_optimizer.encode_state(
                situation=self.test_history[-1].applied_context,
                last_strategy=self.test_history[-1].strategy_type,
                last_performance=self.test_history[-1].improvement_score
            )
            
            recommendations = rl_optimizer.get_policy_recommendations(test_state)
            print(f"   â€¢ ç­–ç•¥å»ºè®®æ•°: {len(recommendations)}")
        
        # éªŒè¯å­¦ä¹ æ•ˆæœ
        self.assertGreater(learning_stats['total_experiences'], 0)
        
        print("   âœ… å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_05_predictive_optimization_integration(self):
        """æµ‹è¯•é¢„æµ‹æ€§ä¼˜åŒ–æ¡†æ¶é›†æˆ"""
        print("\nğŸ”® æµ‹è¯•5: é¢„æµ‹æ€§ä¼˜åŒ–æ¡†æ¶é›†æˆ")
        
        # åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹
        self.predictive_framework.initialize_predictive_models(self.test_history)
        
        # ç”Ÿæˆç³»ç»Ÿé¢„æµ‹
        current_context = self.test_history[-1].applied_context
        current_params = {
            'replacement_ratio': 0.3,
            'similarity_threshold': 0.8,
            'performance_threshold': 0.7
        }
        
        predictions = self.predictive_framework.generate_system_predictions(
            current_context, current_params
        )
        
        print(f"   â€¢ ç”Ÿæˆé¢„æµ‹æ•°: {len(predictions)}")
        
        if predictions:
            pred = predictions[0]
            print(f"   â€¢ é¦–ä¸ªé¢„æµ‹ç½®ä¿¡åº¦: {pred.confidence:.3f}")
            print(f"   â€¢ é¢„æµ‹å¥åº·åº¦: {pred.predicted_situation.get_overall_health():.3f}")
        
        # åˆ›å»ºä¼˜åŒ–è®¡åˆ’
        if predictions:
            optimization_plans = self.predictive_framework.create_optimization_plans(
                predictions, current_params
            )
            
            print(f"   â€¢ ä¼˜åŒ–è®¡åˆ’æ•°: {len(optimization_plans)}")
            
            if optimization_plans:
                plan = optimization_plans[0]
                print(f"   â€¢ é¦–ä¸ªè®¡åˆ’è¡ŒåŠ¨æ•°: {len(plan.actions)}")
                print(f"   â€¢ è®¡åˆ’ç½®ä¿¡åº¦: {plan.confidence:.3f}")
        
        # è·å–æ¡†æ¶çŠ¶æ€
        framework_status = self.predictive_framework.get_framework_status()
        print(f"   â€¢ é¢„æµ‹æ¨¡å‹æ•°: {len(framework_status.get('prediction_models', {}))}")
        print(f"   â€¢ æ¡†æ¶å¥åº·åº¦: {framework_status.get('framework_health', {}).get('overall_health', 0):.3f}")
        
        # éªŒè¯é¢„æµ‹æ¡†æ¶
        self.assertGreater(len(predictions), 0)
        
        print("   âœ… é¢„æµ‹æ€§ä¼˜åŒ–æ¡†æ¶é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_06_hyperparameter_optimization_integration(self):
        """æµ‹è¯•è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–é›†æˆ"""
        print("\nğŸ›ï¸ æµ‹è¯•6: è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–é›†æˆ")
        
        # åˆ›å»ºæ€§èƒ½ç›®æ ‡å‡½æ•°
        current_context = self.test_history[-1].applied_context
        objective_func = self.hyperparameter_optimizer.create_performance_objective(
            self.effectiveness_tracker, current_context
        )
        
        # å»ºè®®å‚æ•°é…ç½®
        suggested_params = self.hyperparameter_optimizer.suggest_parameters()
        print(f"   â€¢ å»ºè®®å‚æ•°: {suggested_params}")
        
        # è¯„ä¼°å‚æ•°é…ç½®
        performance_score = self.hyperparameter_optimizer.evaluate_parameters(suggested_params)
        print(f"   â€¢ æ€§èƒ½è¯„åˆ†: {performance_score:.3f}")
        
        # è¿è¡Œå°è§„æ¨¡ä¼˜åŒ–ï¼ˆå‡å°‘è¯„ä¼°æ¬¡æ•°ä»¥åŠ å¿«æµ‹è¯•ï¼‰
        self.hyperparameter_optimizer.max_evaluations = 10
        
        optimization_result = self.hyperparameter_optimizer.optimize(max_time_minutes=1)
        
        print(f"   â€¢ ä¼˜åŒ–è¯„ä¼°æ¬¡æ•°: {optimization_result.evaluation_count}")
        print(f"   â€¢ æœ€ä¼˜æ€§èƒ½: {optimization_result.objective_value:.3f}")
        print(f"   â€¢ æ˜¯å¦æ”¶æ•›: {self.hyperparameter_optimizer.is_converged}")
        
        # è·å–å‚æ•°é‡è¦æ€§
        param_importance = self.hyperparameter_optimizer.get_parameter_importance()
        if param_importance:
            print(f"   â€¢ é‡è¦å‚æ•°: {max(param_importance.items(), key=lambda x: x[1])[0]}")
        
        # éªŒè¯è¶…å‚æ•°ä¼˜åŒ–
        self.assertGreater(optimization_result.evaluation_count, 0)
        self.assertIsInstance(optimization_result.parameters, dict)
        
        print("   âœ… è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_07_performance_benchmark_integration(self):
        """æµ‹è¯•æ™ºèƒ½æ€§èƒ½åŸºå‡†æµ‹è¯•é›†æˆ"""
        print("\nğŸ“ˆ æµ‹è¯•7: æ™ºèƒ½æ€§èƒ½åŸºå‡†æµ‹è¯•é›†æˆ")
        
        # å®šä¹‰æµ‹è¯•ç›®æ ‡å‡½æ•°
        def mock_target_function(context, strategy, parameters):
            """æ¨¡æ‹Ÿç›®æ ‡å‡½æ•°"""
            time.sleep(0.01)  # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
            
            # åŸºäºè¾“å…¥è®¡ç®—æ¨¡æ‹Ÿæ€§èƒ½
            health = context.get_overall_health()
            perf_score = health * 0.7 + parameters.get('replacement_ratio', 0.3) * 0.3
            
            # æ¨¡æ‹Ÿç­–ç•¥æ•ˆæœ
            total_rules = 10
            successful_before = 7
            failed_before = 3
            
            before_metrics = ExecutionMetrics(
                total_rules_executed=total_rules,
                successful_executions=successful_before,
                failed_executions=failed_before,
                average_execution_time=1.0,
                total_execution_time=1.0 * total_rules,
                rule_match_accuracy=0.85
            )
            
            total_rules_after = total_rules + 1
            successful_after = successful_before + 1
            failed_after = total_rules_after - successful_after
            
            after_metrics = ExecutionMetrics(
                total_rules_executed=total_rules_after,
                successful_executions=successful_after,
                failed_executions=failed_after,
                average_execution_time=max(0.5, before_metrics.average_execution_time - 0.1),
                total_execution_time=max(0.5, before_metrics.average_execution_time - 0.1) * total_rules_after,
                rule_match_accuracy=min(0.95, before_metrics.rule_match_accuracy + 0.05)
            )
            
            effectiveness = StrategyEffectiveness(
                strategy_type=strategy,
                applied_context=context,
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                improvement_score=perf_score,
                application_timestamp=datetime.now()
            )
            
            return effectiveness
        
        # è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•
        benchmark_result = self.benchmark_system.run_benchmark(
            'quick_performance',
            mock_target_function
        )
        
        print(f"   â€¢ å®Œæˆè¿­ä»£æ•°: {benchmark_result.iterations_completed}")
        print(f"   â€¢ æˆåŠŸç‡: {benchmark_result.success_count / max(1, benchmark_result.iterations_completed):.2%}")
        print(f"   â€¢ æµ‹è¯•ç”¨æ—¶: {benchmark_result.total_duration:.2f}ç§’")
        
        # éªŒè¯æŒ‡æ ‡æ”¶é›†
        metrics = benchmark_result.metrics
        print(f"   â€¢ æ”¶é›†çš„æŒ‡æ ‡ç±»å‹: {len(metrics)}")
        
        for metric_type, values in metrics.items():
            if values:
                print(f"   â€¢ {metric_type.value}: å¹³å‡å€¼={sum(values)/len(values):.4f}")
        
        # è·å–åŸºå‡†æµ‹è¯•çŠ¶æ€
        status = self.benchmark_system.get_benchmark_status()
        print(f"   â€¢ å¯ç”¨é…ç½®: {len(status['available_configurations'])}")
        print(f"   â€¢ å†å²è®°å½•: {status['history_count']}")
        
        # éªŒè¯åŸºå‡†æµ‹è¯•
        self.assertGreater(benchmark_result.iterations_completed, 0)
        self.assertGreater(len(metrics), 0)
        
        print("   âœ… æ™ºèƒ½æ€§èƒ½åŸºå‡†æµ‹è¯•é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_08_full_system_integration(self):
        """æµ‹è¯•å®Œæ•´ç³»ç»Ÿé›†æˆ"""
        print("\nğŸ”„ æµ‹è¯•8: å®Œæ•´ç³»ç»Ÿé›†æˆå·¥ä½œæµ")
        
        print("   æ­¥éª¤1: åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶...")
        
        # 1. åŠ è½½å†å²æ•°æ®åˆ°æ‰€æœ‰ç»„ä»¶
        for effectiveness in self.test_history:
            self.effectiveness_tracker.record_strategy_application(
                strategy_type=effectiveness.strategy_type,
                applied_context=effectiveness.applied_context,
                before_metrics=effectiveness.before_metrics,
                after_metrics=effectiveness.after_metrics,
                improvement_score=effectiveness.improvement_score
            )
        
        # 2. åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹
        self.predictive_framework.initialize_predictive_models(self.test_history)
        
        print("   æ­¥éª¤2: åˆ†æå½“å‰çŠ¶æ€...")
        
        # 3. åˆ†ææ¨¡å¼
        pattern_analysis = self.pattern_engine.analyze_patterns(
            self.test_history[-20:], include_predictions=True
        )
        
        # 4. è·å–å½“å‰æœ€ä½³ç­–ç•¥
        export_data = self.effectiveness_tracker.export_performance_data('summary')
        current_best_strategy = export_data.get('best_performing_strategy', 'performance_focused')
        
        print(f"   â€¢ å½“å‰æœ€ä½³ç­–ç•¥: {current_best_strategy}")
        print(f"   â€¢ è¯†åˆ«çš„æ¨¡å¼æ•°: {pattern_analysis['summary']['total_patterns']}")
        
        print("   æ­¥éª¤3: ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        # 5. å‚æ•°ä¼˜åŒ–
        current_params = {
            'replacement_ratio': 0.3,
            'similarity_threshold': 0.8,
            'performance_threshold': 0.7
        }
        
        optimized_params = self.parameter_optimizer.optimize_parameters(
            strategy_type=ReplacementStrategyType.PERFORMANCE_FOCUSED,
            current_parameters=current_params,
            performance_history=self.test_history[-10:],
            situation_context=self.test_history[-1].applied_context
        )
        
        # 6. ç”Ÿæˆé¢„æµ‹å’Œè®¡åˆ’
        predictions = self.predictive_framework.generate_system_predictions(
            self.test_history[-1].applied_context, optimized_params
        )
        
        optimization_plans = self.predictive_framework.create_optimization_plans(
            predictions, optimized_params
        ) if predictions else []
        
        print(f"   â€¢ ä¼˜åŒ–é¢„æµ‹æ•°: {len(predictions)}")
        print(f"   â€¢ ä¼˜åŒ–è®¡åˆ’æ•°: {len(optimization_plans)}")
        
        print("   æ­¥éª¤4: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–...")
        
        # 7. å¼ºåŒ–å­¦ä¹ å»ºè®®
        if self.test_history:
            rl_state = self.rl_optimizer.encode_state(
                situation=self.test_history[-1].applied_context,
                last_strategy=self.test_history[-1].strategy_type,
                last_performance=self.test_history[-1].improvement_score
            )
            
            rl_action = self.rl_optimizer.choose_action(rl_state)
            rl_recommendations = self.rl_optimizer.get_policy_recommendations(rl_state)
            
            print(f"   â€¢ RLå»ºè®®æ•°: {len(rl_recommendations)}")
        
        print("   æ­¥éª¤5: ç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        # 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        integration_report = {
            'timestamp': datetime.now().isoformat(),
            'effectiveness_tracking': {
                'total_records': len(self.test_history),
                'best_strategy': current_best_strategy,
                'avg_performance': sum(eff.improvement_score for eff in self.test_history) / len(self.test_history)
            },
            'pattern_recognition': {
                'patterns_found': pattern_analysis['summary']['total_patterns'],
                'high_confidence_patterns': pattern_analysis['summary']['high_confidence_patterns'],
                'predictions_generated': len(pattern_analysis['predictions'])
            },
            'parameter_optimization': {
                'optimization_iterations': self.parameter_optimizer.get_optimization_summary().get('total_iterations', 0),
                'parameter_improvements': len([k for k, v in optimized_params.items() if v != current_params.get(k, v)])
            },
            'predictive_framework': {
                'predictions_count': len(predictions),
                'optimization_plans': len(optimization_plans),
                'framework_health': self.predictive_framework.get_framework_status().get('framework_health', {}).get('overall_health', 0)
            },
            'reinforcement_learning': {
                'learning_experiences': self.rl_optimizer.get_learning_statistics()['total_experiences'],
                'exploration_rate': self.rl_optimizer.get_learning_statistics()['exploration_rate']
            },
            'system_status': 'fully_integrated'
        }
        
        print("   ğŸ“‹ é›†æˆæŠ¥å‘Šæ‘˜è¦:")
        print(f"   â€¢ æ•ˆæœè·Ÿè¸ªè®°å½•: {integration_report['effectiveness_tracking']['total_records']}")
        print(f"   â€¢ æ¨¡å¼è¯†åˆ«ç»“æœ: {integration_report['pattern_recognition']['patterns_found']}ä¸ªæ¨¡å¼")
        print(f"   â€¢ å‚æ•°ä¼˜åŒ–æ”¹è¿›: {integration_report['parameter_optimization']['parameter_improvements']}ä¸ªå‚æ•°")
        print(f"   â€¢ é¢„æµ‹æ¡†æ¶å¥åº·åº¦: {integration_report['predictive_framework']['framework_health']:.3f}")
        print(f"   â€¢ RLå­¦ä¹ ç»éªŒ: {integration_report['reinforcement_learning']['learning_experiences']}")
        
        # éªŒè¯å®Œæ•´é›†æˆ
        self.assertEqual(integration_report['system_status'], 'fully_integrated')
        self.assertGreater(integration_report['effectiveness_tracking']['total_records'], 0)
        self.assertGreaterEqual(integration_report['pattern_recognition']['patterns_found'], 0)
        
        print("   âœ… å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•é€šè¿‡")
        
        # ä¿å­˜é›†æˆæŠ¥å‘Š
        report_file = f"phase3_integration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(integration_report, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ“„ é›†æˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def test_09_performance_validation(self):
        """æµ‹è¯•æ€§èƒ½éªŒè¯"""
        print("\nâš¡ æµ‹è¯•9: ç³»ç»Ÿæ€§èƒ½éªŒè¯")
        
        # æµ‹è¯•å¤§è§„æ¨¡æ•°æ®å¤„ç†æ€§èƒ½
        large_history = []
        base_time = datetime.now() - timedelta(hours=48)
        
        print("   ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ®...")
        start_time = time.time()
        
        for i in range(200):  # ç”Ÿæˆ200æ¡è®°å½•
            context = SituationScore(
                rule_density=0.3 + (i % 20) * 0.02,
                execution_efficiency=0.4 + (i % 15) * 0.03,
                goal_progress=0.2 + (i % 25) * 0.025,
                failure_frequency=max(0.05, 0.3 - (i % 30) * 0.008),
                agent_utilization=0.3 + (i % 12) * 0.04,
                phase_distribution=0.4 + (i % 18) * 0.02
            )
            
            total_rules_perf = 8 + (i % 6)
            success_rate_before = 0.6 + (i % 15) * 0.02
            successful_before = int(success_rate_before * total_rules_perf)
            failed_before = total_rules_perf - successful_before
            
            before_metrics = ExecutionMetrics(
                total_rules_executed=total_rules_perf,
                successful_executions=successful_before,
                failed_executions=failed_before,
                average_execution_time=0.8 + (i % 10) * 0.05,
                total_execution_time=(0.8 + (i % 10) * 0.05) * total_rules_perf,
                rule_match_accuracy=0.85 + (i % 10) * 0.01
            )
            
            total_rules_after_perf = before_metrics.total_rules_executed + 1
            success_rate_after = min(1.0, before_metrics.success_rate + 0.05)
            successful_after_perf = int(success_rate_after * total_rules_after_perf)
            failed_after_perf = total_rules_after_perf - successful_after_perf
            
            after_metrics = ExecutionMetrics(
                total_rules_executed=total_rules_after_perf,
                successful_executions=successful_after_perf,
                failed_executions=failed_after_perf,
                average_execution_time=max(0.3, before_metrics.average_execution_time - 0.1),
                total_execution_time=max(0.3, before_metrics.average_execution_time - 0.1) * total_rules_after_perf,
                rule_match_accuracy=min(0.95, before_metrics.rule_match_accuracy + 0.05)
            )
            
            effectiveness = StrategyEffectiveness(
                strategy_type=list(ReplacementStrategyType)[i % len(ReplacementStrategyType)],
                applied_context=context,
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                improvement_score=0.3 + (i % 40) * 0.015,
                application_timestamp=base_time + timedelta(minutes=i * 15)
            )
            
            large_history.append(effectiveness)
        
        data_generation_time = time.time() - start_time
        print(f"   â€¢ æ•°æ®ç”Ÿæˆç”¨æ—¶: {data_generation_time:.2f}ç§’")
        
        # æµ‹è¯•å„ç»„ä»¶çš„å¤§æ•°æ®å¤„ç†æ€§èƒ½
        performance_results = {}
        
        # 1. æ•ˆæœè·Ÿè¸ªå™¨æ€§èƒ½
        start_time = time.time()
        for eff in large_history:
            self.effectiveness_tracker.record_strategy_application(
                strategy_type=eff.strategy_type,
                applied_context=eff.applied_context,
                before_metrics=eff.before_metrics,
                after_metrics=eff.after_metrics,
                improvement_score=eff.improvement_score
            )
        tracking_time = time.time() - start_time
        performance_results['effectiveness_tracking'] = tracking_time
        print(f"   â€¢ æ•ˆæœè·Ÿè¸ªå¤„ç†: {tracking_time:.2f}ç§’ ({len(large_history)/tracking_time:.1f} è®°å½•/ç§’)")
        
        # 2. æ¨¡å¼è¯†åˆ«æ€§èƒ½
        start_time = time.time()
        pattern_result = self.pattern_engine.analyze_patterns(large_history[:100])  # é™åˆ¶æ•°é‡é¿å…è¿‡é•¿
        pattern_time = time.time() - start_time
        performance_results['pattern_recognition'] = pattern_time
        print(f"   â€¢ æ¨¡å¼è¯†åˆ«å¤„ç†: {pattern_time:.2f}ç§’")
        
        # 3. å‚æ•°ä¼˜åŒ–æ€§èƒ½
        start_time = time.time()
        optimized = self.parameter_optimizer.optimize_parameters(
            ReplacementStrategyType.PERFORMANCE_FOCUSED,
            {'replacement_ratio': 0.3, 'similarity_threshold': 0.8},
            large_history[-20:],
            large_history[-1].applied_context
        )
        optimization_time = time.time() - start_time
        performance_results['parameter_optimization'] = optimization_time
        print(f"   â€¢ å‚æ•°ä¼˜åŒ–å¤„ç†: {optimization_time:.2f}ç§’")
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        total_processing_time = sum(performance_results.values())
        print(f"   â€¢ æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}ç§’")
        print(f"   â€¢ å¹³å‡æ¯æ¡è®°å½•: {total_processing_time/len(large_history)*1000:.2f}æ¯«ç§’")
        
        # æ€§èƒ½é˜ˆå€¼éªŒè¯ï¼ˆç¡®ä¿ç³»ç»Ÿèƒ½åœ¨åˆç†æ—¶é—´å†…å¤„ç†å¤§é‡æ•°æ®ï¼‰
        self.assertLess(tracking_time, 5.0, "æ•ˆæœè·Ÿè¸ªå¤„ç†æ—¶é—´è¿‡é•¿")
        self.assertLess(pattern_time, 10.0, "æ¨¡å¼è¯†åˆ«å¤„ç†æ—¶é—´è¿‡é•¿")
        self.assertLess(optimization_time, 3.0, "å‚æ•°ä¼˜åŒ–å¤„ç†æ—¶é—´è¿‡é•¿")
        
        print("   âœ… ç³»ç»Ÿæ€§èƒ½éªŒè¯é€šè¿‡")
    
    def test_10_system_robustness(self):
        """æµ‹è¯•ç³»ç»Ÿå¥å£®æ€§"""
        print("\nğŸ›¡ï¸ æµ‹è¯•10: ç³»ç»Ÿå¥å£®æ€§éªŒè¯")
        
        # æµ‹è¯•å¼‚å¸¸è¾“å…¥å¤„ç†
        print("   æµ‹è¯•å¼‚å¸¸è¾“å…¥å¤„ç†...")
        
        # 1. ç©ºæ•°æ®å¤„ç†
        try:
            empty_result = self.pattern_engine.analyze_patterns([])
            self.assertIsInstance(empty_result, dict)
            print("   âœ“ ç©ºæ•°æ®å¤„ç†æ­£å¸¸")
        except Exception as e:
            self.fail(f"ç©ºæ•°æ®å¤„ç†å¤±è´¥: {e}")
        
        # 2. å¼‚å¸¸ä¸Šä¸‹æ–‡å¤„ç†
        try:
            extreme_context = SituationScore(
                rule_density=1.5,  # è¶…å‡ºæ­£å¸¸èŒƒå›´
                execution_efficiency=-0.5,  # è´Ÿå€¼
                goal_progress=2.0,  # è¶…å‡ºèŒƒå›´
                failure_frequency=-1.0,  # è´Ÿå€¼
                agent_utilization=10.0,  # è¶…å‡ºèŒƒå›´
                phase_distribution=0.5
            )
            
            # åº”è¯¥èƒ½å¤„ç†æå€¼è€Œä¸å´©æºƒ
            health = extreme_context.get_overall_health()
            self.assertIsInstance(health, float)
            print("   âœ“ æå€¼ä¸Šä¸‹æ–‡å¤„ç†æ­£å¸¸")
            
        except Exception as e:
            self.fail(f"æå€¼ä¸Šä¸‹æ–‡å¤„ç†å¤±è´¥: {e}")
        
        # 3. æµ‹è¯•ç»„ä»¶é—´é”™è¯¯ä¼ æ’­
        try:
            # æ•…æ„ä¼ å…¥ä¸åŒ¹é…çš„å‚æ•°
            invalid_params = {'invalid_param': 'invalid_value'}
            
            # åº”è¯¥ä¼˜é›…åœ°å¤„ç†è€Œä¸æ˜¯å´©æºƒ
            result = self.parameter_optimizer.optimize_parameters(
                ReplacementStrategyType.PERFORMANCE_FOCUSED,
                invalid_params,
                self.test_history[-5:],
                self.test_history[-1].applied_context
            )
            
            self.assertIsInstance(result, dict)
            print("   âœ“ æ— æ•ˆå‚æ•°å¤„ç†æ­£å¸¸")
            
        except Exception as e:
            print(f"   âœ“ æ— æ•ˆå‚æ•°è¢«æ­£ç¡®æ‹’ç»: {type(e).__name__}")
        
        # 4. å†…å­˜ä½¿ç”¨ç›‘æ§
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤§é‡æ“ä½œ
        for i in range(50):
            temp_history = self.test_history[-10:]
            eff = temp_history[i % len(temp_history)]
            self.effectiveness_tracker.record_strategy_application(
                strategy_type=eff.strategy_type,
                applied_context=eff.applied_context,
                before_metrics=eff.before_metrics,
                after_metrics=eff.after_metrics,
                improvement_score=eff.improvement_score
            )
            
            if i % 10 == 0:
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_growth = final_memory - initial_memory
        
        print(f"   â€¢ åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        print(f"   â€¢ æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
        print(f"   â€¢ å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
        
        # éªŒè¯å†…å­˜å¢é•¿åœ¨åˆç†èŒƒå›´å†…
        self.assertLess(memory_growth, 100.0, "å†…å­˜å¢é•¿è¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
        
        print("   âœ… ç³»ç»Ÿå¥å£®æ€§éªŒè¯é€šè¿‡")


def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("\n" + "ğŸ¯" + " "*30 + "PHASE 3 é›†æˆæµ‹è¯•å¯åŠ¨" + " "*30 + "ğŸ¯")
    print("="*100)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = unittest.TestLoader().loadTestsFromTestCase(TestPhase3Integration)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(suite)
    
    print("\n" + "="*100)
    print("ğŸ Phase 3 é›†æˆæµ‹è¯•å®Œæˆ")
    print(f"âœ… é€šè¿‡: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"âŒ å¤±è´¥: {len(result.failures)}")
    print(f"ğŸ’¥ é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print("\nå¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"  - {test}: {traceback}")
    
    if result.errors:
        print("\né”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback}")
    
    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun
    print(f"\nğŸ¯ æ•´ä½“æˆåŠŸç‡: {success_rate:.1%}")
    
    if success_rate >= 0.8:
        print("ğŸ‰ Phase 3: Self-Learning Optimization é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("ğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼")
    else:
        print("âš ï¸  é›†æˆæµ‹è¯•å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_integration_tests()
    exit(0 if success else 1)