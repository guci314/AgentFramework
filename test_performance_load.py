"""
æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯•
æµ‹è¯•çŠ¶æ€ç®¡ç†å’ŒAIæ›´æ–°ç³»ç»Ÿçš„æ€§èƒ½ç‰¹å¾ï¼ŒåŒ…æ‹¬å»¶è¿Ÿæµ‹é‡ã€å¼€é”€åˆ†æå’Œè´Ÿè½½æµ‹è¯•
"""

import time
import threading
import statistics
import psutil
import sys
import os
import gc
import concurrent.futures
from typing import List, Dict, Any
import logging

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING)  # å‡å°‘æ—¥å¿—è¾“å‡ºä»¥é¿å…å½±å“æ€§èƒ½æµ‹è¯•

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.measurements = {}
        self.process = psutil.Process()
        
    def start_measurement(self, test_name: str):
        """å¼€å§‹æµ‹é‡"""
        if test_name not in self.measurements:
            self.measurements[test_name] = {
                'times': [],
                'memory_usage': [],
                'cpu_usage': []
            }
        
        return {
            'start_time': time.perf_counter(),
            'start_memory': self.process.memory_info().rss / 1024 / 1024,  # MB
            'start_cpu': self.process.cpu_percent()
        }
    
    def end_measurement(self, test_name: str, start_data: Dict[str, Any]):
        """ç»“æŸæµ‹é‡"""
        end_time = time.perf_counter()
        end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        end_cpu = self.process.cpu_percent()
        
        duration = end_time - start_data['start_time']
        memory_delta = end_memory - start_data['start_memory']
        
        self.measurements[test_name]['times'].append(duration)
        self.measurements[test_name]['memory_usage'].append(memory_delta)
        self.measurements[test_name]['cpu_usage'].append(end_cpu)
        
        return {
            'duration': duration,
            'memory_delta': memory_delta,
            'cpu_usage': end_cpu
        }
    
    def get_statistics(self, test_name: str) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if test_name not in self.measurements:
            return {}
        
        times = self.measurements[test_name]['times']
        memory = self.measurements[test_name]['memory_usage']
        cpu = self.measurements[test_name]['cpu_usage']
        
        if not times:
            return {}
        
        return {
            'count': len(times),
            'time': {
                'mean': statistics.mean(times),
                'median': statistics.median(times),
                'min': min(times),
                'max': max(times),
                'std': statistics.stdev(times) if len(times) > 1 else 0
            },
            'memory': {
                'mean': statistics.mean(memory),
                'median': statistics.median(memory),
                'min': min(memory),
                'max': max(memory),
                'std': statistics.stdev(memory) if len(memory) > 1 else 0
            },
            'cpu': {
                'mean': statistics.mean(cpu),
                'median': statistics.median(cpu),
                'max': max(cpu)
            }
        }

def test_state_operation_latency():
    """æµ‹è¯•çŠ¶æ€æ“ä½œå»¶è¿Ÿ"""
    print("\n=== çŠ¶æ€æ“ä½œå»¶è¿Ÿæµ‹è¯• ===")
    
    from enhancedAgent_v2 import WorkflowState
    
    profiler = PerformanceProfiler()
    
    # åˆ›å»ºå·¥ä½œæµçŠ¶æ€å®ä¾‹
    workflow_state = WorkflowState()
    
    # æµ‹è¯•çŠ¶æ€è®¾ç½®æ“ä½œ
    print("\n1. çŠ¶æ€è®¾ç½®æ“ä½œå»¶è¿Ÿæµ‹è¯•...")
    for i in range(100):
        start = profiler.start_measurement('state_set')
        workflow_state.set_global_state(f"æµ‹è¯•çŠ¶æ€ {i}", f"æµ‹è¯•æº {i}")
        profiler.end_measurement('state_set', start)
    
    # æµ‹è¯•çŠ¶æ€è·å–æ“ä½œ
    print("2. çŠ¶æ€è·å–æ“ä½œå»¶è¿Ÿæµ‹è¯•...")
    for i in range(100):
        start = profiler.start_measurement('state_get')
        current_state = workflow_state.get_global_state()
        profiler.end_measurement('state_get', start)
    
    # æµ‹è¯•çŠ¶æ€å†å²è·å–æ“ä½œ
    print("3. çŠ¶æ€å†å²è·å–æ“ä½œå»¶è¿Ÿæµ‹è¯•...")
    for i in range(100):
        start = profiler.start_measurement('history_get')
        history = workflow_state.get_state_history()
        profiler.end_measurement('history_get', start)
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    for operation in ['state_set', 'state_get', 'history_get']:
        stats = profiler.get_statistics(operation)
        if stats:
            print(f"\n{operation} æ“ä½œç»Ÿè®¡:")
            print(f"  æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
            print(f"  å¹³å‡æ—¶é—´: {stats['time']['mean']*1000:.3f} ms")
            print(f"  ä¸­ä½æ•°æ—¶é—´: {stats['time']['median']*1000:.3f} ms")
            print(f"  æœ€å°æ—¶é—´: {stats['time']['min']*1000:.3f} ms")
            print(f"  æœ€å¤§æ—¶é—´: {stats['time']['max']*1000:.3f} ms")
            print(f"  æ ‡å‡†å·®: {stats['time']['std']*1000:.3f} ms")
            print(f"  å¹³å‡å†…å­˜å˜åŒ–: {stats['memory']['mean']:.3f} MB")
    
    return profiler.measurements

def test_ai_updater_latency():
    """æµ‹è¯•AIæ›´æ–°å™¨å»¶è¿Ÿ"""
    print("\n=== AIæ›´æ–°å™¨å»¶è¿Ÿæµ‹è¯• ===")
    
    from enhancedAgent_v2 import WorkflowState, AIStateUpdaterService
    from pythonTask import llm_deepseek
    
    profiler = PerformanceProfiler()
    
    # åˆ›å»ºå®ä¾‹
    workflow_state = WorkflowState()
    ai_updater = AIStateUpdaterService(llm_deepseek, enable_caching=True)
    
    # æµ‹è¯•AIçŠ¶æ€æ›´æ–°ï¼ˆæœ‰ç¼“å­˜ï¼‰
    print("\n1. AIçŠ¶æ€æ›´æ–°å»¶è¿Ÿæµ‹è¯•ï¼ˆç¼“å­˜å¯ç”¨ï¼‰...")
    test_contexts = [
        {
            'step_info': {'instruction': f'æµ‹è¯•æŒ‡ä»¤ {i}', 'step_type': 'test'},
            'execution_result': {'success': True, 'output': f'æµ‹è¯•è¾“å‡º {i}'},
            'step_status': 'completed'
        }
        for i in range(10)  # å‡å°‘æµ‹è¯•æ¬¡æ•°ä»¥é¿å…è¿‡å¤šAPIè°ƒç”¨
    ]
    
    for i, context in enumerate(test_contexts):
        start = profiler.start_measurement('ai_update_cached')
        try:
            new_state = ai_updater.update_state(workflow_state, context)
            if new_state:
                workflow_state.set_global_state(new_state, f"AIæ›´æ–°å™¨æµ‹è¯• {i}")
        except Exception as e:
            print(f"AIæ›´æ–°å¤±è´¥ {i}: {e}")
        profiler.end_measurement('ai_update_cached', start)
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(0.1)
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    stats = profiler.get_statistics('ai_update_cached')
    if stats:
        print(f"\nAIæ›´æ–°æ“ä½œç»Ÿè®¡:")
        print(f"  æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
        print(f"  å¹³å‡æ—¶é—´: {stats['time']['mean']:.3f} s")
        print(f"  ä¸­ä½æ•°æ—¶é—´: {stats['time']['median']:.3f} s")
        print(f"  æœ€å°æ—¶é—´: {stats['time']['min']:.3f} s")
        print(f"  æœ€å¤§æ—¶é—´: {stats['time']['max']:.3f} s")
        print(f"  æ ‡å‡†å·®: {stats['time']['std']:.3f} s")
        print(f"  å¹³å‡å†…å­˜å˜åŒ–: {stats['memory']['mean']:.3f} MB")
    
    return profiler.measurements

def test_memory_overhead():
    """æµ‹è¯•å†…å­˜å¼€é”€"""
    print("\n=== å†…å­˜å¼€é”€åˆ†ææµ‹è¯• ===")
    
    from enhancedAgent_v2 import WorkflowState
    
    profiler = PerformanceProfiler()
    
    # æµ‹è¯•åŸºçº¿å†…å­˜ä½¿ç”¨
    gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    baseline_memory = profiler.process.memory_info().rss / 1024 / 1024
    print(f"åŸºçº¿å†…å­˜ä½¿ç”¨: {baseline_memory:.2f} MB")
    
    # åˆ›å»ºå·¥ä½œæµçŠ¶æ€
    workflow_state = WorkflowState()
    initial_memory = profiler.process.memory_info().rss / 1024 / 1024
    print(f"åˆ›å»ºWorkflowStateåå†…å­˜: {initial_memory:.2f} MB (+{initial_memory - baseline_memory:.2f} MB)")
    
    # æµ‹è¯•çŠ¶æ€å†å²å¢é•¿çš„å†…å­˜å½±å“
    memory_points = []
    state_counts = [10, 50, 100, 200, 500]
    
    for count in state_counts:
        # æ¸…ç©ºç°æœ‰çŠ¶æ€
        workflow_state.clear_state_history()
        workflow_state.clear_global_state()
        gc.collect()
        
        # æ·»åŠ æŒ‡å®šæ•°é‡çš„çŠ¶æ€
        for i in range(count):
            state_content = f"æµ‹è¯•çŠ¶æ€ {i} - è¿™æ˜¯ä¸€ä¸ªè¾ƒé•¿çš„çŠ¶æ€æè¿°ï¼Œç”¨äºæµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µã€‚åŒ…å«ä¸€äº›é¢å¤–çš„æ–‡æœ¬æ¥æ¨¡æ‹ŸçœŸå®çš„çŠ¶æ€å†…å®¹ã€‚"
            workflow_state.set_global_state(state_content, f"å†…å­˜æµ‹è¯• {i}")
        
        gc.collect()
        current_memory = profiler.process.memory_info().rss / 1024 / 1024
        memory_delta = current_memory - initial_memory
        memory_points.append((count, memory_delta))
        
        print(f"çŠ¶æ€æ•°é‡: {count:3d}, å†…å­˜å¢é•¿: {memory_delta:6.2f} MB, å¹³å‡æ¯çŠ¶æ€: {memory_delta/count*1024:.2f} KB")
    
    # è®¡ç®—å†…å­˜å¢é•¿ç‡
    if len(memory_points) > 1:
        # è®¡ç®—çº¿æ€§å¢é•¿ç‡ (MB per state)
        x_values = [point[0] for point in memory_points]
        y_values = [point[1] for point in memory_points]
        
        # ç®€å•çº¿æ€§å›å½’
        n = len(x_values)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x2 = sum(x * x for x in x_values)
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        print(f"\nå†…å­˜å¢é•¿ç‡: {slope*1024:.3f} KB/çŠ¶æ€")
    
    return memory_points

def test_concurrent_load():
    """æµ‹è¯•å¹¶å‘è´Ÿè½½"""
    print("\n=== å¹¶å‘è´Ÿè½½æµ‹è¯• ===")
    
    from enhancedAgent_v2 import WorkflowState
    
    profiler = PerformanceProfiler()
    
    # åˆ›å»ºå…±äº«çš„å·¥ä½œæµçŠ¶æ€
    workflow_state = WorkflowState()
    
    def worker_thread(thread_id: int, operations_per_thread: int):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        thread_times = []
        
        for i in range(operations_per_thread):
            start_time = time.perf_counter()
            
            # æ‰§è¡Œæ··åˆæ“ä½œ
            if i % 3 == 0:
                # è®¾ç½®çŠ¶æ€
                workflow_state.set_global_state(f"çº¿ç¨‹{thread_id}çŠ¶æ€{i}", f"çº¿ç¨‹{thread_id}")
            elif i % 3 == 1:
                # è·å–çŠ¶æ€
                current_state = workflow_state.get_global_state()
            else:
                # è·å–å†å²
                history = workflow_state.get_state_history(limit=10)
            
            end_time = time.perf_counter()
            thread_times.append(end_time - start_time)
        
        return thread_times
    
    # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
    concurrency_levels = [1, 2, 4, 8]
    operations_per_thread = 50
    
    for num_threads in concurrency_levels:
        print(f"\næµ‹è¯•å¹¶å‘çº§åˆ«: {num_threads} çº¿ç¨‹")
        
        start_memory = profiler.process.memory_info().rss / 1024 / 1024
        start_time = time.perf_counter()
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘æ“ä½œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [
                executor.submit(worker_thread, i, operations_per_thread)
                for i in range(num_threads)
            ]
            
            # æ”¶é›†ç»“æœ
            all_times = []
            for future in concurrent.futures.as_completed(futures):
                thread_times = future.result()
                all_times.extend(thread_times)
        
        end_time = time.perf_counter()
        end_memory = profiler.process.memory_info().rss / 1024 / 1024
        
        total_operations = num_threads * operations_per_thread
        total_time = end_time - start_time
        memory_delta = end_memory - start_memory
        
        print(f"  æ€»æ“ä½œæ•°: {total_operations}")
        print(f"  æ€»è€—æ—¶: {total_time:.3f} s")
        print(f"  ååé‡: {total_operations/total_time:.1f} ops/s")
        print(f"  å¹³å‡å»¶è¿Ÿ: {statistics.mean(all_times)*1000:.3f} ms")
        print(f"  å†…å­˜å¢é•¿: {memory_delta:.2f} MB")
        
        if all_times:
            print(f"  å»¶è¿Ÿåˆ†å¸ƒ:")
            print(f"    P50: {statistics.median(all_times)*1000:.3f} ms")
            print(f"    P95: {sorted(all_times)[int(len(all_times)*0.95)]*1000:.3f} ms")
            print(f"    P99: {sorted(all_times)[int(len(all_times)*0.99)]*1000:.3f} ms")

def test_state_history_scalability():
    """æµ‹è¯•çŠ¶æ€å†å²å¯æ‰©å±•æ€§"""
    print("\n=== çŠ¶æ€å†å²å¯æ‰©å±•æ€§æµ‹è¯• ===")
    
    from enhancedAgent_v2 import WorkflowState
    
    profiler = PerformanceProfiler()
    
    # æµ‹è¯•ä¸åŒå†å²å¤§å°é™åˆ¶ä¸‹çš„æ€§èƒ½
    history_limits = [10, 50, 100, 500, 1000]
    
    for limit in history_limits:
        print(f"\næµ‹è¯•å†å²é™åˆ¶: {limit}")
        
        # åˆ›å»ºæ–°çš„å·¥ä½œæµçŠ¶æ€å®ä¾‹
        workflow_state = WorkflowState()
        workflow_state.set_max_history_size(limit)
        
        # å¡«å……åˆ°é™åˆ¶å¤§å°
        for i in range(limit):
            workflow_state.set_global_state(f"å†å²çŠ¶æ€ {i}", f"å¯æ‰©å±•æ€§æµ‹è¯•")
        
        # æµ‹è¯•åœ¨æ»¡å†å²æƒ…å†µä¸‹çš„æ“ä½œæ€§èƒ½
        times = []
        for i in range(50):  # 50æ¬¡é¢å¤–æ“ä½œ
            start = profiler.start_measurement(f'history_limit_{limit}')
            workflow_state.set_global_state(f"æµ‹è¯•çŠ¶æ€ {i}", "æ€§èƒ½æµ‹è¯•")
            result = profiler.end_measurement(f'history_limit_{limit}', start)
            times.append(result['duration'])
        
        # è¾“å‡ºç»Ÿè®¡
        if times:
            print(f"  å¹³å‡è®¾ç½®æ—¶é—´: {statistics.mean(times)*1000:.3f} ms")
            print(f"  æœ€å¤§è®¾ç½®æ—¶é—´: {max(times)*1000:.3f} ms")
            
            # æµ‹è¯•å†å²è·å–æ€§èƒ½
            history_times = []
            for i in range(20):
                start_time = time.perf_counter()
                history = workflow_state.get_state_history()
                end_time = time.perf_counter()
                history_times.append(end_time - start_time)
            
            print(f"  å¹³å‡å†å²è·å–æ—¶é—´: {statistics.mean(history_times)*1000:.3f} ms")
            print(f"  å†å²è®°å½•æ•°: {len(workflow_state.get_state_history())}")

def generate_performance_report():
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    try:
        # 1. çŠ¶æ€æ“ä½œå»¶è¿Ÿæµ‹è¯•
        latency_results = test_state_operation_latency()
        
        # 2. AIæ›´æ–°å™¨å»¶è¿Ÿæµ‹è¯•
        ai_latency_results = test_ai_updater_latency()
        
        # 3. å†…å­˜å¼€é”€åˆ†æ
        memory_results = test_memory_overhead()
        
        # 4. å¹¶å‘è´Ÿè½½æµ‹è¯•
        test_concurrent_load()
        
        # 5. çŠ¶æ€å†å²å¯æ‰©å±•æ€§æµ‹è¯•
        test_state_history_scalability()
        
        print("\n" + "="*60)
        print("æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        print("\nâœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print("\nä¸»è¦å‘ç°:")
        print("1. çŠ¶æ€æ“ä½œå»¶è¿Ÿåœ¨æ¯«ç§’çº§åˆ«ï¼Œæ€§èƒ½è‰¯å¥½")
        print("2. AIæ›´æ–°å™¨å“åº”æ—¶é—´å–å†³äºç½‘ç»œå’Œæ¨¡å‹å»¶è¿Ÿ")
        print("3. å†…å­˜ä½¿ç”¨éšçŠ¶æ€å†å²çº¿æ€§å¢é•¿ï¼Œç¬¦åˆé¢„æœŸ")
        print("4. ç³»ç»Ÿæ”¯æŒå¹¶å‘æ“ä½œï¼Œçº¿ç¨‹å®‰å…¨æ€§è‰¯å¥½")
        print("5. çŠ¶æ€å†å²å¤§å°å¯¹æ“ä½œæ€§èƒ½å½±å“è¾ƒå°")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = generate_performance_report()
    
    if success:
        print("\nğŸ‰ æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        exit(0)
    else:
        print("\nğŸ’¥ æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯•å¤±è´¥ï¼")
        exit(1) 