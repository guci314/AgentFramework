#!/usr/bin/env python3
"""
æ™ºèƒ½æµ‹è¯•ç»“æœè¯„ä¼°å™¨
ä½¿ç”¨deepseekæ¨¡å‹åˆ¤æ–­æµ‹è¯•æˆ–éªŒè¯ç»“æœæ˜¯å¦é€šè¿‡
"""

import json
import logging
import os
from typing import Dict, Any, Optional
from openai import OpenAI

logger = logging.getLogger(__name__)


class TestResultEvaluator:
    """æ™ºèƒ½æµ‹è¯•ç»“æœè¯„ä¼°å™¨"""
    
    def __init__(self, api_key: str = None, base_url: str = "https://api.deepseek.com"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥ï¼Œå¦‚ä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡DEEPSEEK_API_KEYè·å–
            base_url: DeepSeek APIåŸºç¡€URL
        """
        if api_key is None:
            api_key = os.getenv('DEEPSEEK_API_KEY')
        
        if not api_key:
            raise ValueError("æœªæä¾›DeepSeek APIå¯†é’¥ï¼Œè¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ é€’api_keyå‚æ•°")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = "deepseek-chat"
    
    def evaluate_test_result(self, 
                           result_code: str = None,
                           result_stdout: str = None, 
                           result_stderr: str = None,
                           result_return_value: str = None,
                           context: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨deepseekæ¨¡å‹åˆ¤æ–­æµ‹è¯•ç»“æœæ˜¯å¦é€šè¿‡
        
        Args:
            result_code: æ‰§è¡Œçš„ä»£ç 
            result_stdout: æ ‡å‡†è¾“å‡º
            result_stderr: é”™è¯¯è¾“å‡º
            result_return_value: è¿”å›å€¼
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            DictåŒ…å«åˆ¤æ–­ç»“æœ: {
                "passed": bool,           # æµ‹è¯•æ˜¯å¦é€šè¿‡
                "confidence": float,      # ç½®ä¿¡åº¦ (0-1)
                "reason": str,           # åˆ¤æ–­ç†ç”±
                "test_type": str,        # è¯†åˆ«çš„æµ‹è¯•ç±»å‹
                "details": dict          # è¯¦ç»†åˆ†æ
            }
        """
        
        # æ„å»ºåˆ†ææç¤º
        analysis_prompt = self._build_analysis_prompt(
            result_code, result_stdout, result_stderr, result_return_value, context
        )
        
        try:
            # è°ƒç”¨deepseekè¿›è¡Œåˆ†æ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•ç»“æœåˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­å„ç§ç±»å‹çš„æµ‹è¯•ã€éªŒè¯ã€æ„å»ºç»“æœæ˜¯å¦æˆåŠŸã€‚"
                    },
                    {
                        "role": "user", 
                        "content": analysis_prompt
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=1000
            )
            
            # è§£æJSONå“åº”
            result_json = json.loads(response.choices[0].message.content)
            
            # éªŒè¯å“åº”æ ¼å¼å¹¶è®¾ç½®é»˜è®¤å€¼
            return self._validate_and_normalize_response(result_json)
            
        except Exception as e:
            logger.error(f"DeepSeekè¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›ä¿å®ˆçš„fallbackç»“æœ
            return {
                "passed": False,
                "confidence": 0.1,
                "reason": f"AIè¯„ä¼°å¤±è´¥ï¼Œfallbackåˆ¤æ–­: {str(e)}",
                "test_type": "unknown",
                "details": {
                    "error": str(e),
                    "fallback": True
                }
            }
    
    def _build_analysis_prompt(self, 
                              code: str = None,
                              stdout: str = None, 
                              stderr: str = None,
                              return_value: str = None,
                              context: str = None) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        
        prompt = """# æµ‹è¯•ç»“æœåˆ†æä»»åŠ¡

è¯·åˆ†æä»¥ä¸‹æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æµ‹è¯•ã€éªŒè¯æˆ–æ„å»ºæ˜¯å¦æˆåŠŸé€šè¿‡ã€‚

**é‡è¦æç¤ºï¼šä¸åŒæµ‹è¯•æ¡†æ¶çš„è¾“å‡ºç‰¹æ€§**
- unittestã€pytest: æµ‹è¯•ç»“æœé€šå¸¸è¾“å‡ºåˆ° stderrï¼ˆæ ‡å‡†é”™è¯¯æµï¼‰
- noseã€doctest: å¯èƒ½è¾“å‡ºåˆ° stdout æˆ– stderr  
- æ„å»ºå·¥å…·: makeã€gradle ç­‰é€šå¸¸è¾“å‡ºåˆ° stdout
- é”™è¯¯ä¿¡æ¯ã€è­¦å‘Š: ä¸€èˆ¬åœ¨ stderrï¼Œä½†ä¸ä»£è¡¨æµ‹è¯•å¤±è´¥

## æ‰§è¡Œä¿¡æ¯
"""
        
        if code:
            prompt += f"""
### æ‰§è¡Œçš„ä»£ç 
```
{code[:1000]}{"..." if len(code) > 1000 else ""}
```
"""
        
        if stdout:
            prompt += f"""
### æ ‡å‡†è¾“å‡º (stdout)
```
{stdout[:2000]}{"..." if len(stdout) > 2000 else ""}
```
"""
        
        if stderr:
            prompt += f"""
### é”™è¯¯è¾“å‡º (stderr)  
```
{stderr[:1000]}{"..." if len(stderr) > 1000 else ""}
```
"""
        
        if return_value:
            prompt += f"""
### è¿”å›å€¼ (return_value)
```
{str(return_value)[:1000]}{"..." if len(str(return_value)) > 1000 else ""}
```
"""
        
        if context:
            prompt += f"""
### ä¸Šä¸‹æ–‡ä¿¡æ¯
{context[:500]}{"..." if len(context) > 500 else ""}
"""
        
        prompt += """

## åˆ†æè¦æ±‚

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œåˆ¤æ–­æ­¤æ¬¡æ‰§è¡Œçš„æµ‹è¯•/éªŒè¯/æ„å»ºæ˜¯å¦æˆåŠŸé€šè¿‡ã€‚

### åˆ¤æ–­æ ‡å‡†
1. **å•å…ƒæµ‹è¯•**: 
   - unittest/pytest: æµ‹è¯•ç»“æœé€šå¸¸åœ¨stderrä¸­ï¼ŒæŸ¥æ‰¾"OK"ã€"FAILED"ã€"passed"ã€"failed"ç­‰å…³é”®è¯
   - unittestæˆåŠŸ: "Ran X tests...OK"
   - unittestå¤±è´¥: "FAILED (failures=X)" æˆ– "FAILED (errors=X)"
   - pytestæˆåŠŸ: "X passed"
   - pytestå¤±è´¥: "X failed"
2. **é›†æˆæµ‹è¯•**: æ£€æŸ¥APIå“åº”ã€æ•°æ®åº“è¿æ¥ã€æœåŠ¡çŠ¶æ€ç­‰
3. **ä»£ç æ£€æŸ¥**: è¯­æ³•æ£€æŸ¥ã€lintingã€ç±»å‹æ£€æŸ¥ç­‰
4. **æ„å»ºä»»åŠ¡**: ç¼–è¯‘æˆåŠŸã€æ‰“åŒ…å®Œæˆã€ä¾èµ–å®‰è£…ç­‰
5. **éªŒè¯ä»»åŠ¡**: æ•°æ®éªŒè¯ã€æ ¼å¼æ£€æŸ¥ã€ä¸šåŠ¡é€»è¾‘éªŒè¯ç­‰

**é‡è¦æé†’**: stderrä¸­çš„å†…å®¹ä¸ä¸€å®šæ˜¯é”™è¯¯ï¼unittestç­‰æµ‹è¯•æ¡†æ¶å°†æ­£å¸¸çš„æµ‹è¯•ç»“æœè¾“å‡ºåˆ°stderrã€‚

### è¾“å‡ºæ ¼å¼
å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
    "passed": true,                    // å¸ƒå°”å€¼ï¼šæµ‹è¯•æ˜¯å¦é€šè¿‡
    "confidence": 0.95,                // æµ®ç‚¹æ•°ï¼šç½®ä¿¡åº¦(0-1)
    "reason": "æ‰€æœ‰12ä¸ªæµ‹è¯•ç”¨ä¾‹é€šè¿‡",    // å­—ç¬¦ä¸²ï¼šåˆ¤æ–­ç†ç”±
    "test_type": "unit_test",          // å­—ç¬¦ä¸²ï¼šè¯†åˆ«çš„æµ‹è¯•ç±»å‹
    "details": {                       // å¯¹è±¡ï¼šè¯¦ç»†åˆ†æ
        "total_tests": 12,
        "passed_tests": 12,
        "failed_tests": 0,
        "error_indicators": [],
        "success_indicators": ["12 passed"]
    }
}
```

### test_typeå¯é€‰å€¼
- "unit_test": å•å…ƒæµ‹è¯•
- "integration_test": é›†æˆæµ‹è¯•  
- "build": æ„å»ºä»»åŠ¡
- "lint": ä»£ç æ£€æŸ¥
- "validation": æ•°æ®/æ ¼å¼éªŒè¯
- "execution": ä¸€èˆ¬ä»£ç æ‰§è¡Œ
- "unknown": æ— æ³•ç¡®å®šç±»å‹

### ç‰¹æ®Šæ³¨æ„äº‹é¡¹
1. å³ä½¿stderræœ‰å†…å®¹ï¼Œä¹Ÿå¯èƒ½æ˜¯è­¦å‘Šè€Œéé”™è¯¯
2. æŸäº›æµ‹è¯•æ¡†æ¶æˆåŠŸæ—¶è¿”å›éé›¶é€€å‡ºç 
3. æ³¨æ„åŒºåˆ†æµ‹è¯•æ¡†æ¶çš„è¾“å‡ºæ ¼å¼
4. è€ƒè™‘ä¸åŒç¼–ç¨‹è¯­è¨€çš„æµ‹è¯•å·¥å…·ç‰¹ç‚¹

è¯·ä»”ç»†åˆ†æï¼Œç»™å‡ºå‡†ç¡®åˆ¤æ–­ã€‚
"""
        
        return prompt
    
    def _validate_and_normalize_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å¹¶è§„èŒƒåŒ–å“åº”æ ¼å¼"""
        
        # è®¾ç½®é»˜è®¤å€¼
        normalized = {
            "passed": response.get("passed", False),
            "confidence": min(max(response.get("confidence", 0.5), 0.0), 1.0),
            "reason": response.get("reason", "AIåˆ†æç»“æœ"),
            "test_type": response.get("test_type", "unknown"),
            "details": response.get("details", {})
        }
        
        # ç¡®ä¿passedæ˜¯å¸ƒå°”å€¼
        if isinstance(normalized["passed"], str):
            normalized["passed"] = normalized["passed"].lower() in ["true", "yes", "pass", "passed", "success"]
        
        # ç¡®ä¿confidenceæ˜¯æ•°å­—
        try:
            normalized["confidence"] = float(normalized["confidence"])
        except (ValueError, TypeError):
            normalized["confidence"] = 0.5
        
        return normalized
    
    def quick_evaluate(self, result_return_value: str) -> bool:
        """
        å¿«é€Ÿè¯„ä¼°å‡½æ•°ï¼Œè¿”å›ç®€å•çš„å¸ƒå°”ç»“æœ
        
        Args:
            result_return_value: æµ‹è¯•ç»“æœçš„è¿”å›å€¼
            
        Returns:
            bool: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        evaluation = self.evaluate_test_result(result_return_value=result_return_value)
        return evaluation["passed"]


class MockTestResultEvaluator(TestResultEvaluator):
    """æ¨¡æ‹Ÿæµ‹è¯•ç»“æœè¯„ä¼°å™¨ï¼Œç”¨äºæµ‹è¯•å’Œå¼€å‘"""
    
    def __init__(self):
        # ä¸éœ€è¦çœŸå®çš„APIå¯†é’¥
        pass
    
    def evaluate_test_result(self, **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿè¯„ä¼°é€»è¾‘"""
        
        # è·å–è¾“å‡ºä¿¡æ¯
        stdout = kwargs.get("result_stdout", "")
        stderr = kwargs.get("result_stderr", "")
        return_value = kwargs.get("result_return_value", "")
        
        # ç®€å•çš„å¯å‘å¼åˆ¤æ–­
        combined_output = f"{stdout} {stderr} {return_value}".lower()
        
        # å¤±è´¥æŒ‡æ ‡
        fail_indicators = [
            "failed", "error", "exception", "traceback", 
            "assertion error", "test failed", "0 passed",
            "failure", "fatal", "critical", "1 failed", 
            "2 failed", "3 failed", "4 failed", "5 failed"
        ]
        
        # æˆåŠŸæŒ‡æ ‡  
        success_indicators = [
            "passed", "success", "ok", "all tests passed",
            "build successful", "completed successfully"
        ]
        
        has_failures = any(indicator in combined_output for indicator in fail_indicators)
        has_success = any(indicator in combined_output for indicator in success_indicators)
        
        # ç‰¹æ®Šå¤„ç†: "0 failed" ä¸åº”è¯¥è¢«è®¤ä¸ºæ˜¯å¤±è´¥
        if "0 failed" in combined_output:
            has_failures = False
        
        # åˆ¤æ–­é€»è¾‘ï¼šå¦‚æœåŒæ—¶å­˜åœ¨æˆåŠŸå’Œå¤±è´¥æŒ‡æ ‡ï¼Œä¼˜å…ˆåˆ¤æ–­ä¸ºå¤±è´¥
        if has_failures:
            passed = False
            confidence = 0.8
            reason = "æ£€æµ‹åˆ°å¤±è´¥æŒ‡æ ‡"
        elif has_success:
            passed = True
            confidence = 0.8  
            reason = "æ£€æµ‹åˆ°æˆåŠŸæŒ‡æ ‡"
        elif stderr and not stdout:
            # ç‰¹æ®Šå¤„ç†ï¼šunittestç­‰æµ‹è¯•æ¡†æ¶å°†ç»“æœè¾“å‡ºåˆ°stderr
            # æ£€æŸ¥stderræ˜¯å¦åŒ…å«æµ‹è¯•ç»“æœè€ŒéçœŸæ­£çš„é”™è¯¯
            unittest_patterns = [
                "ran", "test", "ok", "passed", "failed", 
                "errors", "failures", "skipped"
            ]
            is_test_output = any(pattern in stderr.lower() for pattern in unittest_patterns)
            
            if is_test_output:
                # å¦‚æœstderråŒ…å«æµ‹è¯•ç›¸å…³è¾“å‡ºï¼Œé‡æ–°è¯„ä¼°
                if "0 failed" in stderr.lower() or "ok" in stderr.lower():
                    passed = True
                    confidence = 0.7
                    reason = "unittestç»“æœåœ¨stderrä¸­æ˜¾ç¤ºæµ‹è¯•é€šè¿‡"
                else:
                    passed = False
                    confidence = 0.7
                    reason = "unittestç»“æœåœ¨stderrä¸­æ˜¾ç¤ºæµ‹è¯•å¤±è´¥"
            else:
                # çœŸæ­£çš„é”™è¯¯è¾“å‡º
                passed = False
                confidence = 0.6
                reason = "ä»…æœ‰é”™è¯¯è¾“å‡º"
        else:
            passed = True
            confidence = 0.3
            reason = "é»˜è®¤åˆ¤æ–­ä¸ºé€šè¿‡"
        
        return {
            "passed": passed,
            "confidence": confidence,
            "reason": reason,
            "test_type": "mock_evaluation",
            "details": {
                "fail_indicators_found": [ind for ind in fail_indicators if ind in combined_output],
                "success_indicators_found": [ind for ind in success_indicators if ind in combined_output]
            }
        }


# ä¾¿æ·å‡½æ•°
def evaluate_with_deepseek(result_return_value: str, 
                          api_key: str = None,
                          use_mock: bool = False) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨deepseekè¯„ä¼°æµ‹è¯•ç»“æœ
    
    Args:
        result_return_value: æµ‹è¯•ç»“æœè¿”å›å€¼
        api_key: DeepSeek APIå¯†é’¥
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿè¯„ä¼°å™¨
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    if use_mock or not api_key:
        evaluator = MockTestResultEvaluator()
    else:
        evaluator = TestResultEvaluator(api_key=api_key)
    
    return evaluator.evaluate_test_result(result_return_value=result_return_value)


def is_test_passed(result_return_value: str, 
                  api_key: str = None,
                  use_mock: bool = False) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿåˆ¤æ–­æµ‹è¯•æ˜¯å¦é€šè¿‡
    
    Args:
        result_return_value: æµ‹è¯•ç»“æœè¿”å›å€¼
        api_key: DeepSeek APIå¯†é’¥ 
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿè¯„ä¼°å™¨
        
    Returns:
        bool: æµ‹è¯•æ˜¯å¦é€šè¿‡
    """
    evaluation = evaluate_with_deepseek(result_return_value, api_key, use_mock)
    return evaluation["passed"]


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½ç»“æœè¯„ä¼°å™¨...")
    
    # ä½¿ç”¨æ¨¡æ‹Ÿè¯„ä¼°å™¨æµ‹è¯•
    test_cases = [
        "12 tests passed, 0 failed",
        "ERROR: test_addition failed - AssertionError",
        "Build completed successfully",
        "3 passed, 2 failed, 1 error",
        "All checks passed âœ“"
    ]
    
    for test_case in test_cases:
        result = evaluate_with_deepseek(test_case, use_mock=True)
        print(f"è¾“å…¥: {test_case}")
        print(f"åˆ¤æ–­: {'é€šè¿‡' if result['passed'] else 'å¤±è´¥'} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
        print(f"ç†ç”±: {result['reason']}")
        print()